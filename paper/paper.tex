\documentclass[sigplan,screen,acmsmall,anonymous,review]{acmart}%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\newif\ifdraft\drafttrue{}
\newif\iflater\laterfalse{}

\PassOptionsToPackage{names,dvipsnames}{xcolor}

\settopmatter{printfolios=false,printccs=false,printacmref=false}
\setcopyright{none}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.17}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{xspace}
\lstset{
  mathescape=true,
  frame=none,
  xleftmargin=10pt,
  stepnumber=1,
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  % language=haskell,
  keepspaces=true,
  tabsize=2,
  emphstyle={\bf},
  % commentstyle=\it\color{dkgreen},
  stringstyle=\mdseries\ttfamily,
  showspaces=false,
  keywordstyle=\bfseries\ttfamily,
  columns=flexible,
  basicstyle=\footnotesize\ttfamily,
  showstringspaces=false,
  % morecomment=[l]\%,
  % moredelim=**[is][\color{dkgreen}]{@}{@}
}

\include{macros}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newtheorem{claim}{Claim}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2025/02}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Fail Faster}
\subtitle{\hg{TODO: Subtitle}}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Cynthia Richey}
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}
}

\author{Joseph W. Cutler}
\affiliation{
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}
}

\author{Harrison Goldstein}
\affiliation{%
  \institution{University of Maryland}
  \city{College Park}
  \state{Maryland}
  \country{USA}
}

\author{Benjamin C. Pierce}
\affiliation{%
 \institution{University of Pennsylvania}
 \city{Philadelphia}
 \state{Pennsylvania}
 \country{USA}}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Richey and Cutler et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  \hg{TODO: Abstract}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007</concept_id>
       <concept_desc>Software and its engineering</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006</concept_id>
       <concept_desc>Software and its engineering~Software notations and tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011072</concept_id>
       <concept_desc>Software and its engineering~Software libraries and repositories</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011041.10011046</concept_id>
       <concept_desc>Software and its engineering~Translator writing systems and compiler generators</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering}
\ccsdesc[500]{Software and its engineering~Software notations and tools}
\ccsdesc[500]{Software and its engineering~Software libraries and repositories}
\ccsdesc[500]{Software and its engineering~Translator writing systems and compiler generators}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Property-based testing, Generators, Staging, Meta-programming}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{--}
\received[revised]{--}
\received[accepted]{--}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

%\jwc{This paper is about ``Strict functional languages like OCaml and Scala'', but we'll focus on OCaml for presentation.}

%\bcp{The current introduction says, roughly, ``We took a look at
%  base.quickcheck, noticed some inefficiencies, applied known
%  techniques from staged metaprogramming to eliminate them, and
%  observed that speed went up.''  I think we can make much stronger
%  claims, but I'm not sure about exactly how strong or what belongs in
%  the foreground...
%  \begin{itemize}
%  \item One of the first serious uses of staging in anger?
%  \item A use of staging that is novel / challenging in
%  itself, in some way?
%  \item An analysis of sources of inefficiency across a range of PBT
%  frameworks (and a solution that applies to many of them)?
%  \item A usable tool that addresses and overcomes some significant
%  implementation challenges?
%  \item Careful measurements showing where the sources of inefficiency
%  are in existing PBT tools, and which ones matter most?
%  \item (Your claim here...)?
%  \end{itemize}
%}

%\jwc{Agree: I think something like the 3rd is the best reframing. Something
%like: ``We demonstrate that, across languages, monadic PBT generator DSLs can
%have a significant performance overhead, and present a cross-language technique
%for eliminating the overhead while preserving the idiomatic abstraction.
%''}

%\jwc{People have been using staged metaprogrammign to eliminate abstraction
%overheads in parsing for a long time, we turn that lens on generation (Which looks like parsing)}

%\jwc{Another angle that I'd argue needs to come out more is that we demonstrate that
%engineering PBT libraries with performance in mind has significant impact on testing power.
%}

%\jwc{
%Let's lead on with the quote from PBT in practice: performance engineering
%matters.  De-emphasize the abstraction overhead framing. Eliminating abstraction
%overhead is important, so is using the fastest RNG you can possibly use.
%}

%\jwc{``Property based testing is a race against time...''}

Property-based testing (PBT) is a software testing technique which uses
random test inputs to validate logical specifications.  Recent studies on PBT usage~\cite{inpractice} have shown that
practitioners often interleave PBT with their software development process, sometimes
testing their properties every time they save their code. This means that PBT is often run with very short time
budgets---e.g., between 50 milliseconds and 30 seconds---and it is important
that bugs can be found in that time-frame.\iflater
\jwc{phrasing could suggest ``race against time'' aspect.}\hg{Agree, can't see
how to fit this in yet}\fi

Given these strict timelines, the impact of performance in PBT cannot be overstated. One important locus for performance improvement
is the \textit{generators} that produce random inputs to the properties under
test. These generators are written with the help of \textit{generator
libraries}, usually expressed as embedded domain-specific languages (eDSLs) that
provide combinators for building and composing generators. Optimizing these
libraries is a key opportunity to speed up PBT: if programmers are
provided with the tools to write fast generators, then they do not themselves
need to be experts in performance tuning to find bugs fast. \iflater\bcp{Incorporate fail fast somehow.}\fi

However, careful measurements show that the performance of existing generator libraries falls significantly short of what
is possible. This is due to two major sources of inefficiency.
First, the same high-level design that gives generator libraries their
flexibility and expressiveness also introduces overhead: layered abstractions
and indirections hinder compiler optimizations, shifting work to
run time.
Second, the \rand that underlies the generator's operation---the source of the
random numbers that are used to make choices in the generator---can 
constitute an unexpectedly high proportion of a generator's running time.
\iflater
\hg{TODO later, I'm not 100\% sure it's clear how this particular question is
raised by the above. We could strengthen that connection}
\fi
Together, these observations raise a fundamental question: can generator libraries be expressive and high-level without compromising efficiency?
%In practice, PBT is used 
%as a lightweight, ad hoc approach to software correctness,
%\jwc{Move this bit about how it's used to the first sentence. I.e. PBT is a testing technqique where random data is fed to a system under test...}
%aimed at 
%detecting bugs within a short testing window---typically 50 milliseconds 
%to 30 seconds. \jwc{This sells us a little short: we should say something to the effect of ``Recent work on understanding how PBT is used in practice has determined that practitioners often run PBT with very short time budgets...''} Given this practical constraint, it is imperative that 
%inputs be generated as fast as possible; in other words, generators must be 
%treated as performance-sensitive code.

% The locus for improvement in this area \jwc{``This area'' sounds like it's talking about the whole area of PBT... Maybe ``a high-leverage way to improve the performance of generators is to...''} is \textit{generator libraries}: if 

  % in strict functionallanguages like OCaml and Scal
  To address this question, we outline two key principles for designing generator libraries that balance efficiency with flexibility.
 First, we describe a method of optimizing generator libraries using
\textit{multi-stage programming}, or ``staging,'' which has long been used for
optimizing functional DSLs~\cn.
We apply this technique to generators, completely eliminating the overhead of
many common generator abstractions. For example, in unstaged generator
libraries, each call to monadic bind usually spends time constructing and
immediately executing a closure at run time. \jwc{Todo: joe --- cite some evidence for this}
Meanwhile, a staged approach can resolve the bind at compile time, enabling the
compiler to inline and eliminate these closures.
Secondly, we demonstrate the impact of different choices of \rand on generator
performance by conducting a controlled comparison. \iflater\hg{More words here for
parallelism?}\fi Together, these insights provide a recipe for improving existing generator libraries.

%\jwc{I think we should use return/bind as the simplest example, since then you don't have to get into the weeds of allocation or closures. The functional abstractions of a monadic generator DSL block the compiler from generating fast code.}
%\jwc{Then present this case as ``more sophisticated use of staging'' or something?}
%where an unstaged generator DSL might represent a choice between
%a list of options as a dynamically-allocated array of closures, a staged 
%generator can expand to an if-else tree at compile 
%time---eliminating the allocation entirely.
% This approach is not tied to any
%particular PBT library, but rather provides a recipe for improving even
%state-of-the-art implementations \jwc{Sentence structure doesn't follow. Instead maybe ``not any particular, but a recipe for improving in general''}.

%First, we demonstrate the impact of
%RNG choice by optimizing an existing \rand. Our optimized RNG produces 
%exactly 
%the same values as its unoptimized counterpart, allowing us to directly 
%measure the impact of this choice on generator performance. \jwc{Permute? I think staging \emph{then} randomness.} \jwc{Also, i don't love this as the first place we explain the drop-in equivalence of the RNG. It's unmotivated why wen want this.} 

To demonstrate the effectiveness of our approach, we apply it to \bq, a
high-quality PBT library in OCaml, authored by software engineers at the trading firm Jane Street.
Our improved version, \name, is a direct drop-in replacement for
\bq that 
is \textit{100\% semantically equivalent}: that is, given identical random seeds, generators written using \name{}
produce exactly the same sequence of values as those written in \bq. However, in \name, 
the abstractions provided by the generator DSL are zero-cost, and the \rand is 
highly-optimized. The semantic equivalence between \name and \bq allows us to 
perform clean, fine-grained comparisons, as differences in generation speed are
solely attributable to the staging and \rand optimizations.

We evaluate the optimizations in \name with a series of case studies in each
generator library, and show that \name runs faster than \bq.
Further, we show that this performance benefit\hg{wc?} translates to bug-finding
ability by running these case studies in the Etna framework~\cite{etna}, a
platform that simulates real-world PBT usage\hg{Don't love this phrase, I'll
think more about how we should talk about Etna} by injecting bugs into a system and
measuring how quickly various generators detect them.
Finally, to demonstrate the generality of our technique, we implement a version
of \name in Scala \hg{(based on scalacheck?)} and benchmark the same generators---thereby highlighting its portability to other
strict functional languages.
\iflater
\jwc{I think we should more explicitly point out the portability earlier, maybe
up where we say we're applying it to BQ.}\hg{Agree}
\fi

In summary, we show that monadic PBT generator DSLs incur significant performance costs across languages, and present a general technique for improving their efficiency while preserving their idiomatic abstractions.
We make the following contributions:
\begin{enumerate}
    \item We identify two key sources of inefficiencies in PBT generator libraries, which can significantly impact performance: 
    namely, and abstraction overhead, and \rand choice. \secref{section:motiv}
    % \item We apply multi-stage programming to the novel domain of generators. \jwc{Less sure this should be a top-level conrib.} \secref{section:}
    \item We implement our approach in OCaml and Scala, showcasing its cross-language applicability.
    \secref{section:4}
    \item We construct generators in \name and \scalaname, and show that they give substantial performance improvements. Finally, we show
    that these performance improvements extend to bug-finding ability.
     \secref{section:eval}
\end{enumerate}

\jwc{Ensure we have a signposting table-of-contents-ish few sentences here, if it's not covered by the contribs.}\todo{Todo Thia}

\section{Sources of Inefficiency in Monadic Generator Libraries}
\label{section:motiv}
\jwc{In this section we answer: ``What are monadic generator libraries, and why are they slow?''}
\bcp{That's actually not a bad section title!}

\subsection{Background: Monadic Generator DSLs}
\jwc{Usual spiel about pbt and generators: here's a monadic generic generator library (but we're using BQ to illustrage), it has return and bind, this is what they're for.}
\jwc{You often want generators to generate for sparse precoditions so you can exercise a property reasonably well}

\begin{lstlisting}
module Bq : sig
  type 'a t
  val return : 'a -> 'a t
  val bind : 'a t -> ('a -> 'b t) -> 'b t
  val gen_int : lo:int -> hi:int -> int t

  val weighted_union : (int * 'a t) list -> 'a t

  val size : int t
  val with_size : int -> 'a t -> 'a t

  val fixed_point : ('a t -> 'a t) -> 'a t
  $\dots$
end
\end{lstlisting}

\jwc{\texttt{gen\_int} here is really \texttt{int\_uniform\_inclusive}. I also think we should completely omit named arguments in the paper so we don't confuse with splices.}

\jwc{Here's an example of a generator: you can sample from it and it gives you pairs of ints, one less than the other.}

\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  let%bind x = Bq.gen_int 0 100 in
  let%bind y = Bq.gen_int 0 x in
  return (x,y)
\end{lstlisting}

\jwc{This desugars to...}

\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  Bq.bind (Bq.gen_int 0 100) (fun x ->
    Bq.bind (Bq.gen_int 0 x) (fun y ->
      Bq.return (x,y)
  ))
\end{lstlisting}


\jwc{
Explain effect ordering here!!
\begin{itemize}
  \item \texttt{Bq.bind} has the CBV sampling semantics
  \item If you \texttt{Bq.bind} \texttt{x} and then use it twice, both uses refer to the \emph{same} value.
  \item Sampling is effectful: it mutates the internal state.
  \item Permuting two independent binds gives you a generator that is distributionally equivalent, but not equivlaent when considered up to pointwise equality as \emph{functions} from seed to value.
  \item Note that it's much more sensible to compare the bugfinding performance of two function-equivalent generators:
\end{itemize}
}

\jwc{
  \begin{itemize}
    \item PBT generator libraries include more functions than just the monad interface and sampling, libraries include helper functions.
    \item With \texttt{weighted\_union}, to make a weighted choices, and \texttt{fixed\_point} to define recursive generators.
    \item Also \texttt{size} and \texttt{with\_size} to adjust the size parameter --- these are useful to ensure we terminate with recursive generators.
  \end{itemize}
}

\jwc{This section needs to explain approximately how weighted union works --- compute a sum of the weights, sample between 0 and the sum, pick
the first element where the partial sums to the left exceeds the sampled number.
(This has been written about a billion times, cite one of the papers about this.)
}

\begin{lstlisting}
let tree_of g = fixed_point (fun rg ->
  let%bind n = size in
  weighted_union [
    (1, return E);
    (n,
      let%bind x = g in
      let%bind l = with_size (n / 2) rg in
      let%bind r = with_size (n / 2) rg in
      return (Node (l,x,r))
    )
  ]
)
\end{lstlisting}

\jwc{This generator generates binary trees using all the combinators, yay. Explain em.}



\jwc{IMPORTANT: We are using OCaml because we have to pick a syntax for this section, but these abstraction overheads exist in other languages -- at least scala, rust.}

\subsection{Abstraction Overhead of Generator DSLs}
Just how large is the abstraction overhead of monadic generator DSLs, and where does it come from?
Figure~\ref{fig:bq-internals} shows the internals of (a simplified version of) \bq{}.
A generator \texttt{'a Bq.t} is just a function of type~\texttt{int -> SR.t -> 'a},
taking an \texttt{int} representing the current size parameter and a random seed \texttt{SR.t},
and returning a generated value \texttt{'a}. It is an invariant of the library that function of this type are
deterministic: for a fixed size and seed, it will return the same value. All of the randomness in testing comes from varying the initial seed.
The monad functions \texttt{return} and \texttt{bind} are defined in the usual way
for an instance of the reader monad \cn{}. \texttt{Return x} ignores the size and seed and just returns
\texttt{x}, while \texttt{bind g k} first runs \texttt{g} and then passes the generated value to the continuation \texttt{k}.
\bq{} uses a mutable seed, so the \texttt{random} passed to \texttt{(k a)} will be different from the one passed to \texttt{g}.
The \texttt{gen\_int} combinator simply calls out to the randomness library \texttt{Splittable\_random}, aliased as \texttt{SR} here.
\jwc{Should we do this?}

Different PBT libraries use variations on this basic design. Haskell's QuickCheck uses a reader monad with an immutable state
that is ``split'' at \texttt{bind}s \cn. Meanwhile, ScalaCheck (a) uses an immutable state type and state monad \cn{} to thread
the state through, and (b) has an \texttt{Option} as its return type, to allow generation to fail.
\jwc{However, the principles are the same, and we will use \bq{} to illustrate.}


\begin{figure}
\begin{lstlisting}
module Bq = struct
  type 'a t = int -> SR.t -> 'a

  let return (x : 'a) : 'a t = fun _ _ -> x

  let bind (g : 'a t) (k : 'a -> 'b t) : 'b t =
    fun size random ->
      let a = g size random in
      (k a) size random

  let gen_int (lo : int) (hi : int) : int t =
    fun _ random -> SR.int random lo hi
end
\end{lstlisting}
\caption{Internals of a Monadic Generator eDSL}
\label{fig:bq-internals}
\end{figure}

Just how much run-time overhead does this monadic abstraction introduce? To
illustrate, let's return to our running example of a constrained pair of
integers, written in both \bq{} and ScalaCheck.
Figure~\ref{fig:overhead-explanation-code} shows two versions of the generator, written in both languages.
The first versions (\texttt{int\_pair} in \bq{} and \texttt{intPair} in ScalaCheck) are written with the monadic generator combinators from their respective libraries.
\jwc{Maybe we can avoid rewriting these if we already showed them earlier? Also maybe we duplicate the tutorial discussion of monadic generator libraries to include Scala?}
The second versions (\texttt{int\_pair\_inlined} and \texttt{intPairInlined})
are semantically identical to the first, but have been rewritten
by (1) inlining all generator combinator definitions, and then (2)
repeatedly reducing simplifiable terms like \lstinline{(fun x -> e) e'} --- where an anonymous function is defined and then immediatly called~\footnote{This is usually known as a ``$\beta$-redex''.
Note that when reducing, we do not reduce to \lstinline{e[e'/x]}, as this does not preserve the order of effects.
}
--- to \lstinline{let x = e' in e}.

\begin{figure}
\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  let%bind x = (Bq.gen_int 0 100) in
  let%bind y = (Bq.gen_int 0 x) in
  Bq.return (x,y)

let int_pair_inlined : int -> SR.t -> int * int =
  fun _ sr ->
    let x = Splittable_random.int sr ~lo:0 ~hi:100 in
    let y = Splittable_random.int sr ~lo:0 ~hi:x in
    (x,y)

def intPair : Gen[(Long,Long)] = for {
  x <- Gen.choose(0,1000)
  y <- Gen.choose(0,x)
} yield (x,y)

def intPairInlined : (Gen.Parameters, Seed) => (Option[(Long,Long)],Seed) = {
 (p,seed) =>
  val (x,seed2) = chLng(0,1000)(p,seed)
  x match {
    case None => (None,seed2)
    case Some(x) =>
      val (y,seed3) = chLng(0,x)(p,seed2)
      y match {
        case None => (None,seed)
        case Some(y) => (Some(x,y),seed3)
      }
    }
}
\end{lstlisting}
\caption{Int Pair Generators in \bq{} and ScalaCheck}
\label{fig:overhead-explanation-code}
\end{figure}

The performance impact of this inling is large (Figure~\ref{fig:overhead-explanation-perf}). In both languages, the
inlined version takes (on average) \textbf{half} as much time to generate a single pair of
\texttt{int}s. 
Microbenchmarks of more realistic generators (see Section~\ref{section:eval}
\jwc{more precise ref}) show an even more dramatic performance boost for
inlining in this manner.

Because the inlined versions of the generator are identical to the un-inlined
versions except for mechanical, semantics-preserving transformations,
this performance difference is attributable solely to
the different machine code generated by the compiler.
Indeed, compilers of effectful and strict functional languages (including JIT
compilers, in the case of Scala 3)
use heuristics to determine if and when to perform this particular kind of
simplification~\footnote{As we discuss in Section~\ref{asdf} \jwc{fixme}, purity
means that Haskell is a slightly different story. GHC can and often does
transformations of this form.}.  Even in cases as simple as
Figure~\ref{fig:overhead-explanation-code}, the indirection of \texttt{return}
and \texttt{bind} causes these heuristics to not fire.  Moreover, the story is
even worse for recursive generators, as the heuristics are necessarily even more
conservative for optimizing recursive functions. \jwc{unclear if we need this sentence, it's sorta obvious}



\begin{figure}
  \begin{tabular}{lll}
  Library    & Generator          & Average Time per Generation (ns) \\
  \bq{}         & \texttt{int\_pair}          & 70                       \\
  \bq{}         & \texttt{int\_pair\_inlined} & 35                       \\
  ScalaCheck & \texttt{intPair}            & 458                      \\
  ScalaCheck & \texttt{intPairInlined}     & 266                     
  \end{tabular}
\caption{Microbenchmarks of Generators in \bq{} (using \texttt{core\_bench} \cn) and ScalaCheck (using \texttt{jmh} \cn)}
\label{fig:overhead-explanation-perf}
\end{figure}

While neither of these microbenchmarks exhibit this behavior, more complex
generators suffer further performance penalties due to \emph{closure
allocation}. In cases where the compiler cannot statically eliminate it \jwc{``it'' = ``the allocation''},
running a monadic bind allocates a short-lived closure: we allocate a closure
for the continuation, and then immediately jump into it.
In strict functional languages like OCaml and Scala,
Each individual closure allocation is relatively cheap, but doing lots of allocation in a
generator is very costly because each allocation brings us closer to the next costly GC pause \cn{}.
Similarly to \jwc{the previous one} this effect is magnified in recursive generators: each iteration
through the recursive loop re-allocates closures for \texttt{bind}s, so the amount of allocation
\emph{per generated value} scales linearly with the number of recursive generator calls.

\subsubsection{Overhead of Choice Combinators}
The monad functions \texttt{return} and \texttt{bind} aren't the only combinators that incur an
abstraction overhead: all of them do! \jwc{I'm so bad at starting new sections...} ``Choice'' combinators like \texttt{weighted\_union}, however
are particularly costly, because they unavoidably allocate a lot.

\jwc{
Aside from the usual fact that the compiler can't ``see through'' the
abstraction boundary to specialize, combinators like union and weighted union
necessarily allocate a lot in the generation hot path.  Even without the dumb BQ
array thing, to call union and weighted union, the types mean that you have to
allocate a list.  These allocations are extremely costly, and happen every
sample from the generator. If the generator is recursive (like the tree
generator), they occur at each recursive call.
}

In practice, \texttt{weighted\_union}s is almost always\footnote{
There are some generators where \texttt{weighted\_union} is passed a list which
was itself the result of a generator: the well-typed STLC term generator used in Section~\ref{section:eval} is an example of this.
} passed an explicitly constructed list, like
\texttt{weighted\_union [(3,g); (1,g'); (2,g'')]}. 
This is because the most common use case for \texttt{weighted\_union} is
to choose between one of the different constructors of an algebraic datatype,
the options for which are always known.
The list passed to \texttt{weighted\_union} allocated at the call site, and then \emph{immediately}
destructed by the function. In principle, since the elements of the list and its length $n$ are known statically,
a compiler could unroll the loops in \texttt{weighted\_union} to depth $n$ and specialize the function to avoid allocating the list.
In practice however almost~\jwc{Once again, GHC is a notable exception here \cn{}} no compilers perform this kind of optimization.

% \jwc{This builds the cdf of the distribution, samples from 0 to the total, and binary searches through the array to find the bucket.}
% \jwc{Allocating this array is unnecessary: we can simply compute the total weight, sample \texttt{x} between 0 and the total, and then walk the list accumulating the sum, until the accumulator exceeds \texttt{x}. Since in practice the list of possible options is quite small (usually at most 10 in practice), the linear time scan is going to be much faster.}
% \jwc{(The point of this para will just be to emphasize that it's almost always better to be allocation-aware, instead of algorithmically clever.)}



% \subsubsection{Function call overhead}
\subsection{Inefficient Randomness Libraries}
\jwc{We really need to have explained the effect ordering equivalence aspect of the evaluation before here.}

The core of any PBT generator library is a source of randomness: to generate
random values of some datatype, we some access to random numbers!
Different PBT libraries use different randomness libraries implementing different algorithms
\footnote{The common term for such an algorithm or library is a ``Random Number
Generator'' (RNG) we will avoid this term and instead say ``randomness library''
to avoid confusing RNG implementations with the PBT gnenerator libraries that
use them.}.
Following the original Haskell QuickCheck implementation, \bq\ uses the SplitMix
algorithm \cn, implemented as a (stateful) OCaml library called \texttt{Splittable\_random}. Meanwhile,
ScalaCheck uses the JSF algorithm \cn.
% https://www.pcg-random.org/posts/bob-jenkins-small-prng-passes-practrand.html
% https://burtleburtle.net/bob/rand/smallprng.html

The randomness library sits at the heart of the hot path.
Even basic generators --- like ones generating a single \texttt{int} or \texttt{float} uniformly within a range --- can sample \emph{unboundedly many}
random numbers, since they usually use versions of rejection sampling to find a value within the range \cn.
Moreover, generator combinators like \texttt{list} usually make $O(n)$ calls to even those basic generators.
Because of this, the speed of a single sample matters a great deal. Unfortunately,
existing PBT libraries make relatively inefficient choices on this front,
leading to worse bugfinding power than what is possible.

\hg{This paragraph feels a bit clumsy right now. What if we started with a table
of the RNGs used by a bunch of popular PBT frameworks and then argued that
things like Lehmer are faster?}

For example, significantly faster algorithms than SplitMix or JSF exist,
such as the Lehmer algorithm\cn. In microbenchmarks, the Lehmer algorithm runs almost 2x as fast as SplitMix \cn.
% https://github.com/lemire/testingRNG/tree/master
Moreover, PBT libraries could even consider eschewing the requirement that a source of
entropy pass statistical tests like BigCrush\cn. \hg{This needs more discussion} 
This is common practice in other areas of testing already:
fuzzers often simply use a buffer full of arbitrary bytes as a source of entropy
\cn. Such approaches are also faster than algorithms like SplitMix: bumping a
pointer and reading from memory (which can be pipelined trivially) will always
be faster on modern CPUs than any random sampling algorithm with data-dependent arithmetic
instructions.\hg{Should we just always be doing this? I think we want to argue
that this is a bridge too far because you still need to generate the buffer
ahead of time and you can run out of randomness?}
\jwc{HG: talk about the tradeoffs here.}

\jwc{The point here is that ``you should be thinking about using a more performant RNG, we'll show you that it does have real benefits (and quantify them). But there are tradeoffs here.'}

Of course, simply arguing that the randomness library is on the hot
path does not guarantee that a faster sampling leads to measurably faster
generation; for that, we need an experiment. The most obvious experiment is to
simply swap out the randomness library for \bq{} with a totally different, faster one. But, as
discussed previously, generators with different generation orders can be
difficult to compare. To get around this, we exploit a ``natural experiment:
OCaml's \texttt{Splittable\_random} library is slow in a way that
can be improved \emph{without} changing its extensional behavior.
In particular,
due to implementation details related to the OCaml garbage collector,
values of the OCaml type \texttt{int64} are not machine words, but rather
\emph{pointers} to machine words. This means
that \emph{all} \texttt{int64} operations (both arithmetic and bitwise) must
allocate memory cells to contain their output, which has a significant performance benefit.
By building a version that uses much faster ``unboxed'' 64-bit integer arithmetic, we can demonstrate just how much bug-finding
performance can be improved just by using a more performant randomness library.

\section{Speeding up Generator DSLs by Staging and Faster Randomness Libraries}
\jwc{This section needs a much better title...}
\jwc{Emphasize that this is an \emph{COMPLETELY EQUIVALENT drop in replacement!} We din't just build a different library with different distributions.}

\jwc{Usual introduction to this section, corresponding to how we talked about it in the intro. ``We present a library that XYZ''.}

\hg{TODO: Signposting --- I got a few paragraphs in and realized I wasn't sure what I was reading}

\subsection{Background: Multi-Stage Programming}
\label{subsection:msp}

In Muli-stage programming (also known more simply as staging), programs execute
over the course of multiple stages, with each stage producing the code to be run
in the next.  This is an old idea in programming languages, with roots going
back to quasiquotation in LISP \cn{}.  For the purposes of this paper, we will
primarily consider two stages: compile time and run time. Staged programs thus
execute twice: once at compile time, which produces more code, which is then
compiled and run at run time.

\subsubsection{Uses of Staging in eDSL Construction}
One of the primary uses of staging is in embedded DSL construction \cite{sheard99, lms, trattdsl}.
While embedded DSLs are great \jwc{words}, they have well-known drawback: 
the functional abstractions used to build eDSLs prevent compilers from generating efficient machine code \cn.
As alluded to previously \jwc{This sentence here because of the section-ordering transpose... may need to mention this earlier.}, this effect is known as ``abstraction overhead'' \cite{carrette05, moller20}: the layers of abstraction that 
make eDSLs so usable are precisely what prevents them from being fast.
\jwc{Something to the effect of ``the issues we noticed in BQ and SC exist across many eDSLs''}
In light of this issue, staging is often used as a lightweight compiler for DSLs. The compile-time evaluation stage
transforms the DSL code, eliminating abstractions to produce code that the host language compiler can generate fast machine code for.
\jwc{This previous paragraph is peak ``shitty first draft''.}
This recipe has been used to great effect across domains to stage eDSLs for stream processing \cite{moller20,strymonas}, parser combinators \cite{sspc, staged-parsers, krishnaswami19, flap},
and query processing \cite{rhyme}.

Many languages have some degree of staging functionality, though support varies
from full native support (Scala 3 \cite{scalamacros}, Haskell \cite{templatehaskell}) to compiler extensions (OCaml \cite{metaocaml,macocaml}, Java \cite{mint}).
Moreover, many more languages have lighter-weight staging-like metaprogramming support in the form of macros.
See Section~\ref{subsection:other-langs} for a full discussion of just how portable these ideas are. \jwc{this sentence is a placeholder lol}

For this paper, we have implemented staged PBT libraries in both OCaml --- using the MetaOCaml fork of the OCaml compiler \cn --- and Scala 3 --- using
\texttt{scala.quoted}. For presentation purposes, all staged code presented in the body of this paper is written in OCaml.
However, we believe all of the concepts presented are portable across languages with staging support, and we have reimplemented \name in Scala 3.

\subsubsection{Staging in MetaOCaml}
MetaOCaml's staging functionality is exposed through a type \texttt{'a code}. A
value of type \texttt{t code} at compile time is a (potentially open) OCaml term
of type \texttt{t}.

Values of \texttt{code} type are introduced by \emph{quotes}, written
\texttt{.<$\ldots$>.}. Brackets delay execution of a program until run time. For
example, the program \texttt{.< 5 + 1 >.} has type \texttt{int code}.  Note that
this is not the same as \texttt{.< 6 >.}. Because brackets delay computation,
the code is not \emph{executed} until the next stage (run time).
Values of type \texttt{code} can be combined together using \emph{escape},
written \texttt{.\~{}(e)} (or just \texttt{.\~{}x}, when \texttt{x} is a variable).  Escape lets you take a value of type
\texttt{code}, and ``splice'' it directly into a quote.  For example, this
program \texttt{let x = .<1 * 5>. in .< .\~{}(x) + .\~{}(x) >.} evaluates to
\texttt{.<(1 * 5) + (1 * 5)>.}.  MetaOCaml ensures correct scoping and macro
hygiene, ensuring that variables are not shadowed when open terms are spliced.

The power of staging for optimization away abstraction overheads comes from
defining functions that accept and return \texttt{code} values.  A function
\texttt{f : 'a code -> 'b code} is a function that takes a program computing a
run-time \texttt{'a} and transforms it into a program computing a run-time
\texttt{'b}. In particular, because \texttt{f} itself runs at compile time, the
abstraction of using \texttt{f} is necessarily completely eliminated by
run time.
A code-transforming function \texttt{'a code -> 'b code} can also be converted \emph{code for a function} --- a value of type
\texttt{('a -> 'b) code} --- with the following program:
\begin{lstlisting}
let eta (f : 'a code -> 'b code) : ('a -> 'b) code = .<fun x -> .\~(f .<x>.)>.
\end{lstlisting}

This program is known as ``the trick'' in the partial evaluation and multi-stage programming literature.
\cn{} 
% USE THIS: https://arxiv.org/pdf/2309.08207
It returns a code for a function that takes an argument \texttt{x}, and then it splats in the result of calling \texttt{f} on just
the quoted \texttt{x}.
\jwc{Does this make any sense?}

The trick is best illustrated by an example. The following program reduces to \texttt{.< fun x -> (1 + x) mod 2 == 0 >}.
\begin{lstlisting}
let is_even x = .< .~x mod 2 == 0 >. in
let succ x = .< 1 + .~x >. in
eta (succ . is_even)
\end{lstlisting}
By composing the two code-transforming functions together at compile time, and only then turning them into a run-time function,
the functions are fused together... \jwc{words}.
This is the basis of how staging is used to eliminate the abstraction of DSLs
(like a generator DSL). By writing DSL combinators as compile-time functions ---
and only calling \texttt{eta} at the end on the completed DSL program ---  we
can ensure that any overhead of using them is eliminated by run time.

% \jwc{
%   \begin{itemize}
%     \item The \texttt{'a code} type, quote, escape or ``splice'', stage distinction.
%     \item MetaOCaml ensures correct scoping and hygene by preventing alpha-collision when you splice open terms.
%     \item The difference between \texttt{('a -> 'b) code} and \texttt{'a code -> 'b code}
%     \item We can convert one way, but not the other.
%     \item Functions \texttt{'a code -> 'b code} ``fuse''.
%     \begin{itemize}
%       \item Consider writing \texttt{even . succ}. This
%       \item Consider \texttt{even\_c : int code -> bool code = fun cx -> .<.~cx mod 2 == 0>.} and \texttt{succ\_c : int code -> int code = fun cx -> .< .~x + 1 >.}
%       \item If you do \texttt{to\_dyn (even\_c . succ\_c)}, you get \texttt{fun x -> (x + 1) mod 2 == 0}. Composing functions from code to code, and then \emph{only at the end} stamping out
%       a dynamic function value eliminates the function abstraction.
%     \end{itemize}
%     \item This is the basis of (WORD). By defining a library with functions with types like \texttt{'a code -> 'b code}, we can ensure that the
%     abstractions the library introduces are fully eliminated at compile time.
%   \end{itemize}
% }

\subsection{Design of a Staged Generator DSL}
\label{subsection:basic-design}
\jwc{Needs a lot more signposting at the top here.}

Recall \jwc{did we talk about this} that staging a DSL involves changing the
combinators to run at compile time by carefully annotating their types with \texttt{code}s.
Deciding which types \texttt{t} can
instead be \texttt{t code} --- in other words,
figuring out which parts of the DSL can be determined statically (and can be
part of the compile-time stage), and which parts are only known dynamically (and
hence must be \texttt{code}) --- is an art known as ``binding-time analysis'' \cn.

The crux of our binding time analysis is that the particular random seed and
size parameter \jwc{if explained} are only known at run time (the later stage),
but the code of the generator itself is known at compile time.\hg{Are we going
to talk more about how we did this analysis?} \jwc{Well it's sort of right here: you just think about it, it's the same thing as staging a parser too.}
Generators ---
values of type \texttt{'a Bq.t} --- are always constructed statically in practice,
so all of the combinators we use to build them can run at compile time.

This means our library's generator type \texttt{'a
Gen.t} should have the type \texttt{int code -> SR.t code -> 'a code}: a compile-time
function from dynamically-known size and seed to dynamically-determined result.

This type, along with basic staged monadic generator DSL functionality can be found in Figure~\ref{fig:gen-staged-basic}.
\texttt{Return} is the constant generator, but this time it 
runs at compile time. Given \texttt{cx : 'a code}, the code for a \texttt{'a}, it
returns the generator which always generates that value.
\texttt{bind g k} sequences generators by passing the result of running the generator \texttt{g} to a continuation \texttt{k}. However, instead of getting access
to the particular value generated by \texttt{g}, the continuation \texttt{k} gets access to \texttt{code} for the value sampled from \texttt{g}:
at compile time, we only know \texttt{g} will generate \emph{some} \texttt{'a}, but not which one \footnote{Readers familiar with OCaml may notice that \texttt{return : 'a code -> 'a Gen.t} and \texttt{bind : 'a Gen.t -> ('a code -> 'b Gen.t) -> 'b Gen.t}
do not have the correct types for a monad instance, preventing us from using \texttt{let\%bind} notation. We rectify this issue in Section~\ref{subsection:codecps} by importing some clever ideas from the staging literature.}.
Operationally, bind takes code for the size and seed, and returns code that (1) let-binds a variable \texttt{a} to spliced-in code that runs \texttt{g}, and then
(2) runs the spliced-in continuation \texttt{k}.
Both function applications \texttt{g size\_c random\_c}
and \texttt{k .<a>. size\_c random\_c} run at compile time.
\texttt{Gen.int} is the generator that samples an int from the randomness library.
Given any size and random seed, it returns a code block that calls \texttt{SR.int} with that random seed. Because the lower and upper bounds
might not be known at compile time --- they may themselves be the results of calling \texttt{Gen.int} ---
the arguments \texttt{lo} and \texttt{hi} are of type \texttt{int code}, and get spliced into the code block as arguments to \texttt{SR.int}.
Lastly, \texttt{to\_bq} turns a staged generator into code for a normal \bq\
generator. This function is just a 2-argument version of ``The Trick'' (\texttt{eta}
from Section~\ref{subsection:msp}).
\hg{TODO: Be really careful with formatting of the above paragraph. It's very
easy for some inline code to get split weirdly over a line break or just be
difficult to parse in general, and that could throw the reader off when the
content is already pretty low-level}


\begin{figure}
\begin{lstlisting}
module Gen = struct
  type 'a t = int code -> Random.t code -> 'a code

  let return (cx : 'a code) : 'a t = fun size_c random_c -> cx

  let bind (g : 'a t) (k : 'a code -> 'b t) : 'b t =
    fun size_c random_c ->
      .<
        let a = .~(g size_c random_c) in
        .~(k .<a>. size_c random_c)
      >.

  let int (lo : int code) (hi : int code) : int t =
    fun size_c random_c ->
      .< SR.int .~random_c .~lo .~hi >.

  let to_bq (g : 'a code Gen.t) : ('a Bq.t) code =
  .<
    fun size random -> .~(g .<size>. .<random>.)
  >.
end
\end{lstlisting}
\caption{Basic Staged Generator Library}
\ref{fig:gen-staged-basic}
\end{figure}

Returning to our running example, Figure~\ref{fig:running-staged} shows the
int-pair example written with the staged \texttt{Gen.t} monad, as well as the
inlined code that results from calling \texttt{Gen.to\_bq} (changing some identifier names for clarity).
The code generated is identical to the manually inlined version from Section~\ref{section:motiv}, and of course runs equally fast.

\begin{figure}
\begin{lstlisting}
let int_pair_staged : (int * int) Gen.t =
  Gen.bind (Gen.int .<0>. .<100>.) (fun cx ->
    Gen.bind (Gen.int .<0> cx) (fun cy ->
      Gen.return .<(.~cx,.~cy)>.
    )
  )

let int_pair : (int * int) Bq.t code = Gen.to_bq int_pair_staged
(* .< fun size random ->
        let x = SR.int random 0 100 in
        let y = SR.int random 0 x in
        (x,y)
    >.
*)
\end{lstlisting}
\caption{Pairs of Ints, Staged}
\ref{fig:running-staged}
\end{figure}

\jwc{Is there anything more we need to say here?}

\subsection{Staging Combinators}

In Section~\ref{section:motiv}, we noted that generator combinators like \texttt{weighted\_union}
often allocate lists in the hot path of the generator. Even though these lists are usually small ---
at most a few dozen elements in practice --- each allocation takes us closer to the next garbage collection.

This is an ideal opportunity to exercise another feature of staging: compile-time specialization.
Since we almost always know the particular list of choices at compile time, a staged version of \texttt{weighted\_union}
can generate \emph{different code} depending on the number of generators in the union.
If we use weighted union on a compile-time list of generators \texttt{g1}, \texttt{g2}, and \texttt{g3},
we can emit code that picks between the generators without realizing the list at
run time.

\begin{figure}
\begin{lstlisting}
module Gen =
$\dots$
  let pick (acc : int code) (weighted_gens : (int code * 'a t) list) (size : int code) (random : Sr.t code) : 'a code =
    match weighted_gens with
    | [] -> .< failwith "Error" >.
    | (wc,g) :: gens' ->
      .<
        if .~acc <= .~wc then .~(g size random)
        else
          let acc' = .~acc - .~wc in
          .~(pick .<acc'>. gens' size random)
      >.

  let weighted_union (weighted_gens : (int code * 'a t) list) : 'a t =
    let sum_code = List.foldr (fun acc (w,_) -> .<.~acc + .~w>. ) .<0>. weighted_gens in
    fun size random ->
      .<
        let sum = .~sum_code in
        let r = SR.int .~random_c 0 sum in
        .~(pick .<r>. weighted_gens size random)
      >.
\end{lstlisting}
\caption{Staged Weighted Union}
\label{fig:staged-weighted-union}
\end{figure}

Figure~\ref{fig:staged-weighted-union} shows the code for such a staged weighted union.
Crucially, it takes a \emph{compile-time} list \texttt{weighted\_gens} of
generators and weights. The weights themselves might only be known at run time --- it is
common to use the current size parameter as a weight, for instance ---
so they are \texttt{code}s.
\texttt{Gen.weighted\_union} begins by computing \texttt{sum\_code}, an \texttt{int code}
that is the sum of the weights. Note that this happens at compile time: we fold over a list
known at compile time to produce another code value. 
We then call \texttt{SR.int} to sample a random number \texttt{r} between \texttt{0} and the sum.
Finally, we splice in the result of calling the helper function \texttt{pick}. \texttt{pick}
produces a tree of \texttt{if}s by again traversing the list of generators at compile time.
This tree of \texttt{ifs} ``searches'' for the generator corresponding to the
sampled value \texttt{r}, and then runs it.
\hg{I'm not sure this helps me. I think I'd prefer a less detailed explanation
of the code that conveys the intuition and let the reader actually read the code
if they want to know specifics. As it stands the explanation is just too dense
for me to process}

Figure~\ref{fig:staged-weighted-union-example} demonstrates a use of this staged weighted union.
Given a list (in this case constant) generators with weights ``the current size parameter'', \texttt{2}, and \texttt{1},
the generated code first computes the sum of these numbers, samples between \texttt{0} and the sum, and then
traverses a tree of three \texttt{if}s to find the correct value to return.

\begin{figure}
\begin{lstlisting}
let grades : char Bq.t = Gen.to_bq (
  Gen.bind size (fun n ->
    Gen.weighted_union [
      (n, Gen.return .<'a'>.);
      (.<2>., Gen.return .<'b'>.);
      (.<1>., Gen.return .<'c'>.);
    ]
  )
)
(*
.< fun size random ->
    let sum = size + 2 + 1 + 0 in
    let r = SR.int random 0 sum in
    if r <= size then 'a'
    else
      let r' = r - size in
      if r' <= 2 then 'b'
      else
        let r'' = r' - 2 in
        if r'' <= 1 then 'c'
        else
          failwith "Error"
>.
*)
\end{lstlisting} 
\caption{Use of Staged Weighted Union}
\label{fig:staged-weighted-union-example}
\end{figure}

\subsection{Let-Insertion and Effect Ordering}
Careful readers might note that the definition of \texttt{bind} in Section~\ref{subsection:basic-design}
was more complicated than one might expect. In particular, why not define bind in a more standard way: as
\texttt{let bind' g k = fun size random -> k (g size random) size random}, without the code block
that let-binds the spliced code \texttt{.\~{}(g size random)}?\hg{Need to spell
this out --- the vast majority of readers won't have that precise question in
their heads. Might even be worth comparing the two implementations here side by side}
Unfortunately, using \texttt{Gen.bind'} leads to incorrect code being generated.
For example, consider \texttt{Gen.bind' (Gen.int .<0>. .<1>.) (fun x -> Gen.return .<(.~x,.~x)>.)}.
This generates the run time code \texttt{fun size random -> (SR.int random 0 1, int SR.random 0 1)},
which is incorrect. 
This is not equivalent to the behavior of writing the same code with base quickcheck \jwc{Which is a property we want to hold for our eval.}.
Instead of generating a single integer and returning it twice, it samples two different integers.

This problem is intimately related to nondeterministic effects in the presence of CBN/CBV evaluation ordering \cn.
In essence, the behavior of splice \texttt{.\~{}cx} in a staged function \texttt{f(cx : 'a code) = ...} is to \emph{copy}
the entire block of code, effects and all. To ensure that the randomness effects of the first generator are executed only once,
but that the value can be used in the continuation multiple times, \texttt{bind} let-binds the result of generation to a variable,
and then passes that to the continuation.

The library \jwc{ensure this is consistent with the way we talk about the project, cf cross-language} is careully designed to 
preserve exactly the effect order of base quickcheck \jwc{and the scala version to preserve the effect ordering of ScalaCheck}.
\jwc{This fact should go earlier. Not really sure about where this subsection should actually land, but it needs to be said somehwere.}
\hg{+1, I think this feels sort of out of place here, but I also think moving it
up would make that part harder to understand too. Is there a way to tie this in
to our discussion about maintaining the precise set of choices? The two issues
are different, but they're related}

\subsection{CodeCPS and a Monad Instance}
\label{subsection:codecps}
This is all great so far,\hg{too informal} but there's a subtle issue that prevents the version of the library design discussed so far
from being used as a proper drop-in replacement for an existing generator DSL: the types of \texttt{return : 'a code -> 'a Gen.t}
and \texttt{bind : 'a Gen.t -> ('a code -> 'b Gen.t) -> 'b Gen.t} aren't quite right.
For the type \texttt{'a Gen.t} to actually be a monad, 
the these types cannot mention \texttt{code}. This is not just a theoretical issue, it is a significant usability concern:
the syntactic sugar for monadic programming (\texttt{let\%bind} in OCaml, \texttt{foreach} in Scala, \texttt{do} in Haskell, etc)
that makes it so appealing can \emph{only} be used if the types involved are
actually the proper monad function types.
\hg{A pedantic reader may ask: Are we concerned with the monad {\em laws} as
well, or just the type signature?}

To support real monadic programming, we'll need to adjust the type of \texttt{'a Gen.t} slightly.
An initial attempt is to try \texttt{type 'a t = int code -> SR.t code -> 'a}. If we strip the \texttt{code} off the result type,
the functions \texttt{return (x : 'a) = fun \_ \_ -> x} and \texttt{bind (g : 'a t) (k : 'a -> 'b t) = fun size random -> k (g size random) size random}
have the proper types for a monad instance. Then, any combinators of type \texttt{'a Gen.t} before simply become \texttt{'a code Gen.t} with this new version. \jwc{do we want a different name for the first cut?}

However, this definition of bind doesn't have call-by value effect semantics, as
discussed in the previous section!  And because the type of \texttt{g size random} is
just \texttt{'a} (not necessarily \texttt{'a code}), we cannot perform the
let-insertion needed to preserve the CBV effects. To solve this problem, we
turn to a classic technique from the multistage programming literature: writing
our staged programs in continuation-passing style \cite{bondorf92}.
\hg{Slow down! This is all very interesting and very technical! Try making each
of these sentences two sentences, add some citations, and maybe spell this out
wiht an example}
\jwc{I could, but i'm just not sure this is critical.}

In Figure~\ref{fig:codecps-and-final-gen}, we follow prior work \cite{kovacs24, carrette05} and define
the type \texttt{'a CodeCps.t = 'z. ('a -> 'z code) -> 'z code}: a polymorphic continuation transformer with the result type
always in \texttt{code}
\footnote{This is an instance of the \emph{codensity} monad \cite{janis08}, a fact which deserves further investigation.}.
The monad instance for this type is the standard instance for a CPS monad with polymorphic return type.
In prior work, this type is often referred to as the ``code generation'' monad
(and sometimes, ironically, called ``\texttt{Gen}'').
This is because a value of type \texttt{('a code) CodeCps.t} is like an ``action'' that generates \texttt{code}:
\texttt{CodeCps.run} passes the continuation transformer the identity continuation to produce a \texttt{'a code}.
To avoid confusion with random data generators, we refer to this type as \texttt{CodeCps.t}.
Most importantly, the \texttt{CodeCps} type supports a function \texttt{let\_insert}, which, given \texttt{cx : 'a code},
let-binds \texttt{let x = .\~{}cx}, and then passes \texttt{.<x>.} to the
continuation.
\hg{I'm getting lost in the inline code again}

We can then redefine our staged generator monad type to be
\texttt{'a Gen.t = int code -> SR.t code -> 'a CodeCPS.t}, as shown in Figure~\ref{fig:codecps-and-final-gen}.
\jwc{any types that were \texttt{'a Gen.t} before are now \texttt{'a code Gen.t}}.
This gives us the best of both worlds. First, we get a monad instance for \texttt{'a Gen.t} with the correct types, which lets us
use the monadic syntactic sugar of our chosen language. Moreover, we also get to maintain the correct
effect ordering: effectful combinators like \texttt{Gen.int} do their \emph{own} let-insertion, ensuring
that a program like \texttt{Gen.bind Gen.int (fun x -> ...)} generates a let-binding for the result of sampling the randomness library.
For example, \texttt{bind (int .<0>. .<1.>) (fun cx -> return .<(.~cx,.~cx)>.)} now correctly generates
\texttt{.< fun size random -> let x = SR.int random 0 1 in (x,x) >.}. \jwc{Should we do out the whole reduction sequence here? It might be explanatory, but it also might be boring.}

This design is less obviously correct, and does require some care. Rather than \texttt{bind} ensuring correct evaluation order once and for all,
individual combinators must be carefully written to ensure that \texttt{'a code} values that contian effects are \texttt{let\_insert}'d.
In Section~\ref{subsection:pbt-for-pbt}, we discuss how we use PBT to ensure that the OCaml is written correctly.

\jwc{... and all of this works identically in Scala, too.}

\begin{figure}
\begin{lstlisting}
module CodeCps = struct
  type 'a t = { cps : 'z. ('a -> 'z code) -> 'z code }

  let return x = {cps = fun k -> k x}

  let bind (x : 'a t) (f : 'a -> 'b t) : 'b t =
    {cps = fun k -> x.cps (fun a -> (f a).cps k)}

  let run (t : ('a code) t) : 'a code = t.cps (fun x -> x)

  let let_insert (cx : 'a code) : 'a code t =
    {cps = fun k -> k .< let x = .~cx in .~(k .<x>.) >.}
end

module Gen = struct
  type 'a t = int code -> SR.t code -> 'a CodeCps.t

  let return (x : 'a) : 'a t = fun _ _ -> Codecps.return x

  let bind (g : 'a t) (f : 'a -> 'b t) =
    fun size random ->
      CodeCps.bind (g size random) (fun x ->
        (f x) size random
      )
  
  let int (lo : int code) (hi : int code) : int code t =
    fun size random -> let_insert .< SR.int .~random .~lo .~hi >.

end
\end{lstlisting} 
\caption{CodeCPS and The Final Gen Monad}
\label{fig:codecps-and-final-gen}
\end{figure}


\subsubsection{The Trick at Other Types}
To write more interesting generators, we also need the ability to generate code that manipulates run-time values.
For instance, consider this generator

\jwc{need a better example here}

\begin{figure}
\begin{lstlisting}
let int_or_zero : int Bq.t =
  let%bind n = size in
  if n <= 5 then return 0 else Bq.int

let split_bool (b : bool code) : bool Gen.t =
  fun _ _ ->  _

\end{lstlisting}
\caption{??}
\label{fig:trick-example}
\end{figure}


\jwc{Describe split --- this section is approximately experts-only, but you can just point at the Andras paper and the things it cites.}

\subsection{Recursive Generators}
\jwc{Generating recursive datatypes requires recursive generators!}

Different generator DSLs handle defining recursive generators differently. Some allow recursive generators
to be defined as recursive functions (or in Haskell's case, recursive values), while others
(like \bq) expose a fixpoint combinator to construct recursive generators.
Generator fixpoint combinators are simliar to a usual \texttt{fix : ('a -> 'a) -> 'a} combinator,
except that they operate at monadic type \texttt{fixed\_point : ('a Bq.t -> 'a Bq.t) -> 'a Bq.t}.
Given a step function that takes a ``handle'' to sample from a recursive generator call, it
ties the knot and builds a recursive generator.

In our case, letting programmers define recursive generators as recursive functions is out of the question.
With staged programming, recursion must be handled with care: it is far to easy to accidentally recursively
define an infinite \texttt{code} value and have the program diverge at compile time, when trying to write
a \texttt{code} representing a recursive program.
To this end, we develop a staged recursive generator combinator\footnote{
In reality, we actually have a more general API that allows programmers to define \emph{parameterized}
recursive combinators, of type \texttt{'r code -> 'a code Gen.t}, for any type \texttt{'r}. See Appendix~\jwc{appendix} for details.
}, whose API is shown in Figure~\ref{fig:staged-recursive-generator}.
The code for this combinator can be found in Appendix~\jwc{appendix}.
The recursion API consists of an opaque type \texttt{'a handle}, and a function \texttt{recurse} to perform recursive calls.
Programmers can then define recursive generators by \texttt{fixed\_point}, which ties the recursive knot.
\jwc{don't want to handle GADTs}

% \begin{lstlisting}
%   type ('a,'r) handle = 'r code -> 'a code t

%   let recurse (f : ('a,'r) handle) (x : 'r code) : 'a code t =
%     fun size random ->
%       Codecps.bind ((f x) size random) @@ fun c ->
%       Codecps.let_insert c

%   let recursive (step : ('a,'r) handle -> 'r code -> 'a code t) (x0 : 'r code) : 'a code t =
%     fun size_c random_c -> 
%       Codecps.return @@
%         .< let rec go x size random = .~(
%               Codecps.code_generate @@
%               (step (fun xc' -> fun ~size' ~random' -> Codecps.return .< go .~xc' .~size' .~random' >.) .<x>.) .<size>. .<random>.
%             )
%           in go .~x0 .~size_c .~random_c
%         >.
% end
% \end{lstlisting}

\begin{figure}
\begin{lstlisting}
type 'a handle
val recurse : 'a handle -> 'a code Gen.t
val fixed_point : ('a handle -> 'a code Gen.t) -> 'a code Gen.t
\end{lstlisting}
\caption{Staged Recursive Generator Combinator API}
\label{fig:staged-recursive-generator}
\end{figure}


\subsection{Staged Type-Derived Generators}

\jwc{
  \begin{itemize}
    \item In PBT in practice, we learned that in many cases, programmers don't
    even write custom generators, instead relying on type-derived generators.
    \item These generators just produce arbitrary values of a given type, not necessarily enforcing validity conditions.
    \item If 
    \item Let's talk about how type-deriving works. In languages with typeclasses, it works
    by typeclass resolution. In OCaml it works by PPX system, but the principle
    is the same. You define rules to go from generators of a subtypes to a generator of the larger type, and then apply those rules to build up a full generator.
    \item \begin{itemize}
      \item For base types, you just call the associated generator
      \item For product types, you sample from all the component gnerators, bind the values, and then tuple up the values, and return the tuple
      \item For variant types, you use a weighted union to choose one of the component generators with a weighted union.
      \item For recursive types, wrap the whole thing in a fixedpoint and then use the recusive handle as the ``component generator'' for all recursive instance of the type.
    \end{itemize}
    \item This kind of generic deriving of generators works just as well for
    staged generators. Just replace the standard combinators in question with
    staged ones!
    \item We built this in OCaml, but you can easily use typeclasses to do it in scala too.
    \item Note that this is (essentially) 3-stage metaprograming. You're using either the PPX mechanism or typeclass resolution to generate a bunch of staged combinator calls, which then run at compile time, which then run at run time.
  \end{itemize}
}
\jwc{Todo: Thia}

\subsection{Faster SplitMix with Unboxed \texttt{int64}s}
\label{subsection:faster-rng}

\jwc{Unclear what we should call this section}
As we discussed in Section~\ref{section:motiv}, choosing an inefficient randomness library is
another bottleneck for finding bugs fast. While generator libraries by
and large use sensible sources of randomness, they are not explicitly chosen
with performance in mind. Indeed, much faster randomness libraries\cn and fast sources
of entropy \cn exist. \jwc{... more here?}
To demonstrate that faster random sampling can significanty impact bugfinding power, we
use the natural experiment provided by OCaml's inefficient implementation of
SplitMix. By replacing this slow randomness library with a faster but extensionally equivalent
implemetnation, we are able to precisely quantify the bugfinding speedup that using a randomness library gives across a range of PBT scenarios.

We emphasize that while we believe that the insight that faster sampilng translates to faster bug finding
is a cross-langauge one, the specific technical contents of this section are OCaml-specific.
In the case of most other PBT frameworks \cn, the randomness library used operates on by machine
integers, so this \emph{particular} inefficiency does not exist.

The precise details of how the SplitMix algorithm works \cn are unimportant for the
present paper, but the critical component is that all of its operations are are
defined in terms of arithmetic bitwise operations on 64-bit integers. In OCaml,
because of details related to the garbage collector, the 64-bit integer type \texttt{int64} is represented
at run time as a \emph{pointer} to an unscanned block of memory containing (among other things)
a 64-bit integer \cn. This means that all operations that return an \texttt{int64} must allocate this block of memory.
This has a significant impact on the performance of generators. A single call to
one of the \bq library functions --- like generating an integer uniformly in a range --- may call the \texttt{splittable\_random}
algorithm multiple times. Each sample from \texttt{splittable\_random} allocates 9
times \jwc{this is a call to \texttt{next\_int64}}, and each allocation brings us closer
to the next garbage collection pause. While small allocations like these are \emph{very} fast to perform and subsequently collect in OCaml,
\footnote{The OCaml GC is a generational collector \cn, and since these allocations are small and mostly very short lived, they will all be minor allocations, never to be promoted.}, we will see in Section~\ref{section:eval} that this can have
a large performance impact on some generators that spend most of their time sampling data.
To circumvent this allocation and provide an equivalent version of \texttt{splittable\_random}, we
reimplement SplitMix in C, and call out to it with the OCaml FFI. The C version of the library uses
proper \texttt{int64\_t} arithmetic, only boxing and unboxing integers at the call boundaries between OCaml and C code.
Ideally in the future, one would not need to call out to C for this: the Jane Street bleeding-edge OCaml compiler has support
for unboxed types \cn, which (among other things) would let us implement a version of SplitMix that does not allocate, directly
in OCaml. Unfortunately, the Jane Street branch of the compiler is incompatible with MetaOCaml, which we use to implement the metaprogramming
discussed in the previous sections.

\section{Evaluation}
\label{section:eval}
\begin{itemize}
  \item \textbf{RQ1}: Do generators written using \name run faster than those written using base quickcheck?
  \item \textbf{RQ2}: Do performance speedups translate to better bug-finding ability?
  \item \textbf{RQ3}: Does our technique generalize to other strict functional languages?
\end{itemize}

All experiments were run on a 64-bit Linux machine with 264 GB RAM and an Intel Xeon Platinum 8375C CPU, running Ubuntu 24.04.1 LTS.
The \name library was implemented using the
4.14.1+BER MetaOCaml compiler variant.  All OCaml benchmarks
were run invoking \texttt{ocamlopt}, the native code compiler for OCaml, using compiler flags \texttt{-flamda -o3}.

. \todo{Joe : Scala info ...}

\subsection{Implementing and Testing Generators}
\begin{figure}[h]
  \includegraphics[scale=0.34]{allocs.png}\includegraphics[scale=0.334]{time.png}
\end{figure}
\label{subsection:pbt-for-pbt}
To establish the equivalence of generators up to program equality, 
we employ a PBT harness to compare their outputs over 1,000 random seeds. 
If a discrepancy arises, we report the unequal trees; if no counterexample is found,
we conclude that the generators exhibit equivalent behavior.
\subsection{Benchmarking speed \& resource usage}

\jwc{NOTE: we should test generator speed across both languages, but speed -> bugfinding ability in only OCaml.}
\jwc{
  Baseline generators to test speed in both languages:
  \begin{itemize}
    \item Single int
    \item Pair of ints, constrained
    \item List of ints without bind (use map): lots of sampling, minimal binds.
    \item Unableled Trees of a fixed size (no weighted union) minimal sampling, lots of binds.
  \end{itemize}
}
\subsection{Impact on bug-finding ability}
To answer \textbf{RQ2}, we evaluate the bug-finding ability of three of our case studies---BST, RBT, and STLC---using Etna, a platform for assessing and comparing property-based testing (PBT) techniques. 
Etna allows users to measure the effectiveness of different generator implementations by injecting bugs into the system under test and recording the time taken for a relevant 
property to fail in response. For example, in a tree with an ordering property, a bug injection might consist of inserting a value into the tree that violates that ordering. 

Each case study includes a diverse set of \textit{tasks}, where a task consists of a specific bug-property pair designed to test different aspects of the system (e.g., BST
includes tasks that test insertion, deletion, and union operations on binary search trees).

We evaluate our case studies with both type-derived and custom generators.
Each generator has four implementations, which are equivalent up to program equality:
\begin {itemize}
  \item The original \bq generator (``BQ'').
  \item A staged version of the generator (``Staged'').
  \item A staged version of the generator using the C \rand (``Staged C'').
  \item A staged version incorporating the CSR \rand (``Staged CSR'').
\end{itemize}

To ensure a fair comparison, we initialize each of these generators with the same random seed, ensuring that they produce identical trees.

\subsubsection{Variance between seeds}
The time-to-failure of a given strategy on a given seed is not necessarily representative of 
the strategy's average time-to-fail over a large number of trials, so we normalize by computing the
timing \textit{differential} between optimized and unoptimized generators (the ``speedup''). While this works in most cases,
it is not perfect.
For example, it is theoretically possible
to choose a seed such that the \textit{first} value produced by a generator discovers the bug, which would elide any differential speedup (and likewise
if both generators fail to find the bug).
To account for these edge cases, we repeat the above process for 10 seeds and compute the geometric mean of their speedups.

% To produce \todo{FIG}, we repeated the above process for 10 seeds and computed the geometric average of their speedups. 
%This is important because the time-to-failure of a given strategy on a given seed is not necessarily 
%representative of the strategy's overall bug-finding ability, although it often is. For example, it is theoretically possible 
%to choose a seed such that the \textit{first} value produced by a generator discovers the bug, but it would be misleading to report this result.

\subsubsection{Variance between trials}
Although the variance \textit{between} seeds in Etna can be very vast, timing results are replicable for a given seed. Across 1,000 trials on the same seed, 
the observed variance in time-to-failure was less than a nanosecond. \tr{Do we need this?}


\section{Other Languages and Libraries}
\label{subsection:other-langs}

\begin{figure}[h]
  \centering
  \vspace{0.5em} % Kept the vertical spacing
  \begin{tabular}{lrl}
  \toprule
  Benchmark & \multicolumn{2}{c}{avg. time (ns/op) $\pm$ std. dev.} \\
  \midrule
  BST & $3,560.813$ & $\pm\>1,511.201$ \\
  BST, Staged & $751.287$ & $\pm\>39.890$ \\
  BST (Type-Derived) & $44,619.863$ & $\pm\>5,761.568$ \\
  BST (Type-Derived), Staged & $6,863.209$ & $\pm\>416.188$ \\
  RBT (Type-Derived) & $60,559.846$ & $\pm\>4,901.980$ \\
  RBT (Type-Derived), Staged & $8,511.528$ & $\pm\>324.615$ \\
  STLC & $44,497.623$ & $\pm\>1,970.484$ \\
  STLC, Staged & $7,151.259$ & $\pm\>355.639$ \\
  \bottomrule
  \end{tabular}
 \caption{\todo{Understand Joe's implementation of this...}}
 \label{fig:benchmark-table}
 \end{figure}

\jwc{TODO: Move this section later (cf conversation in WH meeting 3/7/25)}

\tr{Here's where we talk about what's going on in the world, and ultimately make the argument that \bq{} is the best tool to reproduce.}

\jwc{Note that this explanation require some prior note of exactly *why* BQ is slow... i.e. the abstraction overhead of the library is high.}

\jwc{
  \begin{itemize}
    \item Scala. Functional abstractions like QC generators are known to be costly in Scala, that's why they have LMS (in Scala 2, and Macros in Scala 3). Example: parser combinators (``On Staged Parser Combinators for Efficient Data Processing''), functional data structures (\href{https://ppl.stanford.edu/papers/popl13_rompf.pdf}{Link}), web programming (``Efficient High-Level Abstractions for Web Programming'').
    Al of this should still work in scala. Could easily be incorporated into ScalaCheck, with minor modification: ScalaCheck uses a state monad to thread around the seed, instead of a stateful one (like BQ), or a splittable one (like Haskell). So you have to adapt to that. But same diff.
    \item Python. Maybe could do it?? Hypothesis + MacroPy
    \item Haskell: GHC does a lot of these optimizations already, since the code is pure. Since QC generators are relatively small programs,
    GHC has little trouble specializing them. Of course, this is not guaranteed. A version of this idea can easily be ported to the original QC with template haskell, to guarantee
    the highest-performance generators.
    \item Rust: Not GC'd, so no alloc overhead but bind'd generators still dispatch through run-time data.
  \end{itemize}
}

\section{Related Work}
\jwc{A generator is a parser of randomness, and people have implemented lots of staged parser libraries!}

\section{Conclusion \& Future Work}
\jwc{Why we stopped where we stopped: can you squeeze more out of this?}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}


%%
%% If your work has an appendix, this is the place to put it.
\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
