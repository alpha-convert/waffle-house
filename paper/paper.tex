\documentclass[sigplan,screen,acmsmall,anonymous,review]{acmart}%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\newif\ifdraft\drafttrue{}
\newif\iflater\latertrue{}

\PassOptionsToPackage{names,dvipsnames}{xcolor}

\settopmatter{printfolios=false,printccs=false,printacmref=false}
\setcopyright{none}

\usepackage{listings}
\usepackage{subcaption}

\lstset{
  mathescape=true,
  frame=none,
  xleftmargin=10pt,
  stepnumber=1,
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  % language=haskell,
  keepspaces=true,
  tabsize=2,
  emphstyle={\bf},
  % commentstyle=\it\color{dkgreen},
  stringstyle=\mdseries\ttfamily,
  showspaces=false,
  keywordstyle=\bfseries\ttfamily,
  columns=flexible,
  basicstyle=\footnotesize\ttfamily,
  showstringspaces=false,
  % morecomment=[l]\%,
  % moredelim=**[is][\color{dkgreen}]{@}{@}
}

\include{macros}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newtheorem{claim}{Claim}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2025/02}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Fail Faster}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Cynthia Richey}
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}
}

\author{Joseph W. Cutler}
\affiliation{
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}
}

\author{Harrison Goldstein}
\affiliation{%
  \institution{University of Maryland}
  \city{College Park}
  \state{Maryland}
  \country{USA}
}

\author{Benjamin C. Pierce}
\affiliation{%
 \institution{University of Pennsylvania}
 \city{Philadelphia}
 \state{Pennsylvania}
 \country{USA}}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Richey and Cutler et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract} \tr{Abstract needs a lot of work.}
Property-based testing (PBT) is a software testing approach that verifies a system against a large suite of automatically-generated inputs. Since testing pipelines typically run under strict time constraints, PBT generators must produce inputs as quickly as possible to maximize the likelihood of finding bugs in the time available. However, existing PBT libraries often prioritize generality at the cost of performance. We introduce \texttt{waffle\_house}, a high-performance generator library that uses staging, a lightweight compilation technique, to eliminate many common generator abstractions. To evaluate \texttt{waffle\_house}, we design a novel benchmarking methodology that compares generators based on program equivalence, isolating performance improvements from differences in input distribution. Using this methodology, we compare \texttt{waffle\_house} to a leading generator library, and, through extensive evaluation over a diverse range of generators, demonstrate that \texttt{waffle\_house} significantly improves generation speed and resource efficiency while matching \texttt{base\_quickcheck}'s expressiveness.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007</concept_id>
       <concept_desc>Software and its engineering</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006</concept_id>
       <concept_desc>Software and its engineering~Software notations and tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011072</concept_id>
       <concept_desc>Software and its engineering~Software libraries and repositories</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011041.10011046</concept_id>
       <concept_desc>Software and its engineering~Translator writing systems and compiler generators</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering}
\ccsdesc[500]{Software and its engineering~Software notations and tools}
\ccsdesc[500]{Software and its engineering~Software libraries and repositories}
\ccsdesc[500]{Software and its engineering~Translator writing systems and compiler generators}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Property-based testing, Generators, Staging, Meta-programming}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{--}
\received[revised]{--}
\received[accepted]{--}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\jwc{This paper is about ``Strict functional languages like OCaml and Scala'', but we'll focus on OCaml for presentation.}

\bcp{The current introduction says, roughly, ``We took a look at
  base.quickcheck, noticed some inefficiencies, applied known
  techniques from staged metaprogramming to eliminate them, and
  observed that speed went up.''  I think we can make much stronger
  claims, but I'm not sure about exactly how strong or what belongs in
  the foreground...
  \begin{itemize}
  \item One of the first serious uses of staging in anger?
  \item A use of staging that is novel / challenging in
  itself, in some way?
  \item An analysis of sources of inefficiency across a range of PBT
  frameworks (and a solution that applies to many of them)?
  \item A usable tool that addresses and overcomes some significant
  implementation challenges?
  \item Careful measurements showing where the sources of inefficiency
  are in existing PBT tools, and which ones matter most?
  \item (Your claim here...)?
  \end{itemize}
}

\jwc{Agree: I think something like the 3rd is the best reframing. Something
like: ``We demonstrate that, across languages, monadic PBT generator DSLs can
have a significant performance overhead, and present a cross-language technique
for eliminating the overhead while preserving the idiomatic abstraction.
''}

\jwc{People have been using staged metaprogrammign to eliminate abstraction
overheads in parsing for a long time, we turn that lens on generation (Which looks like parsing)}

\jwc{Another angle that I'd argue needs to come out more is that we demonstrate that
engineering PBT libraries with performance in mind has significant impact on testing power.
}

\jwc{
Let's lead on with the quote from PBT in practice: performance engineering
matters.  De-emphasize the abstraction overhead framing. Eliminating abstraction
overhead is important, so is using the fastest RNG you can possibly use.
}

\jwc{``Property based testing is a race against time...''}

Property-based testing (PBT) is a widely-used testing framework\hg{PBT isn't a
framework, it's a ``technique'' or ``approach'' --- Base\_QuickCheck is a framework} consisting of
two key components: a set of \textit{properties} that a system must satisfy, and
a way of producing
a large number of \textit{inputs} to that system. In contrast to traditional
unit testing, where test inputs are hand-written, PBT users create a random input
\textit{generator} that automatically produces inputs. Sometimes, this process
can itself be automated through the use of PBT libraries that synthesize
generators from type definitions; however, when inputs are required to maintain
invariants not communicated by the type, users must write a custom generator (or
filter out a broad class of invalid inputs).

\hg{I think this paragraph buries the lede. I'd start the paragraph talking
about performance and citing the 50ms number, and then jump right into what's
slow.}
Writing ``good'' generators—ones that successfully uncover bugs in the system under
test—is challenging. Much effort has been dedicated to this problem, often by
developing sophisticated domain-specific languages for specifying constrained
generators. However, these approaches do not always reflect the needs of real
PBT users. Under ideal conditions, a PBT suite would continue to run
until it stopped finding bugs, real-world usage patterns are very different; a
study of expert PBT users reported time budgets of between 50 milliseconds and
30 seconds for their test suites~\cite{inpractice}. Consequently, generation
time is a significant factor in PBT efficacy---that is, if a generator can
produce inputs twice as fast, it has twice as many chances within a given time
bound to find a bug. Therefore, it is imperative that PBT libraries are built
with performance in mind.

To better understand and address performance challenges in the landscape of PBT
generators, we explore popular libraries across several widely-used programming
languages.\hg{Again, this sentence also starts the paragraph too slowly; rather than
focus on our exploration (which isn't really the point of the paper) lead with
the result --- that abstraction overhead, boxing/unboxing, and allocations slow
down QuickCheck ports} We find that the flexibility of these libraries comes at a cost to 
performance,
introducing abstraction overhead, frequent boxing and unboxing, and avoidable
allocations (which lead to costly garbage collection pauses). We focus primarily 
on OCaml's \bq, selected for its efficiency relative to other OCaml PBT libraries 
and its integration with industrial build systems\hg{Let's not try to justify
our choice of OCaml \bq here --- we should just talk about strict functional
languages and justify this choice later}, where it is expected to run 
within tight time constraints. These
inefficiencies raise a fundamental question: how can PBT libraries produce code
that is both efficient \textit{and} general? By analyzing
\bq\ and similar libraries in other languages \tr{Rust?
More?}, we provide a broader perspective on common performance challenges in PBT
and explore potential strategies for more efficient generator implementations.

Our solution is \name, a generator library\hg{Can we say ``an approach to PBT
generator libraries'' or something more general like that?} that preserves
\bq’s functionality while improving performance through
\textit{multi-stage programming} (staging). Staging is a lightweight,
domain-specific compilation technique that allows us to completely eliminate the
runtime overhead of many common generator abstractions. \jwc{This relies on the fact that the code of generators is known statically at compile time} For instance, in
\name, monadic operations are zero-cost.
\name’s staged eliminates unnecessary allocations,
generates static control structures, and leverages unboxed integer libraries to
achieve performance improvements over \texttt{base\_quickcheck}, making it
well-suited to real-world PBT applications.

We \hg{also} present a new methodology for evaluating the relative effectiveness of PBT
generators.\hg{Might be an over-claim, we'll need to write this carefully}
Prior work has compared generators based on bug-finding ability: 
generators are considered equally effective if they find bugs in a
system at the same rate over a large number of trials ~\cite{etna}. However,
because \name\ preserves the semantics of
\bq, a \name\ generator produces exactly
the same inputs as its \bq\ counterpart. By comparing
generators based on \textit{program equality}, we ensure that any speedups in
bug-finding ability stem from performance enhancements rather than variations in
input distribution. This approach establishes a foundation for benchmarking
optimizations that enhance performance without compromising expressiveness.
\hg{I'm not totally convinced by this paragraph. Here's how I'd explain this:\\
In order to justify the switching cost associated with using \name, we designed
\name\ to be incredibly easy to benchmark: most PBT generator optimizations make
changes up to {\em distributional equivalence}, allowing the precise sampling
order to differ as long as the probability distribution is maintained, but
\name's optimizations are correct up to {\em program equivalence}. This means
that a \name\ generator produces precisely the same values in the same order as
the original generator, only faster. With this setup, benchmarking becomes
significantly easier than for prior work---our measurements have incredibly low
variance, and performance gains from \name\ translate directly to improved
bug-finding speed.}

Finally, we conduct an extensive evaluation, implementing both type-derived and
custom generators in \name\ and \bq. Our
generators produce a diverse range of inputs, including recursive data
structures, lambda calculus terms, and regular expressions. We benchmark
generation time, resource usage, and time-to-failure for each pair of staged and
unstaged generators, demonstrating that \name\ achieves
significant gains in speed and resource.

In summary, we dramatically improve the bug-finding effectiveness of PBT generators
by optimizing them for speed. Specifically, we make the following contributions:
\begin{enumerate}
    \item An empirical analysis of the sources of inefficiency in PBT
    generators. \jwc{And an argument that PBT generator libraries should be
    engineered for performance}
    \hg{Let's talk about this, I'm not sure I buy it}
    \item A library, \name, which offers efficient \jwc{performant?} generator \proposechange{functions}{combinators} through the
    use of multi-stage programming; this is the first known application of
    staging to PBT.\hg{Let's also talk about this}
    \item The insight that generators should be compared by program equality
    \item An evaluation demonstrating the improved performance of generators constructed
    using our library in a controlled comparison.
\end{enumerate}

\jwc{
  Things to ensure people come away with answers to:
  \begin{itemize}
    \item Is there generality?
    \item Is this important?
    \item Did they actually solve the problem?
  \end{itemize}
}

\section{Background}

\jwc{In this paper, we'll use OCaml to illustrate the ideas, but the concepts
are portable to other languages: see Section~\ref{subsection:other-langs}}

\subsection{Property-Based Testing}
\jwc{Usual spiel about pbt and generators: here's the BQ generator library, it has return and bind, this is what they're for.}

\begin{lstlisting}
module Bq = struct
  type 'a t = int -> SR.t -> 'a

  let return (x : 'a) : 'a t = fun size seed -> x

  let bind (g : 'a t) (f : 'a -> 'b t) : 'b t =
    fun size seed ->
      let x = g size seed in
      (f x) size seed

  let gen_int (lo : int) (hi : int) : int t =
    fun size seed -> SR.int seed lo hi
end
\end{lstlisting}

\jwc{\texttt{gen\_int} here is really \texttt{int\_uniform\_inclusive}. I also think we should completely omit named arguments in the paper so we don't confuse with splices.}

\jwc{Here's an example of a generator: you can sample from it and it gives you pairs of ints, one less than the other.}

\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  let%bind x = Bq.gen_int 0 100 in
  let%bind y = Bq.gen_int 0 x in
  return (x,y)
\end{lstlisting}

\jwc{This desugars to...}

\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  Bq.bind (Bq.gen_int 0 100) (fun x ->
    Bq.bind (Bq.gen_int 0 x) (fun y ->
      Bq.return (x,y)
  ))
\end{lstlisting}


\subsection{Multi-Stage Programming}
\label{subsection:msp}

\jwc{
  \begin{itemize}
    \item In Muli-stage programming (also known more simply as staging), programs execute over the course of multiple stages,
with each stage producing the code to be run in the next.
\item This is an old idea, with roots going back to quasiquotation in LISP, and picking up steam again in the 1990s with MetaML \cn{}.
\item For the purposes of this paper, we will only consider two stages: compile time and run time. Staged programs thus execute twice: once at compile time,
which produces more code, which is then compiled and run at run time.
\item Staging has many applications, but chief among them is its use for \emph{optimization}. We can write programs such that during the compile time stage, they are partially evaluated to eliminate
abstraction overhead. 
\item Many languages have some degree of staging functionality, either provided as a library \cn or build directly into the language's implementation \cn.
\item For this paper, we use MetaOCaml \cn for OCaml staging, but all of the staging concepts are portable to any other language with multi-stage programming functionality.
  \end{itemize}
}

\jwc{This section is going to need work.}

MetaOCaml's staging functionality is exposed through a type \texttt{'a code}. A
value of type \texttt{t code} at compile time is a (potentially open) OCaml term
of type \texttt{t}.

Values of \texttt{code} type are introduced by \emph{quotes}, written
\texttt{.<$\ldots$>.}. Brackets delay execution of a program until run time. For
example, the program \texttt{.< 5 + 1 >.} has type \texttt{int code}.  Note that
this is not the same as \texttt{.< 6 >.}. Because brackets delay computation,
the code is not \emph{executed} until the next stage (run time).
Values of type \texttt{code} can be combined together using \emph{escape},
written \texttt{.\~{}($\ldots$)}.  Escape lets you take a value of type
\texttt{code}, and ``splice'' it directly into a quote.  For example, this
program \texttt{let x = .<1 * 5>. in .< .\~{}(x) + .\~{}(x) >.} evaluates to
\texttt{.<(1 * 5) + (1 * 5)>.}.  MetaOCaml ensures correct scoping and macro
hygiene, ensuring that variables are not shadowed when open terms are spliced.

The power of staging for optimization away abstraction overheads comes from
defining functions that accept and return \texttt{code} values.  A function
\texttt{f : 'a code -> 'b code} is a function that takes a program computing a
run-time \texttt{'a} and transforms it into a program computing a run-time
\texttt{'b}. In particular, because \texttt{f} itself runs at compile time, the
abstraction of using \texttt{f} is necessarily completely eliminated by
run time.
A code-transforming function \texttt{'a code -> 'b code} can also be converted \emph{code for a function} --- a value of type
\texttt{('a -> 'b) code} --- with the following program:
\begin{lstlisting}
let eta (f : 'a code -> 'b code) : ('a -> 'b) code = .<fun x -> .\~(f .<x>.)>.
\end{lstlisting}

This program is known as ``the trick'' in the partial evaluation and multi-stage programming literature.
\cn{} 
% USE THIS: https://arxiv.org/pdf/2309.08207
It returns a code for a function that takes an argument \texttt{x}, and then it splats in the result of calling \texttt{f} on just
the quoted \texttt{x}.
\jwc{Does this make any sense?}

The trick is best illustrated by an example. The following program reduces to \texttt{.< fun x -> (1 + x) mod 2 == 0 >}.
\begin{lstlisting}
let is_even x = .< .~x mod 2 == 0 >. in
let succ x = .< 1 + .~x >. in
eta (succ . is_even)
\end{lstlisting}
By composing the two code-transforming functions together at compile time, and only then turning them into a run-time function,
the functions are fused together... \jwc{words}.
This is the basis of how staging is used to eliminate the abstraction of DSLs
(like a generator DSL). By writing DSL combinators as compile-time functions ---
and only calling \texttt{eta} at the end on the completed DSL program ---  we
can ensure that any overhead of using them is eliminated by run time.

% \jwc{
%   \begin{itemize}
%     \item The \texttt{'a code} type, quote, escape or ``splice'', stage distinction.
%     \item MetaOCaml ensures correct scoping and hygene by preventing alpha-collision when you splice open terms.
%     \item The difference between \texttt{('a -> 'b) code} and \texttt{'a code -> 'b code}
%     \item We can convert one way, but not the other.
%     \item Functions \texttt{'a code -> 'b code} ``fuse''.
%     \begin{itemize}
%       \item Consider writing \texttt{even . succ}. This
%       \item Consider \texttt{even\_c : int code -> bool code = fun cx -> .<.~cx mod 2 == 0>.} and \texttt{succ\_c : int code -> int code = fun cx -> .< .~x + 1 >.}
%       \item If you do \texttt{to\_dyn (even\_c . succ\_c)}, you get \texttt{fun x -> (x + 1) mod 2 == 0}. Composing functions from code to code, and then \emph{only at the end} stamping out
%       a dynamic function value eliminates the function abstraction.
%     \end{itemize}
%     \item This is the basis of (WORD). By defining a library with functions with types like \texttt{'a code -> 'b code}, we can ensure that the
%     abstractions the library introduces are fully eliminated at compile time.
%   \end{itemize}
% }

\subsection{Other Languages and Libraries}
\label{subsection:other-langs}

\jwc{TODO: Move this section later (cf conversation in WH meeting 3/7/25)}

\tr{Here's where we talk about what's going on in the world, and ultimately make the argument that \bq{} is the best tool to reproduce.}

\jwc{Note that this explanation require some prior note of exactly *why* BQ is slow... i.e. the abstraction overhead of the library is high.}

\jwc{
  \begin{itemize}
    \item Scala. Functional abstractions like QC generators are known to be costly in Scala, that's why they have LMS (in Scala 2, and Macros in Scala 3). Example: parser combinators (``On Staged Parser Combinators for Efficient Data Processing''), functional data structures (\href{https://ppl.stanford.edu/papers/popl13_rompf.pdf}{Link}), web programming (``Efficient High-Level Abstractions for Web Programming'').
    Al of this should still work in scala. Could easily be incorporated into ScalaCheck, with minor modification: ScalaCheck uses a state monad to thread around the seed, instead of a stateful one (like BQ), or a splittable one (like Haskell). So you have to adapt to that. But same diff.
    \item Python. Maybe could do it?? Hypothesis + MacroPy
    \item Haskell: GHC does a lot of these optimizations already, since the code is pure. Since QC generators are relatively small programs,
    GHC has little trouble specializing them. Of course, this is not guaranteed. A version of this idea can easily be ported to the original QC with template haskell, to guarantee
    the highest-performance generators.
    \item Rust: Not GC'd, so no alloc overhead but bind'd generators still dispatch through runtime data.
  \end{itemize}
}

\section{Why are Monadic Generator Libraries Slow?}
\jwc{Why is a monadic generator library slow?}
\jwc{NOTE: We can simplify this further by getting rid of the size parameter. Make the story even cleaner, I think!}

\jwc{IMPORTANT: We are using OCaml because we have to pick a syntax for this section, but these abstraction overheads exist in other languages -- at least scala, rust.}

\jwc{
  \begin{itemize}
    \item It's been long-known that clean functional abstractions have a runtime overhead (this should be familiar by this time in the paper).
    \item How does (simplified) BQ work?
    \begin{itemize}
      \item The basic generator type: \texttt{'a generator = int -> SR.t -> 'a}. Size and random seed to deterministic value. (note: \texttt{SR.t} is a mutable seed)
      \item This gets a monad intance in the obvious way (show code).
      \item Also show the code for \texttt{int}, how it calls the underlying SR function.
    \end{itemize}
    % \item Note that \emph{extensionally} \texttt{generate (create (fun size random -> e)) size random = e}, but the OCaml compiler does not always perform this optimization, or do the inlining required to expose it.
    % (When sufficiently obfuscated behind returns and binds ...) This program compiles to code that (1) allocates the closure for `e', (2) passes it to create (which returns the closure), and then calls (3) generate, which immediately jumps into the closure.
    \item Show benchmarks of the running example, versus the version where you inline everything. ()
    \item Let's look at the running example: inline it all the way.
  \end{itemize}
}

\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  Bq.bind (Bq.gen_int 0 100) (fun x ->
    Bq.bind (Bq.gen_int 0 x) (fun y ->
      Bq.return (x,y)
  ))

let int_pair_inlined : (int * int) Bq.t
  fun sr ->
    let x = Splittable_random.int sr ~lo:0 ~hi:100 in
    let y = Splittable_random.int sr ~lo:0 ~hi:x in
    (x,y)
\end{lstlisting}

\jwc{Show benchmark difference between these two generators: the benchmark is in \texttt{waffle-house/handwritten-ocaml/bin/basic-compare.ml}. It's about 2x. (see comment in latex here.)}

\jwc{The native code OCaml compiler, even with -O3, fails to specialize this code and eliminate the overhead of this abstraction --- inlining all of the funciton definitions and then performing beta-reductions speeds up sampling by a factor of 2.}

% ┌───────┬──────────┬─────────┬────────────┐
% │ Name  │ Time/Run │ mWd/Run │ Percentage │
% ├───────┼──────────┼─────────┼────────────┤
% │ bq    │  70.47ns │  78.00w │    100.00% │
% │ fused │  35.86ns │  78.00w │     50.89% │
% └───────┴──────────┴─────────┴────────────┘

\jwc{The overhead of the abstraction is 2x.
The reality is actually worse: actual base-quickcheck includes even more indirection in its type.}

\jwc{Demonstrate this in Scala too!!!}

\jwc{
The problem is plain: while the abstractions that PBT libraries like base quickcheck provide are indespensible for writing idiomatic generators, the performance overhead of using them is dramatic.
The compiler (and flambda) uses heuristics (\href{https://ocaml.org/manual/5.3/flambda.html}{here}) to decide when to specialize and inline code, and these heuristics cannot guarantee that generator abstractions are zero-cost.
These heuristics are also extremely conservative about inlining and specializing recursive functions, which all generators of recursive data types must be.
}

\tr{Each of these consist of an explanation of the problem and pseudocode outlining the solution.}
\subsubsection{Monadic DSL Abstraction Overhead}

\jwc{
  \begin{itemize}
    \item While it's the ``correct'' abstraction for generators, using a monadic interface prevents the compiler from specializing generator code. Using monadic bind and return obscures the control and data flow of a generator from the compiler. This is
    compounded when handling recursive functions.
    \item In cases where the compiler cannot statically eliminate them, running a monadic bind allocates a short-lived closure: we allocate a closure for the continuation, and then immediately jump into it.
    \item Each closure allocation is relatively cheap (they're going to be minor allocations since they're short-lived), but doing lots of allocation in a generation hot loop adds up fast.
    \item Similar things have been noticed about monadic parsing DSLs in the past.
  \end{itemize}
}

\subsubsection{Combinator Abstraction Overhead}
\jwc{Many combinators like \texttt{union} and \texttt{weighted\_union} needlessly allocate large data structures in the generation hot path.
Using a combination of staging and more careful algorithm design, we can avoid this overhead.
}
\jwc{Ensure this discussion is initially multi-language, but uses OCaml to demo.}

\begin{lstlisting}
  let of_weighted_list alist =
  let weights, values = List.unzip alist in
  let value_array = Array.of_list values in
  let total_weight, cumulative_weight_array =
    let array = Array.init (Array.length value_array) ~f:(fun _ -> 0.) in
    let sum =
      List.foldi weights ~init:0. ~f:(fun index acc weight ->
        let acc' = acc +. weight in
        array.(index) <- acc';
        acc'
      )
    in
    sum, array
  in
  create (fun ~size:_ ~random ->
    let choice = Splittable_random.float random ~lo:0. ~hi:total_weight in
    match
      Array.binary_search
        cumulative_weight_array
        ~compare:Float.compare
        `First_greater_than_or_equal_to
        choice
    with
    | Some index -> value_array.(index)
    | None -> assert false
   )
\end{lstlisting}

\jwc{This builds the cdf of the distribution, samples from 0 to the total, and binary searches through the array to find the bucket.}
\jwc{Allocating this array is unnecessary: we can simply compute the total weight, sample \texttt{x} between 0 and the total, and then walk the list accumulating the sum, until the accumulator exceeds \texttt{x}. Since in practice the list of possible options is quite small (usually at most 10 in practice), the linear time scan is going to be much faster.}
\jwc{(The point of this para will just be to emphasize that it's almost always better to be allocation-aware, instead of algorithmically clever.)}

\jwc{Moreover, the possible options is almost always statically known in practice: you use weightedunion to generate (say) a datatype with possible
variants, whose options are known at compile time.
Put more simply, you basically always call weighted union with an \emph{explicit list}: For this reason, you should not have to incur the cost of allocating this list at runtime.}

\jwc{ofc, sometimes you only know the list at runtime (see the STLC generator for an example), so you the library also includes this.}

% \subsubsection{Function call overhead}
\subsubsection{Choice of RNG}

\jwc{
  \begin{itemize}
    \item Because the RNG sits at the heart of the generation hot path, RNG
    speed matters enormously. Which RNG you pick matters...

    \item Different PBT libraries use different RNGs --- QuickCheck and \bq use
    splittable_random, Scalacheck uses \href{https://burtleburtle.net/bob/rand/smallprng.html}{this one}.  These
    are reasonably performant RNGs, but faster ones exist. Because this is the
    core of the hot path of the generator, speeding up sampling just a little can
    can dramatically speed up generation time.
    \item Significantly faster RNGs that still pass pRNG benchmarks like BigCrush exist (like \href{https://github.com/lemire/testingRNG/blob/master/source/lehmer64.h}{this one}), and using them can 
    speed up generation enormously.

    \item However, showing this
    is tricky, because different RNGs have different sampling orders, but our
    eval is made much stronger by the fact that all of our generators are functionally equivalent.

    \item In this paper demonstrate this fact by noticing a ``natural experiment'': OCaml's splittable random library is
    slow in a way that can be improved \emph{without} changing the extensional behavior. By resolving this issue, we can demonstrate
    just how much you can improve bug-finding performance simply by speeding up the RNG.

    % To illustrate this, we demonstrate a particular inefficiency of
    % OCaml's BQ, resolve it in a later section, and then show how much you can
    % bug-finding performance just by speeding up and RNG.
  \end{itemize}
}

\jwc{The inefficiency of BQ's RNG presents a ``natural experiment'' (in the
sense of economics): we can demonstrate how using a faster RNG can improve
bugfinding time, but while maintainign program equivlanece. (This should go in the last paragraph here...)}

The core of any PBT generator library is a random-number generator. Following
the original Haskell QuickCheck, \bq uses the SplitMix algorithm \cn,
implemented as an OCaml library called \texttt{Splittable\_random}. The precise
details of how SplitMix works are unimportant for the present paper, but the
main operations are defined in terms of bitwise operations on 64-bit integers.
However, due to implementation details related to the garbage collector, all
runtime OCaml values are either (a) pointers to a block of memory, or (b) 63-bit ``immediate'' values.
As such, the OCaml type of 64-bit ingeters, \texttt{int64}, is \emph{boxed}: it
is a pointer to a block of memory that contains a 64-bit integer.  This means
that \emph{all} \texttt{int64} operations (both arithmetic and bitwise) must
allocate memory cells to contain their output.

This has a significant impact on the performance of generators. A single call to
one of the \bq library functions may sample from \texttt{Splittable\_random}
multiple times, and each sample from \texttt{Splittable\_random} allocates 9
words \jwc{this is a call to \texttt{next\_int64}}.  While seemingly small, we
will see in Section~\ref{section:eval} that this amount of allocation has a
significant impact on significant.

In Section~\ref{subsection:faster-rng}, we show how our library mitigates this
performance hit by using OCaml's FFI to reimplement the RNG in non-allocating C code.
We also discuss how the ``unboxed types'' extension to the OCaml compiler \cn could instead be used
to the same effect without leaving OCaml.

It is worth noting that the inefficiency of boxed \texttt{int64}s is not due to
the abstraction overhead of the generator DSL itself, but simply due to language
implementation and algorithm choices. We include this discussion --- and
the corresponding evaluation
--- to demonstrate the testing-time savings of taking seriously the notion that PBT libraries
should be engineered for performance.

\jwc{If you're not using the fasteest RNG possible, you're leaving RNG on the table.}

\section{Eliminating Abstraction Overhead of Generator DSLs by Staging}
\jwc{Emphasize that this is an \emph{COMPLETELY EQUIVALENT drop in replacement!} We din't just build a different library with different distributions.}

\jwc{Usual introduction to this section, corresponding to how we talked about it in the intro. ``We present a library that XYZ''.}

\subsection{Basic Design}

Recall that staging a DSL requires carefully changing some of the types and
combinators to transform \texttt{code}s.  Deciding which types \texttt{t} can
instead be \texttt{t code} is an art known as ``binding-time analysis'' \cn:
figuring out which parts of the DSL can be determined statically (and can be
part of the compile-time stage), and which parts are only known dynamically (and
hence must be \texttt{code}).

The crux of our binding time analysis is that the particular random seed and
size parameter \jwc{if explained} are only known at run time (the later stage),
but the code of the generator itself is known at compile time. Generators ---
values of type \texttt{'a Bq.t} --- are never constructed dynamically at run
time in practice, so all of the combinators we use to build them can run at compile time.

This means our library's generator type \texttt{'a
Gen.t} should be \texttt{int code -> SR.t code -> 'a code}: a compile-time
function from dynamically-known size and seed to dynamically-determined result.

This type, along with basic monadic generator DSL functionality can be found in Figure~\ref{fig:gen-staged-basic}.
The monadic interface is given by a return and bind, as usual.
\texttt{return} is the constant generator, but this time it operates on codes. Given \texttt{cx : 'a code}, the code for a \texttt{'a}, it returns the generator which always generates that value.
\texttt{bind g k} sequences generators by passing the result of running the generator \texttt{g} to a continuation \texttt{k}. However, instead of getting access
to the particular value generated by \texttt{g}, the continuation \texttt{k} gets access to \texttt{code} for the value sampled from \texttt{g}:
at compile time, we only know \texttt{g} will generate \emph{some} \texttt{'a}, but not which one.
Operationally, bind takes code for the size and seed, and returns code that (1) let-binds a variable \texttt{a} to spliced-in code that runs \texttt{g}, and then
(2) runs the spliced-in continuation \texttt{k}.
Both function applications \texttt{g size\_c random\_c}
and \texttt{k .<a>. size\_c random\_c} run at compile time.
\texttt{Gen.int} is the generator that samples an int from the RNG.
Given any size and random seed, it returns a code block that calls \texttt{SR.int} with that random seed. Because the lower and upper bounds
might not be known at compile time --- they may themselves be the results of calling \texttt{Gen.int} ---
the arguments \texttt{lo} and \texttt{hi} are of type \texttt{int code}, and get spliced into the code block as arguments to \texttt{SR.int}.
Lastly, \texttt{to\_bq} turns a staged generator into code for a normal \bq
generator. This function is just a 2-argument version of ``The Trick'' (\texttt{eta}
from Section~\ref{subsection:msp}).


\begin{figure}
\begin{lstlisting}
module Gen = struct
  type 'a t = int code -> Random.t code -> 'a code

  let return (cx : 'a code) : 'a t = fun size_c random_c -> cx

  let bind (g : 'a t) (k : 'a code -> 'b t) : 'b t =
    fun size_c random_c ->
      .<
        let a = .~(g size_c random_c) in
        .~(k .<a>. size_c random_c)
      >.

  let int (lo : int code) (hi : int code) : int t =
    fun size_c random_c ->
      .< SR.int .~random_c .~lo .~hi >.

  let to_bq (g : 'a code Gen.t) : ('a Bq.t) code =
  .<
    fun size random -> .~(g .<size>. .<random>.)
  >.
end
\end{lstlisting}
\caption{Basic Staged Generator Library}
\ref{fig:gen-staged-basic}
\end{figure}

Returning to our running example, we can define a staged generator for pairs of ints

\begin{lstlisting}
let int_pair_staged : ((int * int) code) Gen.t =
  Gen.bind (Gen.int .<0>. .<100>.) (fun cx ->
    Gen.bind (Gen.int .<0> cx) (fun cy ->
      Gen.return .<(.~cx,.~cy)>.
    )
  )
let int_pair : (int * int) Bq.t code = Gen.to_bq int_pair_staged
====
.<
  fun size random ->
    let x = SR.int random 0 100 in
    let y = SR.int random 0 x in
    (x,y)
.>
\end{lstlisting}

\jwc{Changing some identifier names }

\jwc{So if you write with the \texttt{Gen} combinators and then only call \texttt{to\_bq} at the end, you get something that inlines everything all the way down!}

\subsection{Staging More Combinators}

\jwc{Weighted unions: recall that we should not need to materialize}

% let rec genpick n ws =
% match ws with
% | [] -> { rand_gen = fun ~size_c:_ ~random_c:_ -> Codecps.return .< failwith "Fell of the end of pick list" >. }
% | (k,g) :: ws' ->
%       { rand_gen = 
%         fun ~size_c ~random_c ->
%           Codecps.bind (Codecps.split_bool .< Float.compare .~(v2c n) .~(v2c k) <= 0 >.) (fun leq ->
%             if leq then
%               g.rand_gen ~size_c ~random_c
%             else
%               Codecps.bind (Codecps.let_insertv .< .~(v2c n) -. .~(v2c k) >.) @@ fun n' ->
%               (* let%bind n' =  in *)
%               (genpick n' ws').rand_gen ~size_c ~random_c
%         )
%       }

\begin{lstlisting}
  let weighted_union (weighted_elts : (float code * 'a code) list) : 'a t =
    let sum_code = List.foldr _ in
    let pick (rc : int code) (weighted_elts : (float code * 'a code) list) : 'a code =
      match weighted_elts with
      | [] -> .< failwith "Error" >.
      | (wc,xc) :: elts' ->
        .<
          if .~rc <= .~wc then .~xc
          else
            let r' = .~rc - .~wc in
            .~(go .<r'>. elts')
        >.

    fun size_c random_c ->
      .<
        let sum = .~sum_code in
        let r = SR.int .~random_c 0 sum in
        .~(pick .<r>. weighted_elts)
      >.
\end{lstlisting}

Then, if we write

\begin{lstlisting}
let grades : charBq.t gen = to_bq (
  weighted_union [
    (.<3.0>., .<'a'>.);
    (.<2.0>., .<'b'>.);
    (.<1.0>., .<'c'>.);
  ]
)
===
Bq.create (
  fun size random ->
    let sum = 3.0 + 2.0 + 1.0 in
    let r = SR.int random 0 sum in
    if r <= 3.0 then 'a'
    else
      let r' = r - 3.0 in
      if r' <= 2.0 then 'b'
      else
        let r'' = r' - 2.0 in
        if r'' <= 1.0 then 'c'
        else
          failwith "Error"
)
\end{lstlisting}

\subsection{Effect Ordering, and Equivalence is Program Equality}
\jwc{``Wait, why was the defintion of bind like that''?
Careful readers might note that the definition of bind was a little strange. In particular, why not just do this?}
\begin{lstlisting}
  let bind (g : 'a gen) (k : 'a code -> 'b gen) : 'b gen =
    fun size_c random_c ->
      k (g size_c random_c) size_c random_c
\end{lstlisting}
\jwc{The answer is that this doesn't generate the right code! In particular, this definition of bind means that
\texttt{bind (int 0 1) (fun x -> return .<(.~x,.~x)>.)} would generate \texttt{fun size random -> (int random 0 1, int random 0 1)},
which is \emph{not} the desired behavior. (Cite cbn/cbv randomness here).
A splice is a (hygenic) syntactic replacment, so
if a function \texttt{f : 'a code -> 'b code} splices its argument multiple times into its output, the argument code will be directly copied in.
This is why we insert a let-binding in the definition of bind, to ensure that
the explicit sequencing of bind is preserved into explicit sequencing in the
generated code.
}

\jwc{
The library is carefully design to preserve exactly the effect order of BQ. If
you write the same generator in both, they run identically: for the same size
and random seed, they produce the same value.
}

\jwc{Note: tradeoff between maintaining the effect ordering and even more performance.}

\bcp{How much faster could we go if we changed the effect ordering?
  If it might be a lot, then should we measure this too?}

\subsection{Size Parameters, A Real Monad Instance, CPS, and Recursion}

\subsubsection{Size Parameters}
\jwc{Introduce the size parameter here. (if you deleted it beforehand.)}

\subsubsection{CodeCPS and a Monad Instance}
\jwc{
  \begin{itemize}
    \item Careful readers might note that the types of return and bind are not actually the right types.
    \item This means that they aren't compatible with ppx-jane syntax.
    \item We could try a different type, \texttt{type 'a gen = int code -> SR.t code -> 'a}. This has an actual monad instance (show), and ou can write teh combinators of before, with \texttt{'a code gen} everywhere you had \texttt{'a gen} before.
    \item But because the result type isn't always in \texttt{code}, \texttt{bind} can't perform the let-insertion needed to preserve effect ordering.
    \item Instead, we turn to a classic technique from the multistage programming literature: writing our staged program in continuation-passing style.
    \item We define \texttt{type 'a codecps = 'z. ('a -> 'z code) -> 'z code}. This is the polymorphic CPS monad, with the continuation type always in `code'. (Footnote: this is an instance of the \emph{codensity} monad, cf ``Asymptotic Improvement of Computations over Free Monads'')
    (Cite: ``improving binding times without explicit cps conversion'', ``Multi-stage programming with functors and monads: Eliminating abstraction overhead from generic code'', and ``Closure-Free Functional Programming in a Two-Level Type Theory'')
    \item This lets us define the function \texttt{let\_insert : 'a code -> 'a codecps} as \texttt{let\_insert cx = fun k -> k .< let x = .~cx in .~(k .<x>.) >.}.
    \item of course, \texttt{codecps} has the usual monad instance for the CPS monad with a polymorphic result type.
    \item We then define \texttt{type 'a gen = int code -> SR.t code -> 'a codecps}, which lets us have it both ways. the effectful programs like \texttt{int} do their \emph{own} let-insertion, and we get a real monad instance.
  \end{itemize}
}

\jwc{This gives us the final definitions:}

\begin{lstlisting}
module CodeCps = struct
  type 'a t = { cps : 'z. ('a -> 'z code) -> 'z code }

  let return x = {cps = fun k -> k x}

  let bind (x : 'a t) (f : 'a -> 'b t) : 'b t =
    {cps = fun k -> x.cps (fun a -> (f a).cps k)}

  let run (t : 'a code t) : 'a code = t.cps (fun x -> x)

  let let_insert (cx : 'a code) : 'a code t =
    {cps = fun k -> k .< let x = .~cx in .~(k .<x>.) >.}
end

module Gen = struct
  type 'a t = int code -> SR.t code -> 'a CodeCps.t

  let return (x : 'a) : 'a t = fun _ _ -> Codecps.return x

  let bind (g : 'a t) (f : 'a -> 'b t) =
    fun size random ->
      CodeCps.bind (g size random) (fun x ->
        (f x) size random
      )
  
  let int (lo : int code) (hi : int code) : int code t =
    fun size random ->
      let_insert .< SR.int .~random .~lo .~hi >.
end
\end{lstlisting}

\jwc {
And this has the desired effect that
\texttt{bind (int .<0>. .<1.>) (fun x -> return .<(.~x,.~x)>.)} generates
\texttt{fun size random -> let x = SR.int random 0 1 in (x,x)}
}


\subsubsection{Recursive Generators}
\jwc{Describe recursion and recurisve handles}

\subsubsection{The Trick}
\jwc{Describe split --- this section is approximately experts-only, but you can just point at the Andras paper and the things it cites.}

\subsection{Staged Type-Derived Generators}

\jwc{Todo: Thia}

\jwc{This is 3-stage metaprogramming! There's PPX time, compile time, and run time.}

\subsection{Fast Random Number Generators}
\label{subsection:faster-rng}

\jwc{
  \begin{itemize}
    \item Using the fastest RNG possible is important.
    \item In OCaml, the splittalbe random RNG is not as fast as it could be, because of the Ints issue.
    \item In this section, we describe how to resolve the ints issue by calling out ot the FFI. 
    \item All of this can be done with Jane Street bleeding-edge compiler extensions, though these are not yet compatible with MetaOCaml.
    \item Because different libraries 
  \end{itemize}
}

\section{Evaluation}

\subsubsection{Implementation}
\subsection{Benchmarking speed \& resource usage}

\jwc{NOTE: we should test generator speed across both languages, but speed -> bugfinding ability in only OCaml.}
\jwc{
  Baseline generators to test speed in both languages:
  \begin{itemize}
    \item Single int
    \item Pair of ints, constrained
    \item List of ints
  \end{itemize}
}
\subsection{Impact on bug-finding ability}
\section{Conclusion \& Future Work}
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}


%%
%% If your work has an appendix, this is the place to put it.
\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
