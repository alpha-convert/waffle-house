\documentclass[sigplan,screen,acmsmall,anonymous,review]{acmart}%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\newif\ifdraft\drafttrue{}
\newif\iflater\laterfalse{}

\PassOptionsToPackage{names,dvipsnames}{xcolor}

\settopmatter{printfolios=false,printccs=false,printacmref=false}
\setcopyright{none}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.17}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{wrapfig}
\lstset{
  mathescape=true,
  frame=none,
  xleftmargin=10pt,
  stepnumber=1,
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  % language=haskell,
  keepspaces=true,
  tabsize=2,
  emphstyle={\bf},
  % commentstyle=\it\color{dkgreen},
  stringstyle=\mdseries\ttfamily,
  showspaces=false,
  keywordstyle=\bfseries\ttfamily,
  columns=flexible,
  basicstyle=\footnotesize\ttfamily,
  showstringspaces=false,
  % morecomment=[l]\%,
  % moredelim=**[is][\color{dkgreen}]{@}{@}
}

\include{macros}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newtheorem{claim}{Claim}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2025/02}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Fail Faster}
\subtitle{\hg{TODO: Subtitle}}



%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Cynthia Richey}
\authornote{Both authors contributed equally to this work.}
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}
}

\author{Joseph W. Cutler}
\authornotemark[1]
\affiliation{
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}
}

\author{Harrison Goldstein}
\affiliation{%
  \institution{University of Maryland}
  \city{College Park}
  \state{Maryland}
  \country{USA}
}

\author{Benjamin C. Pierce}
\affiliation{%
 \institution{University of Pennsylvania}
 \city{Philadelphia}
 \state{Pennsylvania}
 \country{USA}}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Richey and Cutler et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  \hg{TODO: Abstract}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007</concept_id>
       <concept_desc>Software and its engineering</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006</concept_id>
       <concept_desc>Software and its engineering~Software notations and tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011072</concept_id>
       <concept_desc>Software and its engineering~Software libraries and repositories</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011041.10011046</concept_id>
       <concept_desc>Software and its engineering~Translator writing systems and compiler generators</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering}
\ccsdesc[500]{Software and its engineering~Software notations and tools}
\ccsdesc[500]{Software and its engineering~Software libraries and repositories}
\ccsdesc[500]{Software and its engineering~Translator writing systems and compiler generators}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Property-based testing, Generators, Staging, Meta-programming}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{--}
\received[revised]{--}
\received[accepted]{--}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{section:intro}

%\jwc{This paper is about ``Strict functional languages like OCaml and Scala'', but we'll focus on OCaml for presentation.}

%\bcp{The current introduction says, roughly, ``We took a look at
%  base.quickcheck, noticed some inefficiencies, applied known
%  techniques from staged metaprogramming to eliminate them, and
%  observed that speed went up.''  I think we can make much stronger
%  claims, but I'm not sure about exactly how strong or what belongs in
%  the foreground...
%  \begin{itemize}
%  \item One of the first serious uses of staging in anger?
%  \item A use of staging that is novel / challenging in
%  itself, in some way?
%  \item An analysis of sources of inefficiency across a range of PBT
%  frameworks (and a solution that applies to many of them)?
%  \item A usable tool that addresses and overcomes some significant
%  implementation challenges?
%  \item Careful measurements showing where the sources of inefficiency
%  are in existing PBT tools, and which ones matter most?
%  \item (Your claim here...)?
%  \end{itemize}
%

%\jwc{Agree: I think something like the 3rd is the best reframing. Something
%like: ``We demonstrate that, across languages, monadic PBT generator DSLs can
%have a significant performance overhead, and present a cross-language technique
%for eliminating the overhead while preserving the idiomatic abstraction.
%''}

%\jwc{People have been using staged metaprogrammign to eliminate abstraction
%overheads in parsing for a long time, we turn that lens on generation (Which looks like parsing)}

%\jwc{Another angle that I'd argue needs to come out more is that we demonstrate that
%engineering PBT libraries with performance in mind has significant impact on testing power.
%}

%\jwc{
%Let's lead on with the quote from PBT in practice: performance engineering
%matters.  De-emphasize the abstraction overhead framing. Eliminating abstraction
%overhead is important, so is using the fastest RNG you can possibly use.
%}

%\jwc{``Property based testing is a race against time...''}

Property-based testing (PBT) is a software testing technique which uses
random test inputs to validate logical specifications. Recent studies on PBT usage~\cite{inpractice} have shown that
practitioners often interleave PBT with their software development process, sometimes
testing their properties every time they save their code. This means that PBT is often given a very short time
window to find bugs --- e.g., between 50 milliseconds and 30 seconds~\cite{inpractice}.
In other words, the goal is to fail as fast as possible. \jwc{First cut at ``fail fast''}

Given these strict timelines, the impact of performance in PBT cannot be overstated. One important locus for performance improvement
is the \textit{generators} that produce random inputs to the properties under
test. These generators are written with the help of \textit{generator
libraries}, usually expressed as embedded domain-specific languages (eDSLs) %\cntxt{edsls} 
that provide combinators for building and composing generators. Optimizing these
libraries is a key opportunity to speed up PBT: if programmers are
provided with the tools to write fast generators, then they do not themselves
need to be experts in performance tuning to find bugs fast.

However, careful measurements show that the performance of existing generator libraries falls significantly short of what
is possible. This is due to two major sources of inefficiency.
First, the same high-level design that gives generator libraries their
flexibility and expressiveness also introduces overhead: layered abstractions
and indirections hinder compiler optimizations, shifting work to
run time.
Second, the \rand that underlies the generator's operation---the source of the
random numbers that are used to make choices in the generator---can 
constitute an unexpectedly high proportion of a generator's running time.
\iflater
\hg{TODO later, I'm not 100\% sure it's clear how this particular question is
raised by the above. We could strengthen that connection}
\fi
Together, these observations raise a fundamental question: can generator libraries be expressive and high-level without compromising efficiency?

To address this question, we outline two key principles for designing generator libraries that balance efficiency with flexibility.
First, we describe a method of optimizing generator libraries using
\textit{multi-stage programming}, or ``staging,'' which has long been used to
build DSLs without abstraction overhead (``without regret''~\cn{}).
We apply this technique to generators, completely eliminating the run-time overhead of
many common generator abstractions.
Secondly, we demonstrate the impact of different choices of \rand on generator
performance by conducting a controlled comparison.
Together, we refer to this recipe for improving generator libraries as \name{} --- \jwc{something pithy here}.

To demonstrate the effectiveness of our approach, we build two different PBT libraries in two different langauges.
In OCaml, we build \camlname{}, based on \bq{}  --- a
high-quality PBT library in OCaml, authored by software engineers at the trading firm Jane Street \cn{}.
In Scala 3, we build \scalaname{}, based on ScalaCheck --- a very mature PBT library and the de facto choice in Scala \cn{}.
In both languages, our libraries are direct drop-in replacements that are
\textit{100\% semantically equivalent}: that is, given identical random seeds, a generator written with our technique
produces exactly the same sequence of values as the same generator written with the original generator library.
The semantic equivalence between the original generators and \name{} generators allows us to 
perform clean, fine-grained comparisons, as differences in generation speed are
solely attributable to the staging and \rand optimizations.

We evaluate the optimizations with a series of case studies in each
generator library, and show that the \name generators run faster than their unstaged equivalents.
Further, we show generators that run faster also find bugs faster
% ability \jwc{The paper is called ``fail faster'' --- we shouldn't say ``bug finding ability'', we should say something like ``how fast you can squash bugs''. This is too passive }
by running these case studies in the Etna framework~\cite{etna}., a
platform that simulates real-world PBT usage\hg{Don't love this phrase, I'll
think more about how we should talk about Etna} by injecting bugs into a system and
measuring how quickly various generators detect them.


In summary, we show that monadic PBT generator DSLs incur significant performance costs across languages, and present a general technique for improving their efficiency while preserving their idiomatic abstractions. \jwc{needs a bit about RNG --- BCP thoughts?}
We make the following contributions:
\begin{enumerate}
    \item We identify two key sources of inefficiency in PBT generator libraries, which can significantly impact performance: 
    abstraction overhead and \rand choice. 
    \item We present \name{}, a technique based on staging that eliminates the abstraction overhead of generators. We implement \name{} in both OCaml and Scala 3, showcasing its cross-language applicability.
    \item We demonstrate that both \name{} and choosing a faster \rand yield substantial performance improvements to generator running time. Finally, we show
    that these performance improvements extend to significantly improved bug-finding speed. \jwc{This should be sell-ier: 2x speedups, etc. ``bug-finding speed'' isn't a race against time.}
\end{enumerate}

In Section~\ref{section:motiv}, we introduce monadic generator DSLs, discuss how they work, and take an example-guided tour through some of the ways that
they are slower than is possible.
Then, in Section~\ref{section:faster-generators}, we explain multi-stage programming,
and develop both facets of \name{}, using \camlname{} to illustrate.
Section~\ref{section:eval} evaluates all of the previous work, demonstrating the generator performance improvements
and how they lead to much faster bug finding.

\section{What are Generator Libraries, and why are they Slow?}
\label{section:motiv}
% \jwc{In this section we answer: ``What are monadic generator libraries, and why are they slow?''}
% \bcp{That's actually not a bad section title!}

% \subsection{Background: Monadic Generator DSLs}

Since property-based testing was first introduced in Haskell's QuickCheck
library~\cite{claessenQuickCheckLightweightTool2000}, {\em generators} have been
a key element of the PBT process. While properties are critical for correctly
expressing a developer's intent for a function, the success of the testing
ultimately rests on how well the random inputs to that property exercise the
system under test. QuickCheck's solution to this problem, which has been copied
dozens of frameworks across as many languages, is to provide a domain-specific
language (DSL) for writing random data generators that gives developers complete
control over their distribution of test inputs.

\begin{wrapfigure}{r}{.5\textwidth}
\begin{lstlisting}
module Bq : sig
  type 'a t
  val gen_int : int -> int -> int t
  val return : 'a -> 'a t
  val bind : 'a t -> ('a -> 'b t) -> 'b t

  val weighted_union : (int * 'a t) list -> 'a t

  val size : int t
  val with_size : int -> 'a t -> 'a t

  val fixed_point : ('a t -> 'a t) -> 'a t
  $\dots$
end
\end{lstlisting}
\caption{Some functions from the API of the \bq{} generator DSL.}
\label{fig:bq-api}
\end{wrapfigure}

While many DSL designs are possible, the standard design for QuickCheck-style
generators is via an embedded {\em monadic} language. For the purposes of this
paper, we use the syntax and types from OCaml's \bq library, which is presented in
Figure~\ref{fig:bq-api}, but these kinds of DSLs can also be found in languages
like Haskell, Scala, Python, and many more.
The library provides some basic generators, for
example \texttt{gen\_int} for generating random integers in a range, along with
the ``monadic interface'' consisting of \texttt{return} and \texttt{bind}.  The
generator \texttt{return x} is the constant generator, always generating the
value \texttt{x}. Running a generator \texttt{bind g k} runs the generator
\texttt{g}, producing a value \texttt{a}, and then runs the generator \texttt{k
a}.
\iflater
\jwc{\texttt{gen\_int} here is really \texttt{int\_uniform\_inclusive}. I also
think we should completely omit named arguments in the paper so we don't confuse
with splices.}
\fi

Together, these three functions are the bare minimum for constructing arbitrary
random data generators: \texttt{gen\_int} provides a base source of randomness,
and \texttt{return} and \texttt{bind} allow generators to be composed to create
larger, more complex generators. Figure~\ref{fig:simple-bind} shows a generator
built with these operations; it first samples an int between $0$ and $100$,
names it $x$, samples another between $0$ an $x$, and then returns the pair of
them both.

\begin{figure}[h]
  \begin{subfigure}{.49\textwidth}
\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  Bq.bind (Bq.gen_int 0 100) (fun x ->
    Bq.bind (Bq.gen_int 0 x) (fun y ->
      Bq.return (x,y)))
\end{lstlisting}
\caption{A simple generator using \texttt{bind} explicitly.}\label{fig:simple-bind}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  let%bind x = Bq.gen_int 0 100 in
  let%bind y = Bq.gen_int 0 x in
  return (x,y)
\end{lstlisting}
\caption{An equivalent generator using macros for \texttt{bind}.}\label{fig:simple-macro}
  \end{subfigure}
  \caption{Simple monadic generators for a pair of ordered integers.}\label{fig:simple-pair}
\end{figure}

Most languages in which monadic APIs are common expose some sort of syntactic
sugar for them. In OCaml\footnote{OCaml actually has a few ways to implement
monadic syntax; this is the one provided by Jane Street's libraries.}, this
looks like \texttt{let\%bind x = e in e'}, which desugars to
\texttt{bind e (fun x => e')}. Figure~\ref{fig:simple-macro} shows the same
generator as before, now written the monadic syntax.

% \subsubsection{Subtleties of Bind}
% The \texttt{bind} function in monadic generator libraries sequences generators
% with a call-by-value semantics, regardless of the calling convention of the
% language in which it is implemented. Concretely, this means that sampling
% \begin{center}
%   \texttt{let\%bind x = gen\_int 0 10 in return (x + x)}
% \end{center}
% always produces an even number; \texttt{x} is only sampled once (when the
% \texttt{bind} is evaluated) and from that point on it is a pure integer value. This is
% in contrast to the semantics of some probabilistic programming languages that
% may have call-by-name sampling semantics \cn{}. This is important because \hg{Joe why are we
% talking about this here? What are we going to use this fact for?}.
% \jwc{Well this is why directly inlining the binds doesn't work.}

% Another subtle aspect of \texttt{bind} is that many implementations mutate the
% internal state of the random number generator\footnote{The original QuickCheck
% implementation in Haskell uses a splittable random number generator~\cn, and
% \texttt{bind} splits the seed.} This means that generators do not always obey
% equivalences that one might expect from a monadic program. For example,
% \begin{center}
%   \texttt{let\%bind \_x = return () in gen\_int 0 10}
% \end{center}
% does not necessarily produce the same sequence of values as \texttt{gen\_int 0
% 10}; the \texttt{bind} updates the random seed in the background before
% geenrating the integer, even though it looks like a no-op. Critically, while
% these generators do not produce the same sequence of values, they do produce the
% same {\em distributions} of values. We say that generators obey standard monadic
% equalities up to ``distributional equivalence.''
% In Section~\hg{todo}, we discuss why distributional equivalence is often weaker
% than one might like when evaluating the differences between
% generators.\hg{Update this}

Generator libraries also include other functions that make generator
construction easier; some examples of these are also shown in
Figure~\ref{fig:bq-api}.  The \texttt{weighted\_union} function is a
particularly well-used one: it makes making a weighted choice between different
generators, allowing the developer to combine different sub-generators into a
single program and tune the data distribution. Also important are functions like
\texttt{size} and \texttt{with\_size} that are used to control the sizes of
generated values and \texttt{fixed\_point} that is used to define recursive
generators.

\begin{wrapfigure}{r}{.5\textwidth}
\begin{lstlisting}
let tree_of g = fixed_point (fun rg ->
  let%bind n = size in
  weighted_union [
    (1, return E);
    (n,
      let%bind x = g in
      let%bind l = with_size (n / 2) rg in
      let%bind r = with_size (n / 2) rg in
      return (Node (l,x,r)))
  ])
\end{lstlisting}
\caption{A generator using a variety of convenience functions.}\label{fig:tree_of}
\end{wrapfigure}

The generator in Figure~\ref{fig:tree_of} uses all of the aforementioned
features. It uses \texttt{fixed\_point} to define a recursive generator that
reads the current value of \texttt{size} to determine how to generate a tree.
The generator uses \texttt{weighted\_union} to make a random choice between an
empty tree and a node, choosing a node with weight proportional to the current
size and choosing a leaf otherwise. When generating a node, the generator uses
\texttt{with\_size} to reduce the value of the size parameter for future
iterations.

\subsection{Abstraction Overhead of Generator DSLs}
Just how large is the abstraction overhead of monadic generator DSLs, and where does it come from?
Figure~\ref{fig:bq-internals} shows the internals of (a simplified version of) \bq{}.
A generator \texttt{'a Bq.t} is just a function of type~\texttt{int -> SR.t -> 'a},
taking an \texttt{int} representing the current size parameter and a random seed \texttt{SR.t},
and returning a generated value \texttt{'a}. It is an invariant of the library that function of this type are
deterministic: for a fixed size and seed, it will return the same value. All of the randomness in testing comes from varying the initial seed.
The monad functions \texttt{return} and \texttt{bind} are defined in the usual way
for an instance of the reader monad \cn{}. 
% \bq{} uses a mutable seed, so the \texttt{random} passed to \texttt{(k a)} will be different from the one passed to \texttt{g}.
The \texttt{gen\_int} combinator simply calls out to the randomness library \texttt{Splittable\_random}, aliased as \texttt{SR} here.

Different PBT libraries use variations on this basic design. The largest source of variation is how the random seed is threaded through the generator.
\bq{} uses a reader monad with a mutable seed, while Haskell's QuickCheck uses a reader monad with an immutable state
that is ``split'' at \texttt{bind}s \cn. Meanwhile, ScalaCheck (a) uses an immutable state type and state monad \cn{} to thread
the state through, and (b) has an \texttt{Option} as its return type, to allow generation to fail.

\begin{figure}
\begin{lstlisting}
module Bq = struct
  type 'a t = int -> SR.t -> 'a

  let return (x : 'a) : 'a t = fun _ _ -> x

  let bind (g : 'a t) (k : 'a -> 'b t) : 'b t =
    fun size random ->
      let a = g size random in
      (k a) size random

  let gen_int (lo : int) (hi : int) : int t =
    fun _ random -> SR.int random lo hi
end
\end{lstlisting}
\caption{Internals of a Monadic Generator eDSL}
\label{fig:bq-internals}
\end{figure}

Just how much run-time overhead does this monadic abstraction introduce? To
illustrate, let's return to our running example of a constrained pair of
integers, written in both \bq{} and ScalaCheck.
Figure~\ref{fig:overhead-explanation-code} shows two versions of the generator, written in both languages.
The first versions (\texttt{int\_pair} in \bq{} and \texttt{intPair} in ScalaCheck) are written with the monadic generator combinators from their respective libraries.
The second versions (\texttt{int\_pair\_inlined} and \texttt{intPairInlined})
are semantically identical to the first, but have been rewritten
by (1) inlining all generator combinator definitions, and then (2)
repeatedly reducing simplifiable terms like \lstinline{(fun x -> e) e'} --- where an anonymous function is defined and then immediatly called~\footnote{This is usually known as a ``$\beta$-redex''.
Note that when reducing, we do not reduce to \lstinline{e[e'/x]}, as this does not preserve the order of effects.
}
--- to \lstinline{let x = e' in e}.

\begin{figure}
\begin{lstlisting}
let int_pair : (int * int) Bq.t =
  let%bind x = (Bq.gen_int 0 100) in
  let%bind y = (Bq.gen_int 0 x) in
  Bq.return (x,y)

let int_pair_inlined : int -> SR.t -> int * int =
  fun _ sr ->
    let x = Splittable_random.int sr ~lo:0 ~hi:100 in
    let y = Splittable_random.int sr ~lo:0 ~hi:x in
    (x,y)

def intPair : Gen[(Long,Long)] = for {
  x <- Gen.choose(0,1000)
  y <- Gen.choose(0,x)
} yield (x,y)

def intPairInlined : (Gen.Parameters, Seed) => (Option[(Long,Long)],Seed) = {
 (p,seed) =>
  val (x,seed2) = chLng(0,1000)(p,seed)
  x match {
    case None => (None,seed2)
    case Some(x) =>
      val (y,seed3) = chLng(0,x)(p,seed2)
      y match {
        case None => (None,seed)
        case Some(y) => (Some(x,y),seed3)
      }
    }
}
\end{lstlisting}
\caption{Int Pair Generators in \bq{} and ScalaCheck}
\label{fig:overhead-explanation-code}
\end{figure}

The performance impact of this inling is large (Figure~\ref{fig:overhead-explanation-perf}). In both languages, the
inlined version takes (on average) half as much time to generate a single pair of
\texttt{int}s. 
Microbenchmarks of more realistic generators (see Section~\ref{section:eval})
show an even more dramatic performance boost for inlining in this manner.

Because the inlined versions of the generator are identical to the un-inlined
versions except for mechanical, semantics-preserving transformations,
this performance difference is attributable solely to
the different machine code generated by the compiler.
Indeed, compilers of effectful and strict functional languages (including JIT
compilers, in the case of Scala 3)
use heuristics to determine if and when to perform this particular kind of
simplification~\footnote{As we discuss in Section~\ref{section:other-langs}, purity
means that Haskell is a slightly different story. GHC can and often does
transformations of this form.}.  Even in cases as simple as
Figure~\ref{fig:overhead-explanation-code}, the indirection of \texttt{return}
and \texttt{bind} causes these heuristics to not fire.  Moreover, the story is
even worse for recursive generators, as the heuristics are necessarily even more
conservative for optimizing recursive functions.

\begin{figure}
  \begin{tabular}{lll}
  Library    & Generator          & Average Time per Generation (ns) \\
  \bq{}         & \texttt{int\_pair}          & 70                       \\
  \bq{}         & \texttt{int\_pair\_inlined} & 35                       \\
  ScalaCheck & \texttt{intPair}            & 458                      \\
  ScalaCheck & \texttt{intPairInlined}     & 266                     
  \end{tabular}
\caption{Microbenchmarks of Generators in \bq{} (using \texttt{core\_bench} \cn) and ScalaCheck (using \texttt{jmh} \cn). Average over 10000 generations with random seeds, and a fixed size}
\label{fig:overhead-explanation-perf}
\end{figure}

While neither of these microbenchmarks exhibit this behavior, more complex
generators suffer further performance penalties due to \emph{closure
allocation}. In cases where the compiler cannot statically eliminate it,
running a monadic bind allocates a short-lived closure: we allocate a closure
for the continuation, and then immediately jump into it.
In strict functional languages like OCaml and Scala,
Each individual closure allocation is relatively cheap, but doing lots of allocation in a
generator is very costly because each allocation brings us closer to the next costly GC pause \cn{}.
This effect is also magnified in recursive generators: each iteration
through the recursive loop re-allocates closures for \texttt{bind}s, so the amount of allocation
\emph{per generated value} scales linearly with the number of recursive generator calls.

\subsubsection{Overhead of Choice Combinators}
Like \texttt{return} and \texttt{bind}, combinators like \texttt{weighted\_union} incur a performance penalty
at run time. Aside from the previously-discussed issues that compilers
cannot see through the abstraction boundary to optimize these programs,
choice combinators like \texttt{weighted\_union} come with their own particular abstraction overhead.

In practice, \texttt{weighted\_union} is almost always\footnote{
There are some generators where \texttt{weighted\_union} is passed a list which
was itself the result of a generator: the well-typed STLC term generator used in Section~\ref{section:eval} is an example of this.
} called with an explicitly constructed list, like
\texttt{weighted\_union [(w1,g1); (w2,g2); (w3,g3)]}. 
This is because the most common use case for \texttt{weighted\_union} is
to choose between one of the different constructors of an algebraic datatype,
the options for which are always known.
This list is allocated at the call site and then never needed after the call to \texttt{weighted\_union} returns\footnote{
This overhead is fixable in languages with stack alloctation, which OCaml will soon be \cn{}.
}.
Since the elements of the list and its length $n$ are known,
a compiler could in principle unroll the loops in \texttt{weighted\_union} to depth $n$ and specialize the function at each call site to avoid allocating the list.
Unfortunately, almost~\footnote{Once again, GHC is a notable exception here, performing sophisticated list fusion optimizations \cn{}} no compilers perform this kind of optimization.
This allocation (or rather, its tendency to cause GC pauses) --- as well as the cost of running the code 
to traverse arbitrary lists compared to unrolled loops --- has a significant impact on 
performance.

Last, many PBT libraries --- including both \bq and ScalaCheck --- implement their \texttt{weighted\_union} combinators
in ways that are asymptotically more efficient but slower in practice than the naive algorithm.
Weighted union works by the Fitness Proportionate Selection algorithm \cn{}, which (1) samples a number $r$
between $0$ and the sum of the weights, and then (2) finds the first generator for which the cumulative
weights in the list before it exceeds $r$. This second part can be accomplished in $O(\log_2 n)$ time by
binary searching the list. However, since the lists are short in practice, a linear scan is almost always faster \cn{}.
Moreover, both \bq{} and ScalaCheck allocate auxiliary data structures (an array in \bq{} and a BST in ScalaCheck) to
perform this search, which incurs further run time overhead.

% \subsubsection{Function call overhead}
\subsection{Inefficient Randomness Libraries}
The core of any PBT generator library is a source of randomness: to generate
random values of some datatype, we some access to random numbers!
Different PBT libraries use different randomness libraries implementing different algorithms
\footnote{The common term for such an algorithm or library is a ``Random Number
Generator'' (RNG) we will avoid this term and instead say ``randomness library''
to avoid confusing RNG implementations with the PBT gnenerator libraries that
use them.}.
Following the original Haskell QuickCheck implementation, \bq\ uses the SplitMix
algorithm \cn, implemented as a (stateful) OCaml library called \texttt{Splittable\_random}. Meanwhile,
ScalaCheck uses the JSF algorithm \cn.
% https://www.pcg-random.org/posts/bob-jenkins-small-prng-passes-practrand.html
% https://burtleburtle.net/bob/rand/smallprng.html

The randomness library sits at the heart of the hot path.
Even basic generators --- like ones generating a single \texttt{int} or \texttt{float} uniformly within a range --- can sample \emph{unboundedly many}
random numbers, since they usually use versions of rejection sampling to find a value within the range \cn.
Moreover, generator combinators like \texttt{list} usually make $O(n)$ calls to even those basic generators.
Because of this, the speed of a single sample matters a great deal. Unfortunately,
existing PBT libraries make relatively inefficient choices on this front,
leading to worse bugfinding power than what is possible.

\iflater\hg{This paragraph feels a bit clumsy right now. What if we started with a table
of the RNGs used by a bunch of popular PBT frameworks and then argued that
things like Lehmer are faster?}\fi

For example, significantly faster algorithms than SplitMix or JSF exist,
such as the Lehmer algorithm\cn. In microbenchmarks, the Lehmer algorithm runs almost 2x as fast as SplitMix \cn.
% https://github.com/lemire/testingRNG/tree/master
Moreover, PBT libraries could even consider eschewing the requirement that a source of
entropy pass statistical tests like BigCrush\cn. \hg{This needs more discussion} 
This is common practice in other areas of testing already:
fuzzers often simply use a buffer full of arbitrary bytes as a source of entropy
\cn. Such approaches are also faster than algorithms like SplitMix: bumping a
pointer and reading from memory (which can be pipelined trivially) will always
be faster on modern CPUs.
\hg{Should we just always be doing this? I think we want to argue
that this is a bridge too far because you still need to generate the buffer
ahead of time and you can run out of randomness?}
\jwc{HG: talk about the tradeoffs here.}
\jwc{The point here is that ``you should be thinking about using a more performant RNG, we'll show you that it does have real benefits (and quantify them). But there are tradeoffs here.'}

Of course, simply arguing that the randomness library is on the hot
path does not guarantee that a faster sampling leads to measurably faster
generation; for that, we need an experiment. The most obvious experiment is to
simply swap out the randomness library of either \bq{} or ScalaCheck with a totally different, faster one. But, as
discussed previously, generators with different generation orders can be
difficult to compare. To get around this, we exploit a ``natural experiment:
the \rand{} that \bq{} uses --- OCaml's \texttt{Splittable\_random} library --- is slow in a way that
can be improved \emph{without} changing its behavior.
In particular,
due to implementation details related to the OCaml garbage collector,
values of the OCaml type \texttt{int64} are not machine words, but rather
\emph{pointers} to machine words. This means
that \emph{all} \texttt{int64} operations (both arithmetic and bitwise) must
allocate memory cells to contain their output, which has a significant performance benefit.
By building a version that uses much faster ``unboxed'' 64-bit integer arithmetic, we can demonstrate just how much faster bugs can be found just by using a more performant randomness library.

\section{\name{}}
\label{section:faster-generators}
\jwc{This section needs a much better title...}
% \jwc{Usual introduction to this section, corresponding to how we talked about it in the intro. ``We present a library that XYZ''.}

\jwc{This section needs an intro, and some signposting to say where we're going.}


\subsection{Background: Multi-Stage Programming}
\label{subsection:msp}

In Muli-stage programming (also known more simply as staging), programs execute
over the course of multiple stages, with each stage producing the code to be run
in the next. For the purposes of this paper, we will
primarily consider two stages, namely compile time and run time. Staged programs thus
execute twice: once at compile time, which produces more code, which is then
compiled and run at run time.

\subsubsection{Uses of Staging in eDSL Construction}
One of the primary uses of staging is in embedded DSL construction \cite{sheard99, lms, trattdsl}.
\jwc{Cite racket macros for DSLs here:}
While embedded DSLs are powerful tools, they have well-known drawback: 
the functional abstractions used to build eDSLs prevent compilers from generating efficient machine code \cn.
This effect is known as ``abstraction overhead'' \cite{carrette05, moller20}: the layers of abstraction that 
make eDSLs so usable are precisely what prevents them from being fast.
Indeed, the root causes of many of the issues we discovered in Section~\ref{section:motiv} are
not unique to generator DSLs.
In light of this issue, staging is often used as a lightweight compiler for DSLs. The compile-time evaluation stage
transforms the DSL code, eliminating abstractions to produce code that the host language compiler can generate fast machine code for.
This recipe has been used to great effect across domains to stage eDSLs for stream processing \cite{moller20,strymonas}, parser combinators \cite{sspc, staged-parsers, krishnaswami19, flap},
and query processing \cite{rhyme}.

Many languages have some degree of staging functionality, though support varies
from full native support (Scala 3 \cite{scalamacros}, Haskell \cite{templatehaskell}, \jwc{Racket??}) to compiler extensions (OCaml \cite{metaocaml,macocaml}, Java \cite{mint}).
For this paper, we have implemented staged PBT libraries both in OCaml --- using the MetaOCaml fork of the OCaml compiler \cn --- and Scala 3 --- using
\texttt{scala.quoted}. For presentation purposes, all staged code presented in the body of this paper is \camlname{} code written in OCaml:
\scalaname{} differs almost entirely in syntax alone.
For a further discussion of the potential of building staged generator DSLs in other languages, see Section~\ref{section:other-langs}.

\subsubsection{Staging in MetaOCaml}
MetaOCaml's staging functionality is exposed through a type \texttt{'a code}. A
value of type \texttt{t code} at compile time is a (potentially open) OCaml term
of type \texttt{t}.

Values of \texttt{code} type are introduced by \emph{quotes}, written
\texttt{.<$\ldots$>.}. Brackets delay execution of a program until run time. For
example, the program \texttt{.< 5 + 1 >.} has type \texttt{int code}.  Note that
this is not the same as \texttt{.< 6 >.}. Because brackets delay computation,
the code is not \emph{executed} until the next stage (run time).
Values of type \texttt{code} can be combined together using \emph{escape},
written \texttt{.\~{}(e)} (or just \texttt{.\~{}x}, when \texttt{x} is a variable).  Escape lets you take a value of type
\texttt{code}, and ``splice'' it directly into a quote.  For example, this
program \texttt{let x = .<1 * 5>. in .< .\~{}(x) + .\~{}(x) >.} evaluates to
\texttt{.<(1 * 5) + (1 * 5)>.}.  MetaOCaml ensures correct scoping and macro
hygiene, ensuring that variables are not shadowed when open terms are spliced.

The power of staging for optimization away abstraction overheads comes from
defining functions that accept and return \texttt{code} values.  A function
\texttt{f : 'a code -> 'b code} is a function that takes a program computing a
run-time \texttt{'a} and transforms it into a program computing a run-time
\texttt{'b}. In particular, because \texttt{f} itself runs at compile time, the
fact that the programmer called \texttt{f} does not matter at run time --- the abstraction
that \texttt{f} defines has been eliminated.
A code-transforming function \texttt{'a code -> 'b code} can also be converted \emph{code for a function} --- a value of type
\texttt{('a -> 'b) code} --- with the following program:
\begin{lstlisting}
let eta (f : 'a code -> 'b code) : ('a -> 'b) code = .<fun x -> .\~(f .<x>.)>.
\end{lstlisting}

This program is known as ``the trick'' in the partial evaluation and multi-stage programming literature.
\cn{} 
% USE THIS: https://arxiv.org/pdf/2309.08207
It returns a code for a function that takes an argument \texttt{x}, and then it splats in the result of calling \texttt{f} on just
the quoted \texttt{x}.

The trick is best illustrated by an example. The following program reduces to \texttt{.< fun x -> (1 + x) mod 2 == 0 >}.
\begin{lstlisting}
let is_even x = .< .~x mod 2 == 0 >. in
let succ x = .< 1 + .~x >. in
eta (succ . is_even)
\end{lstlisting}
By composing the two code-transforming functions together at compile time, and only then turning them into a run-time function,
the functions are fused together.
This is the basis of how staging is used to eliminate the abstraction of DSLs
(like a generator DSL). By writing DSL combinators as compile-time functions ---
and only calling \texttt{eta} at the end on the completed DSL program ---  we
can ensure that any overhead of using the combinators is eliminated by run time.

% \jwc{
%   \begin{itemize}
%     \item The \texttt{'a code} type, quote, escape or ``splice'', stage distinction.
%     \item MetaOCaml ensures correct scoping and hygene by preventing alpha-collision when you splice open terms.
%     \item The difference between \texttt{('a -> 'b) code} and \texttt{'a code -> 'b code}
%     \item We can convert one way, but not the other.
%     \item Functions \texttt{'a code -> 'b code} ``fuse''.
%     \begin{itemize}
%       \item Consider writing \texttt{even . succ}. This
%       \item Consider \texttt{even\_c : int code -> bool code = fun cx -> .<.~cx mod 2 == 0>.} and \texttt{succ\_c : int code -> int code = fun cx -> .< .~x + 1 >.}
%       \item If you do \texttt{to\_dyn (even\_c . succ\_c)}, you get \texttt{fun x -> (x + 1) mod 2 == 0}. Composing functions from code to code, and then \emph{only at the end} stamping out
%       a dynamic function value eliminates the function abstraction.
%     \end{itemize}
%     \item This is the basis of (WORD). By defining a library with functions with types like \texttt{'a code -> 'b code}, we can ensure that the
%     abstractions the library introduces are fully eliminated at compile time.
%   \end{itemize}
% }

\subsection{Design of a Staged Generator DSL}
\label{subsection:basic-design}
\jwc{Needs a lot more signposting at the top here.}

To build a staged version of a generator DSL, we must rewrite the the generator
combinators so that they run during the compile time. Of course,
the compile time stage does not do all the work: the DSL must run at compile time,
producing inlined code that uses no combinators, which can then be compiled and run at run time
with different sizes and seeds.

Our job is thus to carefully bisect the DSL, determining which inputs to generator
combinators are known statically (and can be part of the compile time stage), and
which parts are only known at run time (and must be \texttt{code}).
In the staging literature, this task is known as ``binding-time analysis'', and it is more of an art than a science.

% carefully break up so that you push what you can into the compile time stage.
% Figuring out which parts of the generator are known statically ()
% Recall \jwc{did we talk about this} that staging a DSL involves changing the
% combinators to run at compile time by carefully annotating their types with \texttt{code}s.
% Deciding which types \texttt{t} can
% instead be \texttt{t code} --- in other words,
% figuring out which parts of the DSL can be determined statically (and can be
% part of the compile-time stage), and which parts are only known dynamically (and
% hence must be \texttt{code}) --- is an art known as ``binding-time analysis'' \cn.

The crux of our binding time analysis is that the \emph{only} parts of a generator that are not
known at compile time are the particular random seed and
size parameters that will be passed in at run time. Generators themselves are always known entirely at compile time.
This means our library's generator type \texttt{'a
Gen.t} should have the type \texttt{int code -> SR.t code -> 'a code}: a compile-time
function from dynamically-known size and seed to dynamically-determined result.

This type, along with basic staged monadic generator DSL functionality can be found in Figure~\ref{fig:gen-staged-basic}.
The constant generator \texttt{return} has the same code, but this time it 
runs at compile time. Given \texttt{cx : 'a code}, the code for a \texttt{'a}, it
returns the generator which always generates that value. Similarly,
\texttt{bind g k} sequences generators as usual by passing the result of running the generator \texttt{g} to a continuation \texttt{k}. However, instead of getting access
to the particular value generated by \texttt{g}, the continuation \texttt{k} gets access to \texttt{code} for the value sampled from \texttt{g}:
at compile time, we only know \texttt{g} will generate \emph{some} \texttt{'a}, but not which one \footnote{Readers familiar with OCaml may notice that \texttt{return : 'a code -> 'a Gen.t} and \texttt{bind : 'a Gen.t -> ('a code -> 'b Gen.t) -> 'b Gen.t}
do not have the correct types for a monad instance, preventing us from using \texttt{let\%bind} notation. We rectify this issue in Section~\ref{subsection:codecps} by importing some clever ideas from the staging literature.}.
Operationally, bind takes code for the size and seed, and returns code that (1) let-binds a variable \texttt{a} to spliced-in code that runs \texttt{g}, and then
(2) runs the spliced-in continuation \texttt{k}.
Both function applications \texttt{g size\_c random\_c}
and \texttt{k .<a>. size\_c random\_c} run at compile time.
\texttt{Gen.int} is the generator that samples an int from the randomness library.
Given any size and random seed, it returns a code block that calls \texttt{SR.int} with that random seed. Because the lower and upper bounds
might not be known at compile time --- they may themselves be the results of calling \texttt{Gen.int} ---
the arguments \texttt{lo} and \texttt{hi} are of type \texttt{int code}, and get spliced into the code block as arguments to \texttt{SR.int}.
Lastly, \texttt{to\_bq} turns a staged generator into code for a normal \bq\
generator. This function is just a 2-argument version of ``The Trick'' (\texttt{eta}
from Section~\ref{subsection:msp}).
\iflater
\hg{TODO: Be really careful with formatting of the above paragraph. It's very
easy for some inline code to get split weirdly over a line break or just be
difficult to parse in general, and that could throw the reader off when the
content is already pretty low-level}
\fi


\begin{figure}
\begin{lstlisting}
module Gen = struct
  type 'a t = int code -> Random.t code -> 'a code

  let return (cx : 'a code) : 'a t = fun size_c random_c -> cx

  let bind (g : 'a t) (k : 'a code -> 'b t) : 'b t =
    fun size random ->
      .<
        let a = .~(g size random) in
        .~(k .<a>. size random)
      >.

  let int (lo : int code) (hi : int code) : int t =
    fun size random ->
      .< SR.int .~random .~lo .~hi >.

  let to_bq (g : 'a code Gen.t) : ('a Bq.t) code =
  .<
    fun size random -> .~(g .<size>. .<random>.)
  >.
end
\end{lstlisting}
\caption{Basic Staged Generator Library}
\ref{fig:gen-staged-basic}
\end{figure}

Returning to our running example, Figure~\ref{fig:running-staged} shows the
int-pair generator written with the staged \texttt{Gen.t} monad, as well as the
inlined code that results from calling \texttt{Gen.to\_bq} (changing some identifier names for clarity).
The code generated is identical to the manually inlined version from Section~\ref{section:motiv}, and of course runs equally fast.

\begin{figure}
\begin{lstlisting}
let int_pair_staged : (int * int) Gen.t =
  Gen.bind (Gen.int .<0>. .<100>.) (fun cx ->
    Gen.bind (Gen.int .<0> cx) (fun cy ->
      Gen.return .<(.~cx,.~cy)>.
    )
  )

let int_pair : (int * int) Bq.t code = Gen.to_bq int_pair_staged
(* .< fun size random ->
        let x = SR.int random 0 100 in
        let y = SR.int random 0 x in
        (x,y)
    >.
*)
\end{lstlisting}
\caption{Pairs of Ints, Staged}
\ref{fig:running-staged}
\end{figure}

\subsection{Staging Combinators}

In Section~\ref{section:motiv}, we noted that generator combinators like \texttt{weighted\_union}
often allocate lists in the hot path of the generator. Even though these lists are usually small ---
at most a few dozen elements in practice --- each allocation takes us closer to the next garbage collection.

This is an ideal opportunity to exercise another feature of staging: compile-time specialization.
Since we almost always know the particular list of choices at compile time, a staged version of \texttt{weighted\_union}
can generate \emph{different code} depending on the number of generators in the union.
If we use weighted union on a compile-time list of generators \texttt{g1}, \texttt{g2}, and \texttt{g3},
we can emit code that picks between the generators without realizing the list at
run time.

\begin{figure}
\begin{lstlisting}
module Gen =
$\dots$
  let pick (acc : int code) (weighted_gens : (int code * 'a t) list) size random : 'a code =
    match weighted_gens with
    | [] -> .< failwith "Error" >.
    | (wc,g) :: gens' ->
      .<
        if .~acc <= .~wc then .~(g size random)
        else
          let acc' = .~acc - .~wc in
          .~(pick .<acc'>. gens' size random)
      >.

  let weighted_union (weighted_gens : (int code * 'a t) list) : 'a t =
    let sum_code = List.foldr (fun acc (w,_) -> .<.~acc + .~w>. ) .<0>. weighted_gens in
    fun size random ->
      .<
        let sum = .~sum_code in
        let r = SR.int .~random_c 0 sum in
        .~(pick .<r>. weighted_gens size random)
      >.
\end{lstlisting}
\caption{Staged Weighted Union}
\label{fig:staged-weighted-union}
\end{figure}

Figure~\ref{fig:staged-weighted-union} shows the code for such a staged weighted union.
Crucially, it takes a \emph{compile-time} list \texttt{weighted\_gens} of
generators and weights. The weights themselves might only be known at run time --- it is
common to use the current size parameter as a weight, for instance ---
so they are \texttt{code}s.
Instead of building a histogram of the distribution described by the weights at run time and then traversing it,
the compile time \texttt{weighted\_union} combinator generates a
tree of \texttt{if}s --- specialized to the list of weights known at compile time --- that searches for the selected generator.

% \texttt{Gen.weighted\_union} begins by computing \texttt{sum\_code}, an \texttt{int code}
% that is the sum of the weights. Note that this happens at compile time: we fold over a list
% known at compile time to produce another code value. 
% We then call \texttt{SR.int} to sample a random number \texttt{r} between \texttt{0} and the sum.
% Finally, we splice in the result of calling the helper function \texttt{pick}. \texttt{pick}
% produces a tree of \texttt{if}s by again traversing the list of generators at compile time.
% This tree of \texttt{ifs} ``searches'' for the generator corresponding to the
% sampled value \texttt{r}, and then runs it.
% \hg{I'm not sure this helps me. I think I'd prefer a less detailed explanation
% of the code that conveys the intuition and let the reader actually read the code
% if they want to know specifics. As it stands the explanation is just too dense
% for me to process}

Figure~\ref{fig:staged-weighted-union-example} demonstrates a use of this staged weighted union.
Given a list (in this case constant) generators with weights ``the current size parameter'', \texttt{2}, and \texttt{1},
the generated code first computes the sum of these numbers, samples between \texttt{0} and the sum, and then
traverses the tree of three \texttt{if}s to find the correct value to return.

\begin{figure}
\begin{lstlisting}
let grades : char Bq.t = Gen.to_bq (
  Gen.bind size (fun n ->
    Gen.weighted_union [
      (n, Gen.return .<'a'>.);
      (.<2>., Gen.return .<'b'>.);
      (.<1>., Gen.return .<'c'>.);
    ]
  )
)
(*
.< fun size random ->
    let sum = size + 2 + 1 + 0 in
    let r = SR.int random 0 sum in
    if r <= size then 'a' else
      let r' = r - size in
      if r' <= 2 then 'b' else
        let r'' = r' - 2 in
        if r'' <= 1 then 'c' else failwith "Error"
>.
*)
\end{lstlisting} 
\caption{Use of Staged Weighted Union}
\label{fig:staged-weighted-union-example}
\end{figure}

\subsection{Let-Insertion and Effect Ordering}
Careful readers might note that the definition of \texttt{bind} (shown in Figure~\ref{fig:bind-with-let-binding})
was more complicated than one might expect. In particular, why not define bind in the standard way
for a reader monad (Figure~\ref{fig:bind-without-let-binding})?
Unfortunately, the \texttt{bind'} definition is wrong in our context as it leads to incorrect code being generated.
For example, consider \texttt{Gen.bind' (Gen.int .<0>. .<1>.) (fun x -> Gen.return .<(.~x,.~x)>.)}.
This generates the run time code \texttt{fun size random -> (SR.int random 0 1, int SR.random 0 1)},
which is incorrect.  Instead of generating a single integer and returning it
twice, it samples two different integers.
This matters because, as described in Section~\ref{section:intro}, \camlname and
\scalaname should be equivalent to their unstaged counterparts.


\begin{figure}[h]
  \begin{subfigure}{.45\textwidth}
\begin{lstlisting}
let bind (g : 'a t) (k : 'a code -> 'b t) : 'b t =
    fun size random ->
      .<
        let a = .~(g size random) in
        .~(k .<a>. size random)
      >.
\end{lstlisting}
\caption{\texttt{bind}, with a let-binding}\label{fig:bind-with-let-binding}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
\begin{lstlisting}
let bind' (g : 'a t) (k : 'a code -> 'b t) : 'b t =
    fun size random -> k (g size random) size random
\end{lstlisting}
\caption{\texttt{bind'}, the ``standard'' bind for the reader monad}\label{fig:bind-without-let-binding}
  \end{subfigure}
  \caption{\texttt{bind}, two ways}
  \label{fig:bind-two-ways}
\end{figure}

This problem is intimately related to nondeterministic effects in the presence of CBN/CBV evaluation ordering \cn.
In essence, the behavior of splice \texttt{.\~{}cx} in a staged function \texttt{f(cx : 'a code) = ...} is to \emph{copy}
the entire block of code, effects and all. To ensure that the randomness effects of the first generator are executed only once,
but that the value can be used in the continuation multiple times, \texttt{bind} let-binds the result of generation to a variable,
and then passes it to the continuation.

% The library \jwc{ensure this is consistent with the way we talk about the project, cf cross-language} is careully designed to 
% preserve exactly the effect order of base quickcheck \jwc{and the scala version to preserve the effect ordering of ScalaCheck}.
% \jwc{This fact should go earlier. Not really sure about where this subsection should actually land, but it needs to be said somehwere.}
% \hg{+1, I think this feels sort of out of place here, but I also think moving it
% up would make that part harder to understand too. Is there a way to tie this in
% to our discussion about maintaining the precise set of choices? The two issues
% are different, but they're related}

\subsection{CodeCPS and a Monad Instance}
\label{subsection:codecps}
Another subtle issue prevents the version of the library design discussed so far
from being used as a proper drop-in replacement for an existing generator DSL. In particular, the types of \texttt{return : 'a code -> 'a Gen.t}
and \texttt{bind : 'a Gen.t -> ('a code -> 'b Gen.t) -> 'b Gen.t} aren't quite right.
For the type \texttt{'a Gen.t} to actually be a monad, 
the these types cannot mention \texttt{code}. This is not just a theoretical issue, it is a significant usability concern:
the syntactic sugar for monadic programming (\texttt{let\%bind} in OCaml, \texttt{foreach} in Scala, \texttt{do} in Haskell, etc)
that makes it so appealing can \emph{only} be used if the types involved are
actually the proper monad function types.
\iflater
\hg{A pedantic reader may ask: Are we concerned with the monad {\em laws} as
well, or just the type signature?}
\fi

To support real monadic programming, we'll need to adjust the type of \texttt{'a Gen.t} slightly.
An initial attempt is to try \texttt{type 'a t = int code -> SR.t code -> 'a}. If we strip the \texttt{code} off the result type,
the functions \texttt{return (x : 'a) = fun \_ \_ -> x} and \texttt{bind g k = fun size seed -> k (g size seed) size seed}
have the proper types for a monad instance. Then, any combinators of type \texttt{'a Gen.t} before simply become \texttt{'a code Gen.t} with this new version.

However, this definition of bind doesn't have call-by value effect semantics, as
discussed in the previous section!  And because the type of \texttt{g size seed} is
now just \texttt{'a} (not necessarily \texttt{'a code}), we cannot perform the
let-insertion needed to preserve the CBV effects. To solve this problem, we
turn to a classic technique from the multistage programming literature: writing
our staged programs in continuation-passing style \cite{bondorf92}.
\hg{Slow down! This is all very interesting and very technical! Try making each
of these sentences two sentences, add some citations, and maybe spell this out
wiht an example}
\jwc{I could, but i'm just not sure this is important. It's too deep in the weeds for it to be worth explaining, too... nobody's going to read this section except staging fans}

In Figure~\ref{fig:codecps-and-final-gen}, we follow prior work \cite{kovacs24, carrette05} and define
the type \texttt{'a CodeCps.t = 'z. ('a -> 'z code) -> 'z code}: a polymorphic continuation transformer with the result type
always in \texttt{code}
\footnote{This is an instance of the \emph{codensity} monad \cite{janis08}, a fact which deserves further investigation.}.
The monad instance for this type is the standard instance for a CPS monad with polymorphic return type.
In prior work, this type is often referred to as the ``code generation'' monad
(and sometimes, ironically, called ``\texttt{Gen}'').
This is because a value of type \texttt{('a code) CodeCps.t} is like an ``action'' that generates \texttt{code}:
\texttt{CodeCps.run} passes the continuation transformer the identity continuation to produce a \texttt{'a code}.
To avoid confusion with random data generators, we refer to this type as \texttt{CodeCps.t}.
Most importantly, the \texttt{CodeCps} type supports a function \texttt{let\_insert}, which, given \texttt{cx : 'a code},
let-binds \texttt{let x = .\~{}cx}, and then passes \texttt{.<x>.} to the
continuation.
\hg{I'm getting lost in the inline code again}

We can then redefine our staged generator monad type to be
\texttt{'a Gen.t = int code -> SR.t code -> 'a CodeCPS.t}, as shown in Figure~\ref{fig:codecps-and-final-gen}.
\jwc{any types that were \texttt{'a Gen.t} before are now \texttt{'a code Gen.t}}.
This gives us the best of both worlds. First, we get a monad instance for \texttt{'a Gen.t} with the correct types, which lets us
use the monadic syntactic sugar of our chosen language. Moreover, we also get to maintain the correct
effect ordering: effectful combinators like \texttt{Gen.int} do their \emph{own} let-insertion, ensuring
that a program like \texttt{Gen.bind Gen.int (fun x -> ...)} generates a let-binding for the result of sampling the randomness library.
For example, \lstinline{bind (int .<0>. .<1.>) (fun cx -> return .<(.~cx,.~cx)>.)} now correctly generates
\lstinline{.< fun size random -> let x = SR.int random 0 1 in (x,x) >.}.
This design is less obviously correct, and does require some care. Rather than \texttt{bind} ensuring correct evaluation order once and for all,
individual combinators must be carefully written to ensure that \texttt{'a code} values that contian effects are \texttt{let\_insert}'d.\footnote{To ensure that the OCaml code is written correctly, we employ a PBT harness to compare its outputs to the corresponding \bq generator over 1,000 random seeds. 
If no discrepancy arises, we conclude that the generators exhibit equivalent behavior.}

\begin{figure}
\begin{lstlisting}
module CodeCps = struct
  type 'a t = { cps : 'z. ('a -> 'z code) -> 'z code }

  let return x = {cps = fun k -> k x}

  let bind (x : 'a t) (f : 'a -> 'b t) : 'b t =
    {cps = fun k -> x.cps (fun a -> (f a).cps k)}

  let run (t : ('a code) t) : 'a code = t.cps (fun x -> x)

  let let_insert (cx : 'a code) : 'a code t =
    {cps = fun k -> k .< let x = .~cx in .~(k .<x>.) >.}
end

module Gen = struct
  type 'a t = int code -> SR.t code -> 'a CodeCps.t

  let return (x : 'a) : 'a t = fun _ _ -> Codecps.return x

  let bind (g : 'a t) (f : 'a -> 'b t) =
    fun size random ->
      CodeCps.bind (g size random) (fun x ->
        (f x) size random
      )
  
  let int (lo : int code) (hi : int code) : int code t =
    fun size random -> let_insert .< SR.int .~random .~lo .~hi >.
end
\end{lstlisting} 
\caption{CodeCPS and The Final Gen Monad}
\label{fig:codecps-and-final-gen}
\end{figure}


% \subsubsection{The Trick at Other Types}
% To write more interesting generators, we also need the ability to generate code that manipulates run-time values.
% For instance, consider this generator

% \jwc{need a better example here}

% \begin{figure}
% \begin{lstlisting}
% let int_or_zero : int Bq.t =
%   let%bind n = size in
%   if n <= 5 then return 0 else Bq.int

% let split_bool (b : bool code) : bool Gen.t =
%   fun _ _ ->  _

% \end{lstlisting}
% \caption{??}
% \label{fig:trick-example}
% \end{figure}


% \jwc{Describe split --- this section is approximately experts-only, but you can just point at the Andras paper and the things it cites.}

\subsection{Recursive Generators}
\jwc{Generating recursive datatypes requires recursive generators!}

Different generator DSLs handle defining recursive generators differently. Some allow recursive generators
to be defined as recursive functions, while others
(like \bq and ScalaCheck) expose a fixpoint combinator to construct recursive generators.
Given a step function that takes a ``handle'' to sample from a recursive generator call, it
ties the knot and builds a recursive generator.

In our case, letting programmers define recursive generators as recursive functions is out of the question.
With staged programming, recursion must be handled with care: it is far to easy to accidentally recursively
define an infinite \texttt{code} value and have the program diverge at compile time, when trying to write
a \texttt{code} representing a recursive program.
To this end, we develop a staged recursive generator combinator\footnote{
In reality, we actually have a more general API that allows programmers to define \emph{parameterized}
recursive combinators, of type \texttt{'r code -> 'a code Gen.t}, for any type \texttt{'r}. See Appendix~\jwc{appendix} for details.
}, whose API is shown in Figure~\ref{fig:staged-recursive-generator}.
The code for this combinator can be found in Appendix~\jwc{appendix}.
The recursion API consists of an opaque type \texttt{'a handle}, and a function \texttt{recurse} to perform recursive calls.
Programmers can then define recursive generators by \texttt{fixed\_point}, which ties the recursive knot.

% \begin{lstlisting}
%   type ('a,'r) handle = 'r code -> 'a code t

%   let recurse (f : ('a,'r) handle) (x : 'r code) : 'a code t =
%     fun size random ->
%       Codecps.bind ((f x) size random) @@ fun c ->
%       Codecps.let_insert c

%   let recursive (step : ('a,'r) handle -> 'r code -> 'a code t) (x0 : 'r code) : 'a code t =
%     fun size_c random_c -> 
%       Codecps.return @@
%         .< let rec go x size random = .~(
%               Codecps.code_generate @@
%               (step (fun xc' -> fun ~size' ~random' -> Codecps.return .< go .~xc' .~size' .~random' >.) .<x>.) .<size>. .<random>.
%             )
%           in go .~x0 .~size_c .~random_c
%         >.
% end
% \end{lstlisting}

\begin{figure}
\begin{lstlisting}
type 'a handle
val recurse : 'a handle -> 'a code Gen.t
val fixed_point : ('a handle -> 'a code Gen.t) -> 'a code Gen.t
\end{lstlisting}
\caption{Staged Recursive Generator Combinator API}
\label{fig:staged-recursive-generator}
\end{figure}


\subsection{Staged Type-Derived Generators}

Generators are traditionally handwritten, but some libraries allow users to synthesize 
them automatically from type definitions. Type-derived generators are 
convenient---the derivation process requires no manual effort---but also limited: 
they are unable to account for constraints not encoded in the type. 
As a result, generating values with complex preconditions is statistically unlikely.
This state of affairs is not ideal, as inexperienced PBT users, who might derive a generator, run their test harness, find no bugs, and assume the code is correct---when in reality, most values are never tested at all because they do not satisfy the preconditions.

This makes type-derivation a particularly elegant target of \name. Since the process of type derivation is completely automatic,
the end user does not need to understand the staging syntax---but more importantly, \tr{TODO}

\label{subsection:type-derived}
\jwc{
  \begin{itemize}
    \item In PBT in practice, we learned that in many cases, programmers don't
    even write custom generators, instead relying on type-derived generators.
    \item These generators just produce arbitrary values of a given type, not necessarily enforcing validity conditions.
    \item If 
    \item Let's talk about how type-deriving works. In languages with typeclasses, it works
    by typeclass resolution. In OCaml it works by PPX system, but the principle
    is the same. You define rules to go from generators of a subtypes to a generator of the larger type, and then apply those rules to build up a full generator.
    \item \begin{itemize}
      \item For base types, you just call the associated generator
      \item For product types, you sample from all the component gnerators, bind the values, and then tuple up the values, and return the tuple
      \item For variant types, you use a weighted union to choose one of the component generators with a weighted union.
      \item For recursive types, wrap the whole thing in a fixedpoint and then use the recusive handle as the ``component generator'' for all recursive instance of the type.
    \end{itemize}
    \item This kind of generic deriving of generators works just as well for
    staged generators. Just replace the standard combinators in question with
    staged ones!
    \item We built this in OCaml, but you can easily use typeclasses to do it in scala too.
    \item Note that this is (essentially) 3-stage metaprograming. You're using either the PPX mechanism or typeclass resolution to generate a bunch of staged combinator calls, which then run at compile time, which then run at run time.
  \end{itemize}
}
\jwc{Todo: Thia}

\subsection{Faster SplitMix with Unboxed \texttt{int64}s}
\label{subsection:faster-rng}

\jwc{Unclear what we should call this section}
As we discussed in Section~\ref{section:motiv}, choosing an inefficient randomness library is
another bottleneck for finding bugs fast. While generator libraries by
and large use sensible sources of randomness, they are not explicitly chosen
with performance in mind. Indeed, much faster randomness libraries\cn and fast sources
of entropy \cn exist. \jwc{... more here?}
To demonstrate that faster random sampling can significanty impact bugfinding power, we
use the natural experiment provided by OCaml's inefficient implementation of
SplitMix. By replacing this slow randomness library with a faster but semantically equivalent
implemetnation, we are able to precisely quantify the bugfinding speedup that using a randomness library gives across a range of PBT scenarios.

We emphasize that while we believe that the insight that faster sampilng translates to faster bug finding
is a cross-langauge one, the specific technical contents of this section are OCaml-specific.
In the case of most other PBT frameworks \cn, the randomness library used operates on by machine
integers, so this \emph{particular} inefficiency does not exist.

The precise details of how the SplitMix algorithm works \cn are unimportant for the
present paper, but the critical component is that all of its operations are are
defined in terms of arithmetic bitwise operations on 64-bit integers. In OCaml,
because of details related to the garbage collector, the 64-bit integer type \texttt{int64} is represented
at run time as a \emph{pointer} to an unscanned block of memory containing (among other things)
a 64-bit integer \cn. This means that all operations that return an \texttt{int64} must allocate this block of memory.
This has a significant impact on the performance of generators. A single call to
one of the \bq library functions---like generating an integer uniformly in a range---may call the \texttt{splittable\_random}
algorithm multiple times. Each sample from \texttt{splittable\_random} allocates 9
times \jwc{this is a call to \texttt{next\_int64}}, and each allocation brings us closer
to the next garbage collection pause. While small allocations like these are \emph{very} fast to perform and subsequently collect in OCaml,
\footnote{The OCaml GC is a generational collector \cn, and since these allocations are small and mostly very short lived, they will all be minor allocations, never to be promoted.}, we will see in Section~\ref{section:eval} that this can have
a large performance impact on some generators that spend most of their time sampling data.
To circumvent this allocation and provide an equivalent version of \texttt{splittable\_random}, we
reimplement SplitMix in C, and call out to it with the OCaml FFI. The C version of the library uses
proper \texttt{int64\_t} arithmetic, only boxing and unboxing integers at the call boundaries between OCaml and C code.
Ideally in the future, one would not need to call out to C for this: the Jane Street bleeding-edge OCaml compiler has support
for unboxed types \cn, which (among other things) would let us implement a version of SplitMix that does not allocate, directly
in OCaml. Unfortunately, the Jane Street branch of the compiler is incompatible with MetaOCaml, which we use to implement the metaprogramming
discussed in the previous sections.

\section{Evaluation}
\label{section:eval}
% \jwc{Initial section here about eval}\tr{todo}
To assess the impact of our approach, we evaluate the performance and bug-finding speed of 
\name generators across a range of benchmarks. Our experiments compare generators 
built using our technique---with and without \csplitmix---against those implemented with 
existing generator libraries (and their default randomness mechanisms). Specifically, 
we implement 
two fully semantically 
equivalent staged generator libraries that replicate the behavior of \bq 
in OCaml and ScalaCheck in Scala, allowing us to assess the effectiveness of our 
technique across different languages and runtime environments.

This section presents our experimental setup, and answers to the following research questions:

\begin{itemize}
  \item \textbf{RQ1}: Do generators written using our technique run faster than those written with regular generator combinators?
  \item \textbf{RQ2}: Do performance speedups translate to better bug-finding speed?
  % \item \textbf{RQ3}: Does our technique generalize to other strict functional languages?
\end{itemize}

All experiments were run on a 64-bit Linux machine with 264 GB RAM and a 128-core Intel Xeon Platinum 8375C CPU, running Ubuntu 24.04.1 LTS. \camlname and all OCaml benchmarks
were compiled with 4.14.1+BER MetaOCaml \texttt{ocamlopt}, the native code compiler for OCaml, using compiler flag \texttt{-O3}.
The baseline OCaml generators were written with \bq{} 0.16.
\scalaname{} and all Scala benchmarks
were run on Scala 3.6.3
and OpenJDK 21.0.6, using ScalaCheck version 1.17 as the baseline.
We used \texttt{core\_bench} 0.16 \cn in OCaml and \texttt{jmh} 0.4.7 \cn for generator performance microbenchmarking.

\subsection{Benchmarking Generator Speed}
\label{subsection:benchmarking}
To answer \textbf{RQ1}, we microbenchmark generators, comparing generators in \camlname to identical ones in \bq,
and generators in \scalaname to those in ScalaCheck.
 We vary the choice of \rand in our \camlname generators, using both \bq's default
\texttt{splittable\_random} and our improved SplitMix (\csplitmix), as discussed
in \secref{subsection:faster-rng}.

For \camlname, our test cases consist of generators for boolean lists, binary search trees (BST), and simply
typed lambda calculus (STLC) terms. We implement generators for these benchmarks
using a variety of \textit{strategies}, which vary in structure and
sophistication. These include type-derived generators for BSTs and STLC terms,
using the approach outlined in \secref{subsection:type-derived}; two custom BST
generators, one that repeatedly inserts values into an initially empty tree and
one that materializes the tree in a single pass by generating keys, values, and
subtrees at each step; a generator of boolean lists that operates similarly to
the single-pass BST strategy; and a well-typed generator of
STLC terms, whose behavior is the most complex. 
For evaluating \scalaname, we implement a subset of the aformentioned 
strategies---only the boolean list generator, single-pass BST generator, and 
well-typed STLC term generator.

The runtime of \camlname generators across different input sizes is summarized in \figref{fig:time}. 
Our \camlname type-derived BST generator achieves speedups ranging from 8\% to 29.8\%, which increase 
dramatically to 515--865\% when combined with \csplitmix. The insertion-based BST generator sees 
13--21\% speedups with staging alone, and 332--435\% when also 
using \csplitmix. The single-pass BST generator benefits more significantly from 
staging, with speedups of 131--134\%, which rise to 761--795\% when combined with 
\csplitmix. We attribute the differential in performance speedup to the fact that 
BST strategies rely on relatively few calls to generator combinator functions, 
but sample extensively from the \rand; notably, single-pass BST, which uses the largest
number of calls to generator combinators, also sees the greatest benefit from staging. 

For STLC, staging accounts for the bulk of performance gains, yielding
 71--164\% speedups for the type-derived STLC generator, and 360--446\% for 
 the well-typed generator. When adding \csplitmix, these numbers increase to 
 160--415\% and 383--675\%, respectively. This result makes sense because
 STLC generators, particularly the well-typed generator, sample very little from
 randomness libraries, but extensively on generator library functions.

The boolean list generator experiences 261--610\% speedups with 
staging, and its performance actually slightly \textit{decreases} when 
combined with \csplitmix (243--583\%). This is likely because sampling 
booleans is ``cheap'' enough that the overhead of crossing the FFI barrier is 
greater than that of generating values directly in OCaml.

\tr{We might put a graph of calls to generator library functions vs. speedup here, but
I'm not sure it's worth it---thoughts?}

As shown in \figref{fig:scala}, \scalaname achieves an even greater performance 
gain---purely from staging---over ScalaCheck than \camlname does over \bq. This discrepancy arises from
ScalaCheck's representation of generators as functions of type \texttt{size ->
seed -> Option[A]}. Each generator combinator must construct and then
pattern-match on these \texttt{Option} values, introducing significant boxing
and unboxing overhead at each step.  Staging allows us to eliminate this
overhead, resulting in substantial speedups in aggregate.

These results show that staging and \rand choice serve distinct but
complementary roles in generator performance. Sampling-heavy generators see
greater improvements from faster randomness libraries, while combinator-heavy
ones benefit more from staging. Since both factors influence performance in
different ways, generator libraries should use both in order to handle diverse
workloads.

% \subsubsection{Scala} To show that the benefit of staging extends to
% other strict functional languages, we apply it to ScalaCheck, a widely-used PBT
% library written in Scala.

\begin{figure}[h]
  % \includegraphics[scale=0.33]{allocs.png}
  \includegraphics[scale=0.45]{time.png}
  \caption{Time to generate a varying-size value using each strategy, averaged over 10,000 trials. Performance benefits scale with generation size; lower is better. BQ is \bq; Staged SR is \camlname using \bq's \texttt{splittable\_random} RNG; Staged CSR is \camlname using \secref{subsection:faster-rng}'s \csplitmix.}
  \label{fig:time}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.32]{scala.png}
 \caption{
  \jwc{Don't explain the chart in the caption, split this into two different charts --- purple on one, green on the other.}
  \jwc{Also can we use the language's colors for this?}
  \scalaname has better performance relative to ScalaCheck than \camlname relative to \bq. SC is ScalaCheck; BQ is \bq. Comparison is for a fixed size of 100.}
 \label{fig:scala}
 \end{figure}

%\jwc{NOTE: we should test generator speed across both languages, but speed -> bugfinding ability in only OCaml.}
%\jwc{
%  Baseline generators to test speed in both languages:
%  \begin{itemize}
%    \item Single int
%    \item Pair of ints, constrained
%    \item List of ints without bind (use map): lots of sampling, minimal binds.
%    \item Unableled Trees of a fixed size (no weighted union) minimal sampling, lots of binds.
%  \end{itemize}
%}
\subsection{Benchmarking bug-finding speed}
To answer \textbf{RQ2}, we evaluate the bug-finding speed of our BST and STLC
case studies using Etna, a platform for assessing and comparing PBT techniques. 
Etna allows users to measure the effectiveness of different generator
implementations by injecting bugs into the system under test and recording the
time taken for a relevant property to fail in response. For example, in a tree
with an ordering property, a bug injection might consist of inserting a value
into the tree that violates that ordering. Each case study includes a diverse
set of \textit{tasks}, where a task consists of a specific bug-property pair
designed to test different aspects of the system (e.g., BST includes tasks that
test insertion, deletion, and union operations on binary search trees). 
In particular, BST has 37 tasks, and STLC has 20. Strategies for BST and STLC 
are implemented in 
\camlname; they are unchanged from their description in \secref{subsection:benchmarking}.\footnote{Etna does 
not support Scala, so we were unable to test \scalaname's bug-finding speed in Etna, but we
we see no reason our results in this section would not extend to \scalaname.}

It is worth noting that the time-to-failure of a given strategy on a given seed
is not necessarily representative of the strategy's average time-to-failure over
a large number of trials. We normalize by computing the relative performance, or
the ``speedup''. This works in most cases, but it is not perfect.  For example,
it is theoretically possible to choose a seed such that the \textit{first} value
produced by a generator discovers the bug, which would elide any differential
speedup (and likewise if both generators fail to find the bug).  To account for
these cases, we repeat the above process over 30 random seeds---that is, all strategies
are run on the same seed so that they produce identical sequences of values, and this process
is repeated 30 times.\footnote{Although the
variance \textit{between} seeds in Etna can be vast, timing results are
replicable for a \textit{given} seed. Across 1,000 trials using the same seed, 
the observed variance in time-to-failure was less than a nanosecond.}

We run each
strategy on all tasks using a 60-second timeout.  If a bug is not found within
this limit, the task is considered unsolved.  Tasks where all strategies fail to
find the bug are excluded from our dataset, as they provide no basis for
comparison.  Similarly, we exclude tasks where the BQ strategy completes in
under 5ms, as such negligible runtimes do not yield meaningful insights into
relative performance. These filters remove
28/600 (4.67\%) of type-derived STLC's tasks, 
168/600 (28\%) of STLC's tasks, 
149/1110 (14.42\%) of type-derived BST's tasks,
268/1110 (24.1\%) of single-pass BST's tasks, and
371/1110 (33.42\%) of insertion-based BST's tasks. More sophisticated 
strategies tend to find bugs faster, leading to a higher number of filtered 
tasks. By applying these filters, we ensure that our reported speedups reflect 
optimizations that meaningfully impact performance.

The average individual-task speedups for each strategy and benchmark are shown in 
\figref{fig:etna}, with trends in bug-finding speed closely mirroring those in performance 
from \secref{subsection:benchmarking}. The distribution of speedups in \figref{fig:swarm} reveals 
substantial variability, with most tasks clustering near the median and a long tail of outliers 
achieving much larger gains. STLC shows a more bimodal distribution of speedups
than the other benchmarks, which we attribute to the heterogenous difficulty of its tasks:  
some tasks regularly hit the 60-second timeout, while others finish in a fraction of a second. We 
find that ``easier'' tasks---those that run on the order of milliseconds---regularly achieve speedups in the 
range of 300 to 800\%, whereas tasks that run on the order of seconds achieve more moderate speedups of up to 250\%.
Overall, 
these results give strong evidence that \camlname consistently benefits bug-finding speed, sometimes drastically.
\begin{figure}[h]
  \includegraphics[scale=0.333]{speedup.png}
  \caption{Geometric average of all speedups---relative to BQ---for each strategy and benchmark, showing that staging leads to better bug-finding speed across the board.}
  \label{fig:etna}
\end{figure}
\begin{figure}[h]
  \includegraphics[scale=0.35]{swarm.png}
  \caption{Swarm distribution of individual-task speedups across strategies and benchmarks---gray dots represent tasks. Plots with extremely low variance have no swarm overlay; in plots with high variance, outliers correspond to large speedups. \tr{Make mean higher-contrast.}}
  \label{fig:swarm}
\end{figure}

\section{Beyond Strict Functional Languages}
\label{section:other-langs}

In the years since the original QuickCheck paper \cn{}, PBT has had remarkable success
in languages outside of the Haskell world from which it came.
For this reason, readers who are users and developers of PBT libraries in \emph{other} languages ---
including non-strict or procedural ones --- might be curious about how the
techniques brought to bear on OCaml's \bq{} and Scala's ScalaCheck in this paper
might be imported to their favorite language.

We begin by noting that the \name{} principle of choosing fast random number generators
is completely language-agnostic. In languages like OCaml, where runtime value representations
make natively-implemented \rand inefficient, calling out to a C implementation or using standard libraries written in C is a surefire win.
In other languages, serious thought should be put into using as fast of a \rand{} as possible,
as opposed to picking up any suitable option off the shelf.

Next, we discuss language-by-language the degree to which the \name{} insights about staging could be used.
\begin{itemize}
  \item \textbf{Racket} is the next target that we intend to test on.
  The entire Racket philosophy is intertwined with
  using macros to build small eDSLs like the ones we use to write generators, and so it is a natural fit. Indeed,
  there are at least two \cn{} PBT libraries in Racket.
  However, the racket macros literature, while extensive, does not concern itself with staging for performance purposes.
  For this reason, we --- the authorship team of this paper, devoid of much Racket expertise --- decided to not use Racket as a test case in this paper.

  \item \textbf{Haskell} is home to the original PBT implementation \cn{}, and
  so it is natural to ask why we have not built a \name{} in it yet. The basic
  answer is that the Haskell compiler (GHC) is designed specially to eliminate
  the run-time overhead of monadic abstractions! Indeed, because Haskell is
  pure, GHC can aggressively inline and beta-reduce nearly anything as part of
  its normal optimiation steps.  This means that in many cases, the impact of a
  staged PBT library (and indeed, all staged monadic DSLs) negligible.  For
  complex enough programs however, the heuristics GHC uses can degrade, leading
  to performance overheads for monadic code.  For this reason, Haskell does have
  a multi-stage programming system called Template Haskell \cn{}, which is
  sometimes to build eDSLs in contexts where one does not want to rely on the
  optimizer's heuristics.  In short: it is possible to build a \name{} version of Haskell
  QuickCheck, but the benefits would be much less pronounced.
  
  \item \textbf{Rust} shares some similarities with functional languages, and
  indeed, it is host to a number of property-based testing libraries \cn{}. However Rust does not
  structurally encourage monadic eDSLs --- it does not have a special monadic syntax --- and so
  PBT libraries in Rust ask programmers to define generators directly as \texttt{seed -> value} functions.
  For this reason, staging does not seem directly applicable to any of the PBT libraries in Rust.
\end{itemize}

\section{Related Work}

\subsection{Speeding up Property-Based Testing}
\hg{\begin{itemize}
  \item Changing the generation order~\cite{runcimanSmallcheckLazySmallcheck2008,braquehaisSimpleIncrementalDevelopment2017}
  \item Dealing with filtering~\cite{claessenGeneratingConstrainedRandom2015}
  \item QuickerCheck~\cite{krookQuickerCheckImplementingEvaluating2024}
  \item Evaluation of PBT generation~\cite{shiEtnaEvaluationPlatform2023}
\end{itemize}}

\subsection{Staging eDSLs}
\jwc{A generator is a parser of
randomness~\cite{goldsteinParsingRandomness2022}, and people have implemented
lots of staged parser libraries!}
\jwc{cite Lionel Parreaux  work, he's on PC}
\jwc{Macros in Racket}


\section{Conclusion \& Future Work}
\jwc{Why we stopped where we stopped: can you squeeze more out of this?}
\jwc{
  \begin{itemize}
    \item On the staging front, we could do a little with more clever staging to codegen better... Avoided using floating let-insertion to ensure we had precise control over effect ordering, but we could try to relax that.
          Recursive generators have some extra overhead for packing/unpacking accumulators, but could do a GADT cf \texttt{SOP} in 2ltt.
    \item On the RNG front, there's not much more we can squeeze out of the RNG without breakning equiv with splitmix. As discussed, there are much faster things you can do as alterative sources of entropy.
  \end{itemize}
}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bib,harry}

%%
%% If your work has an appendix, this is the place to put it.
\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
