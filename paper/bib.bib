
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkh{\"a}user" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

@article{etna,
author = {Shi, Jessica and Keles, Alperen and Goldstein, Harrison and Pierce, Benjamin C. and Lampropoulos, Leonidas},
title = {Etna: An Evaluation Platform for Property-Based Testing (Experience Report)},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ICFP},
url = {https://doi.org/10.1145/3607860},
doi = {10.1145/3607860},
abstract = {Property-based testing is a mainstay of functional programming, boasting a rich literature, an enthusiastic user community, and an abundance of tools — so many, indeed, that new users may have difficulty choosing. Moreover, any given framework may support a variety of strategies for generating test inputs; even experienced users may wonder which are better in a given situation. Sadly, the PBT literature, though long on creativity, is short on rigorous comparisons to help answer such questions.  

We present Etna, a platform for empirical evaluation and comparison of PBT techniques. Etna incorporates a number of popular PBT frameworks and testing workloads from the literature, and its extensible architecture makes adding new ones easy, while handling the technical drudgery of performance measurement. To illustrate its benefits, we use Etna to carry out several experiments with popular PBT approaches in both Coq and Haskell, allowing users to more clearly understand best practices and tradeoffs.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {218},
numpages = {17},
keywords = {empirical evaluation, mutation testing, property-based testing}
}

@inproceedings{inpractice,
author = {Goldstein, Harrison and Cutler, Joseph W. and Dickstein, Daniel and Pierce, Benjamin C. and Head, Andrew},
title = {Property-Based Testing in Practice},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639581},
doi = {10.1145/3597503.3639581},
abstract = {Property-based testing (PBT) is a testing methodology where users write executable formal specifications of software components and an automated harness checks these specifications against many automatically generated inputs. From its roots in the QuickCheck library in Haskell, PBT has made significant inroads in mainstream languages and industrial practice at companies such as Amazon, Volvo, and Stripe. As PBT extends its reach, it is important to understand how developers are using it in practice, where they see its strengths and weaknesses, and what innovations are needed to make it more effective.We address these questions using data from 30 in-depth interviews with experienced users of PBT at Jane Street, a financial technology company making heavy and sophisticated use of PBT. These interviews provide empirical evidence that PBT's main strengths lie in testing complex code and in increasing confidence beyond what is available through conventional testing methodologies, and, moreover, that most uses fall into a relatively small number of high-leverage idioms. Its main weaknesses, on the other hand, lie in the relative complexity of writing properties and random data generators and in the difficulty of evaluating their effectiveness. From these observations, we identify a number of potentially high-impact areas for future exploration, including performance improvements, differential testing, additional high-leverage testing scenarios, better techniques for generating random input data, test-case reduction, and methods for evaluating the effectiveness of tests.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {187},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/141478.141483,
author = {Bondorf, Anders},
title = {Improving binding times without explicit CPS-conversion},
year = {1992},
issue_date = {Jan. 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {V},
number = {1},
issn = {1045-3563},
url = {https://doi.org/10.1145/141478.141483},
doi = {10.1145/141478.141483},
abstract = {A major obstacle in partial evaluation (program specialization) is the need for binding time improvements.  By reorganizing a source program, the residual programs obtained by specializing the source program may be improved: more computations can be done statically, that is, at specialization time.One well-known effective reorganization is (manual or automatic) conversion into continuation passing style (cps).  This conversion allows data consumers to be propagated through frozen expressions to the data producers.  In this paper we show how such improvements can be obtained without affecting the source program: by writing the   program specializer itself in cps;  traditionally, specialization has been formulated in direct style.The advantages of avoiding cps-converting source programs are: (1) no cps-conversion phase is needed; (2) the generated residual programs are not in cps; (3) since no source level continuations are added, there is no overhead of manipulating closure representations in the generating extensions (e.g. compilers) obtained by self-application; (4) manual “binding time debugging” is easier since binding time analysis is done on a non-converted program.We have implemented a cps-based program specializer; it is integrated in the partial evaluator Similix 4.0.Using a cps-specializer, partially static data  structures can be handled safely in a straightforward way.   The difficulty is to ensure automatically that residual expressions that become part of a partially static data structure are neither duplicated nor discarded.  This is achieved by binding such residual expressions in automatically inserted frozen let-expressions; cps is needed to propagate operations on the partially static data structure through these frozen let-expressions.  Based on this idea, we have implemented an extension of Similix 4.0 that handles partially static data structures.},
journal = {SIGPLAN Lisp Pointers},
month = jan,
pages = {1–10},
numpages = {10}
}

@inproceedings{bondorf92,
author = {Bondorf, Anders},
title = {Improving binding times without explicit CPS-conversion},
year = {1992},
isbn = {0897914813},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/141471.141483},
doi = {10.1145/141471.141483},
abstract = {A major obstacle in partial evaluation (program specialization) is the need for binding time improvements.  By reorganizing a source program, the residual programs obtained by specializing the source program may be improved: more computations can be done statically, that is, at specialization time.One well-known effective reorganization is (manual or automatic) conversion into continuation passing style (cps).  This conversion allows data consumers to be propagated through frozen expressions to the data producers.  In this paper we show how such improvements can be obtained without affecting the source program: by writing the   program specializer itself in cps;  traditionally, specialization has been formulated in direct style.The advantages of avoiding cps-converting source programs are: (1) no cps-conversion phase is needed; (2) the generated residual programs are not in cps; (3) since no source level continuations are added, there is no overhead of manipulating closure representations in the generating extensions (e.g. compilers) obtained by self-application; (4) manual “binding time debugging” is easier since binding time analysis is done on a non-converted program.We have implemented a cps-based program specializer; it is integrated in the partial evaluator Similix 4.0.Using a cps-specializer, partially static data  structures can be handled safely in a straightforward way.   The difficulty is to ensure automatically that residual expressions that become part of a partially static data structure are neither duplicated nor discarded.  This is achieved by binding such residual expressions in automatically inserted frozen let-expressions; cps is needed to propagate operations on the partially static data structure through these frozen let-expressions.  Based on this idea, we have implemented an extension of Similix 4.0 that handles partially static data structures.},
booktitle = {Proceedings of the 1992 ACM Conference on LISP and Functional Programming},
pages = {1–10},
numpages = {10},
location = {San Francisco, California, USA},
series = {LFP '92}
}



@inproceedings{carrette05,
	abstract = {With Gaussian Elimination as a representative family of numerical and symbolic algorithms, we use multi-stage programming, monads and Ocaml's advanced module system to demonstrate the complete elimination of the abstraction overhead while avoiding any inspection of the generated code. We parameterize our Gaussian Elimination code to a great extent (over domain, matrix representations, determinant tracking, pivoting policies, result types, etc) at no run-time cost. Because the resulting code is generated just right and not changed afterwards, we enjoy MetaOCaml's guaranty that the generated code is well-typed. We further demonstrate that various abstraction parameters (aspects) can be made orthogonal and compositional, even in the presence of name-generation for temporaries and other bindings and ``interleaving'' of aspects. We also show how to encode some domain-specific knowledge so that ``clearly wrong'' compositions can be statically rejected by the compiler when processing the generator rather than the generated code.},
	address = {Berlin, Heidelberg},
	author = {Carette, Jacques and Kiselyov, Oleg},
	booktitle = {Generative Programming and Component Engineering},
	editor = {Gl{\"u}ck, Robert and Lowry, Michael},
	isbn = {978-3-540-31977-1},
	pages = {256--274},
	publisher = {Springer Berlin Heidelberg},
	title = {Multi-stage Programming with Functors and Monads: Eliminating Abstraction Overhead from Generic Code},
	year = {2005}}

@article{kovacs24,
author = {Kov\'{a}cs, Andr\'{a}s},
title = {Closure-Free Functional Programming in a Two-Level Type Theory},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ICFP},
url = {https://doi.org/10.1145/3674648},
doi = {10.1145/3674648},
abstract = {Many abstraction tools in functional programming rely heavily on general-purpose        compiler optimization to achieve adequate performance. For example, monadic        binding is a higher-order function which yields runtime closures in the absence        of sufficient compile-time inlining and beta-reductions, thereby significantly        degrading performance. In current systems such as the Glasgow Haskell Compiler,        there is no strong guarantee that general-purpose optimization can eliminate        abstraction overheads, and users only have indirect and fragile control over        code generation through inlining directives and compiler options. We propose a        two-stage language to simultaneously get strong guarantees about code generation        and strong abstraction features. The object language is a simply-typed        first-order language which can be compiled without runtime closures. The        compile-time language is a dependent type theory. The two are integrated in a        two-level type theory.                We demonstrate two applications of the system. First, we develop monads and        monad transformers. Here, abstraction overheads are eliminated by staging and we        can reuse almost all definitions from the existing Haskell ecosystem. Second,        we develop pull-based stream fusion. Here we make essential use of dependent        types to give a concise definition of a concatMap operation with        guaranteed fusion. We provide an Agda implementation and a typed Template        Haskell implementation of these developments.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {259},
numpages = {34},
keywords = {staged compilation, two-level type theory}
}

@inproceedings{janis08,
	abstract = {We present a low-effort program transformation to improve the efficiency of computations over free monads in Haskell. The development is calculational and carried out in a generic setting, thus applying to a variety of datatypes. An important aspect of our approach is the utilisation of type class mechanisms to make the transformation as transparent as possible, requiring no restructuring of code at all. There is also no extra support necessary from the compiler (apart from an up-to-date type checker). Despite this simplicity of use, our technique is able to achieve true asymptotic runtime improvements. We demonstrate this by examples for which the complexity is reduced from quadratic to linear.},
	address = {Berlin, Heidelberg},
	author = {Voigtl{\"a}nder, Janis},
	booktitle = {Mathematics of Program Construction},
	editor = {Audebaud, Philippe and Paulin-Mohring, Christine},
	isbn = {978-3-540-70594-9},
	pages = {388--403},
	publisher = {Springer Berlin Heidelberg},
	title = {Asymptotic Improvement of Computations over Free Monads},
	year = {2008}}

@inproceedings {sheard99,
author = {Tim Sheard and Zine-el-abidine Benaissa and Emir Pasalic},
title = {{DSL} Implementation Using Staging and Monads},
booktitle = {2nd Conference on Domain-Specific Languages (DSL 99)},
year = {1999},
address = {Austin, TX },
url = {https://www.usenix.org/conference/dsl-99/dsl-implementation-using-staging-and-monads},
publisher = {USENIX Association},
month = oct
}

@article{lms,
author = {Rompf, Tiark and Odersky, Martin},
title = {Lightweight modular staging: a pragmatic approach to runtime code generation and compiled DSLs},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/2184319.2184345},
doi = {10.1145/2184319.2184345},
abstract = {Good software engineering practice demands generalization and abstraction, whereas high performance demands specialization and concretization. These goals are at odds, and compilers can only rarely translate expressive high-level programs to modern hardware platforms in a way that makes best use of the available resources.Generative programming is a promising alternative to fully automatic translation. Instead of writing down the target program directly, developers write a program generator, which produces the target program as its output. The generator can be written in a high-level, generic style and can still produce efficient, specialized target programs. In practice, however, developing high-quality program generators requires a very large effort that is often hard to amortize.We present lightweight modular staging (LMS), a generative programming approach that lowers this effort significantly. LMS seamlessly combines program generator logic with the generated code in a single program, using only types to distinguish the two stages of execution. Through extensive use of component technology, LMS makes a reusable and extensible compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process, with common generic optimizations provided by the framework.LMS is well suited to develop embedded domain-specific languages (DSLs) and has been used to develop powerful performance-oriented DSLs for demanding domains such as machine learning, with code generation for heterogeneous platforms including GPUs. LMS has also been used to generate SQL for embedded database queries and JavaScript for web applications.},
journal = {Commun. ACM},
month = jun,
pages = {121–130},
numpages = {10}
}


@article{trattdsl,
author = {Tratt, Laurence},
title = {Domain specific language implementation via compile-time meta-programming},
year = {2008},
issue_date = {October 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {6},
issn = {0164-0925},
url = {https://doi.org/10.1145/1391956.1391958},
doi = {10.1145/1391956.1391958},
abstract = {Domain specific languages (DSLs) are mini-languages that are increasingly seen as being a valuable tool for software developers and non-developers alike. DSLs must currently be created in an ad-hoc fashion, often leading to high development costs and implementations of variable quality. In this article, I show how expressive DSLs can be hygienically embedded in the Converge programming language using its compile-time meta-programming facility, the concept of DSL blocks, and specialised error reporting techniques. By making use of pre-existing facilities, and following a simple methodology, DSL implementation costs can be significantly reduced whilst leading to higher quality DSL implementations.},
journal = {ACM Trans. Program. Lang. Syst.},
month = oct,
articleno = {31},
numpages = {40},
keywords = {domain specific languages, compile-time meta-programming, Syntax extension}
}


@inproceedings{rhyme,
	abstract = {We present Rhyme, an expressive language designed for high-level data manipulation, with a primary focus on querying and transforming nested structures such as JSON and tensors, while yielding nested structures as output. Rhyme draws inspiration from a diverse range of declarative languages, including Datalog, JQ, JSONiq, Einstein summation (Einsum), GraphQL, and more recent functional logic programming languages like Verse. It has a syntax that closely resembles existing object notation, is compositional, and has the ability to perform query optimization and code generation through the construction of an intermediate representation (IR). Our IR comprises loop-free and branch-free code with program structure implicitly captured via dependencies. To demonstrate Rhyme's versatility, we implement Rhyme in JavaScript (as an embedded DSL) and illustrate its application across various domains, showcasing its ability to express common data manipulation queries, tensor expressions ({\`a} la Einsum), and more.},
	address = {Cham},
	author = {Abeysinghe, Supun and Rompf, Tiark},
	booktitle = {Practical Aspects of Declarative Languages},
	editor = {Gebser, Martin and Sergey, Ilya},
	isbn = {978-3-031-52038-9},
	pages = {64--81},
	publisher = {Springer Nature Switzerland},
	title = {Rhyme: A Data-Centric Expressive Query Language for Nested Data Structures},
	year = {2023}}

@article{mat2stencil,
author = {Cao, Huanqi and Tang, Shizhi and Zhu, Qianchao and Yu, Bowen and Chen, Wenguang},
title = {Mat2Stencil: A Modular Matrix-Based DSL for Explicit and Implicit Matrix-Free PDE Solvers on Structured Grid},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622822},
doi = {10.1145/3622822},
abstract = {Partial differential equation (PDE) solvers are extensively utilized across numerous scientific and engineering fields. However, achieving high performance and scalability often necessitates intricate and low-level programming, particularly when leveraging deterministic sparsity patterns in structured grids. In this paper, we propose an innovative domain-specific language (DSL), Mat2Stencil, with its compiler, for PDE solvers on structured grids. Mat2Stencil introduces a structured sparse matrix abstraction, facilitating modular, flexible, and easy-to-use expression of solvers across a broad spectrum, encompassing components such as Jacobi or Gauss-Seidel preconditioners, incomplete LU or Cholesky decompositions, and multigrid methods built upon them. Our DSL compiler subsequently generates matrix-free code consisting of generalized stencils through multi-stage programming. The code allows spatial loop-carried dependence in the form of quasi-affine loops, in addition to the Jacobi-style stencil’s embarrassingly parallel on spatial dimensions. We further propose a novel automatic parallelization technique for the spatially dependent loops, which offers a compile-time deterministic task partitioning for threading, calculates necessary inter-thread synchronization automatically, and generates an efficient multi-threaded implementation with fine-grained synchronization. Implementing 4 benchmarking programs, 3 of them being the pseudo-applications in NAS Parallel Benchmarks with 6.3\% lines of code and 1 being matrix-free High Performance Conjugate Gradients with 16.4\% lines of code, we achieve up to 1.67\texttimes{} and on average 1.03\texttimes{} performance compared to manual implementations.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {246},
numpages = {30},
keywords = {compiler, domain-specific language, finite difference method, multi-stage programming, performance optimization, polyhedral compilation, stencil, structured grid}
}

@article{sspc,
author = {Willis, Jamie and Wu, Nicolas and Pickering, Matthew},
title = {Staged selective parser combinators},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {ICFP},
url = {https://doi.org/10.1145/3409002},
doi = {10.1145/3409002},
abstract = {Parser combinators are a middle ground between the fine control of hand-rolled parsers and the high-level almost grammar-like appearance of parsers created via parser generators. They also promote a cleaner, compositional design for parsers. Historically, however, they cannot match the performance of their counterparts. This paper describes how to compile parser combinators into parsers of hand-written quality. This is done by leveraging the static information present in the grammar by representing it as a tree. However, in order to exploit this information, it will be necessary to drop support for monadic computation since this generates dynamic structure. Selective functors can help recover lost functionality in the absence of monads, and the parser tree can be partially evaluated with staging. This is implemented in a library called Parsley.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {120},
numpages = {30},
keywords = {parsers, meta-programming, combinators}
}


@article{10.1145/2714064.2660241,
author = {Jonnalagedda, Manohar and Coppey, Thierry and Stucki, Sandro and Rompf, Tiark and Odersky, Martin},
title = {Staged parser combinators for efficient data processing},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2714064.2660241},
doi = {10.1145/2714064.2660241},
abstract = {Parsers are ubiquitous in computing, and many applications depend on their performance for decoding data efficiently. Parser combinators are an intuitive tool for writing parsers: tight integration with the host language enables grammar specifications to be interleaved with processing of parse results. Unfortunately, parser combinators are typically slow due to the high overhead of the host language abstraction mechanisms that enable composition.We present a technique for eliminating such overhead. We use staging, a form of runtime code generation, to dissociate input parsing from parser composition, and eliminate intermediate data structures and computations associated with parser composition at staging time. A key challenge is to maintain support for input dependent grammars, which have no clear stage distinction.Our approach applies to top-down recursive-descent parsers as well as bottom-up non-deterministic parsers with key applications in dynamic programming on sequences, where we auto-generate code for parallel hardware. We achieve performance comparable to specialized, hand-written parsers.},
journal = {SIGPLAN Not.},
month = oct,
pages = {637–653},
numpages = {17},
keywords = {parser combinators, multi-stage programming, algebraic dynamic programming}
}

@inproceedings{staged-parsers,
author = {Jonnalagedda, Manohar and Coppey, Thierry and Stucki, Sandro and Rompf, Tiark and Odersky, Martin},
title = {Staged parser combinators for efficient data processing},
year = {2014},
isbn = {9781450325851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660193.2660241},
doi = {10.1145/2660193.2660241},
abstract = {Parsers are ubiquitous in computing, and many applications depend on their performance for decoding data efficiently. Parser combinators are an intuitive tool for writing parsers: tight integration with the host language enables grammar specifications to be interleaved with processing of parse results. Unfortunately, parser combinators are typically slow due to the high overhead of the host language abstraction mechanisms that enable composition.We present a technique for eliminating such overhead. We use staging, a form of runtime code generation, to dissociate input parsing from parser composition, and eliminate intermediate data structures and computations associated with parser composition at staging time. A key challenge is to maintain support for input dependent grammars, which have no clear stage distinction.Our approach applies to top-down recursive-descent parsers as well as bottom-up non-deterministic parsers with key applications in dynamic programming on sequences, where we auto-generate code for parallel hardware. We achieve performance comparable to specialized, hand-written parsers.},
booktitle = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications},
pages = {637–653},
numpages = {17},
keywords = {parser combinators, multi-stage programming, algebraic dynamic programming},
location = {Portland, Oregon, USA},
series = {OOPSLA '14}
}


@inproceedings{krishnaswami19,
author = {Krishnaswami, Neelakantan R. and Yallop, Jeremy},
title = {A typed, algebraic approach to parsing},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314625},
doi = {10.1145/3314221.3314625},
abstract = {In this paper, we recall the definition of the context-free expressions (or µ-regular expressions), an algebraic presentation of the context-free languages. Then, we define a core type system for the context-free expressions which gives a compositional criterion for identifying those context-free expressions which can be parsed unambiguously by predictive algorithms in the style of recursive descent or LL(1). Next, we show how these typed grammar expressions can be used to derive a parser combinator library which both guarantees linear-time parsing with no backtracking and single-token lookahead, and which respects the natural denotational semantics of context-free expressions. Finally, we show how to exploit the type information to write a staged version of this library, which produces dramatic increases in performance, even outperforming code generated by the standard parser generator tool ocamlyacc.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {379–393},
numpages = {15},
keywords = {type theory, parsing, context-free languages, Kleene algebra},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@article{flap,
author = {Yallop, Jeremy and Xie, Ningning and Krishnaswami, Neel},
title = {flap: A Deterministic Parser with Fused Lexing},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591269},
doi = {10.1145/3591269},
abstract = {Lexers and parsers are typically defined separately and connected by a token stream. This separate definition is important for modularity and reduces the potential for parsing ambiguity. However, materializing tokens as data structures and case-switching on tokens comes with a cost. We show how to fuse separately-defined lexers and parsers, drastically improving performance without compromising modularity or increasing ambiguity. We propose a deterministic variant of Greibach Normal Form that ensures deterministic parsing with a single token of lookahead and makes fusion strikingly simple, and prove that normalizing context free expressions into the deterministic normal form is semantics-preserving. Our staged parser combinator library, flap, provides a standard interface, but generates specialized token-free code that runs two to six times faster than ocamlyacc on a range of benchmarks.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {155},
numpages = {24},
keywords = {fusion, lexing, multi-stage programming, optimization, parsing}
}

@inproceedings{mint,
author = {Westbrook, Edwin and Ricken, Mathias and Inoue, Jun and Yao, Yilong and Abdelatif, Tamer and Taha, Walid},
title = {Mint: Java multi-stage programming using weak separability},
year = {2010},
isbn = {9781450300193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806596.1806642},
doi = {10.1145/1806596.1806642},
abstract = {Multi-stage programming (MSP) provides a disciplined approach to run-time code generation. In the purely functional setting, it has been shown how MSP can be used to reduce the overhead of abstractions, allowing clean, maintainable code without paying performance penalties. Unfortunately, MSP is difficult to combine with imperative features, which are prevalent in mainstream languages. The central difficulty is scope extrusion, wherein free variables can inadvertently be moved outside the scopes of their binders. This paper proposes a new approach to combining MSP with imperative features that occupies a "sweet spot" in the design space in terms of how well useful MSP applications can be expressed and how easy it is for programmers to understand. The key insight is that escapes (or "anti-quotes") must be weakly separable from the rest of the code, i.e. the computational effects occurring inside an escape that are visible outside the escape are guaranteed to not contain code. To demonstrate the feasibility of this approach, we formalize a type system based on Lightweight Java which we prove sound, and we also provide an implementation, called Mint, to validate both the expressivity of the type system and the effect of staging on the performance of Java programs.},
booktitle = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {400–411},
numpages = {12},
keywords = {type systems, multi-staged languages, multi-stage programming, java},
location = {Toronto, Ontario, Canada},
series = {PLDI '10}
}


@inproceedings{metaocaml,
	abstract = {MetaOCaml is a superset of OCaml extending it with the data type for program code and operations for constructing and executing such typed code values. It has been used for compiling domain-specific languages and automating tedious and error-prone specializations of high-performance computational kernels. By statically ensuring that the generated code compiles and letting us quickly run it, MetaOCaml makes writing generators less daunting and more productive.},
	address = {Cham},
	author = {Kiselyov, Oleg},
	booktitle = {Functional and Logic Programming},
	editor = {Codish, Michael and Sumii, Eijiro},
	isbn = {978-3-319-07151-0},
	pages = {86--102},
	publisher = {Springer International Publishing},
	title = {The Design and Implementation of BER MetaOCaml},
	year = {2014}}

@article{macocaml,
author = {Xie, Ningning and White, Leo and Nicole, Olivier and Yallop, Jeremy},
title = {MacoCaml: Staging Composable and Compilable Macros},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ICFP},
url = {https://doi.org/10.1145/3607851},
doi = {10.1145/3607851},
abstract = {We introduce MacoCaml, a new design and implementation of compile-time code generation for the OCaml language. MacoCaml features a novel combination of macros with phase separation and quotation-based staging, where macros are considered as compile-time bindings, expression cross evaluation phases using staging annotations, and compile-time evaluation happens inside top-level splices. We provide a theoretical foundation for MacoCaml by formalizing a typed source calculus maco that supports interleaving typing and compile-time code generation, references with explicit compile-time heaps, and modules. We study various crucial properties including soundness and phase distinction. We have implemented MacoCaml in the OCaml compiler, and ported two substantial existing libraries to validate our implementation.},
journal = {Proc. ACM Program. Lang.},
month = aug,
articleno = {209},
numpages = {45},
keywords = {Compile-time code generation, Macros, OCaml, Staging}
}


@article{moller20,
author = {M\o{}ller, Anders and Veileborg, Oskar Haarklou},
title = {Eliminating abstraction overhead of Java stream pipelines using ahead-of-time program optimization},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428236},
doi = {10.1145/3428236},
abstract = {Java 8 introduced streams that allow developers to work with collections of data using functional-style operations. Streams are often used in pipelines of operations for processing the data elements, which leads to concise and elegant program code. However, the declarative data processing style comes at a cost. Compared to processing the data with traditional imperative language mechanisms, constructing stream pipelines requires extra heap objects and virtual method calls, which often results in significant run-time overheads.  In this work we investigate how to mitigate these overheads to enable processing data in the declarative style without sacrificing performance. We argue that ahead-of-time bytecode-to-bytecode transformation is a suitable approach to optimization of stream pipelines, and we present a static analysis that is designed to guide such transformations. Experimental results show a significant performance gain, and that the technique works for realistic stream pipelines. For 10 of 11 micro-benchmarks, the optimizer is able to produce bytecode that is as effective as hand-written imperative-style code. Additionally, 77\% of 6879 stream pipelines found in real-world Java programs are optimized successfully.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {168},
numpages = {29},
keywords = {Java 8, program optimization, static program analysis}
}
@inproceedings{10.1145/3009837.3009880,
author = {Kiselyov, Oleg and Biboudis, Aggelos and Palladinos, Nick and Smaragdakis, Yannis},
title = {Stream fusion, to completeness},
year = {2017},
isbn = {9781450346603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009837.3009880},
doi = {10.1145/3009837.3009880},
abstract = {Stream processing is mainstream (again): Widely-used stream libraries are now available for virtually all modern OO and functional languages, from Java to C# to Scala to OCaml to Haskell. Yet expressivity and performance are still lacking. For instance, the popular, well-optimized Java 8 streams do not support the zip operator and are still an order of magnitude slower than hand-written loops.  We present the first approach that represents the full generality of stream processing and eliminates overheads, via the use of staging. It is based on an unusually rich semantic model of stream interaction. We support any combination of zipping, nesting (or flat-mapping), sub-ranging, filtering, mapping—of finite or infinite streams. Our model captures idiosyncrasies that a programmer uses in optimizing stream pipelines, such as rate differences and the choice of a “for” vs. “while” loops. Our approach delivers hand-written–like code, but automatically. It explicitly avoids the reliance on black-box optimizers and sufficiently-smart compilers, offering highest, guaranteed and portable performance.  Our approach relies on high-level concepts that are then readily mapped into an implementation. Accordingly, we have two distinct implementations: an OCaml stream library, staged via MetaOCaml, and a Scala library for the JVM, staged via LMS. In both cases, we derive libraries richer and simultaneously many tens of times faster than past work. We greatly exceed in performance the standard stream libraries available in Java, Scala and OCaml, including the well-optimized Java 8 streams.},
booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
pages = {285–299},
numpages = {15},
keywords = {Code generation, multi-stage programming, optimization, stream fusion, streams},
location = {Paris, France},
series = {POPL '17}
}

@article{strymonas,
author = {Kiselyov, Oleg and Biboudis, Aggelos and Palladinos, Nick and Smaragdakis, Yannis},
title = {Stream fusion, to completeness},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093333.3009880},
doi = {10.1145/3093333.3009880},
abstract = {Stream processing is mainstream (again): Widely-used stream libraries are now available for virtually all modern OO and functional languages, from Java to C# to Scala to OCaml to Haskell. Yet expressivity and performance are still lacking. For instance, the popular, well-optimized Java 8 streams do not support the zip operator and are still an order of magnitude slower than hand-written loops.  We present the first approach that represents the full generality of stream processing and eliminates overheads, via the use of staging. It is based on an unusually rich semantic model of stream interaction. We support any combination of zipping, nesting (or flat-mapping), sub-ranging, filtering, mapping—of finite or infinite streams. Our model captures idiosyncrasies that a programmer uses in optimizing stream pipelines, such as rate differences and the choice of a “for” vs. “while” loops. Our approach delivers hand-written–like code, but automatically. It explicitly avoids the reliance on black-box optimizers and sufficiently-smart compilers, offering highest, guaranteed and portable performance.  Our approach relies on high-level concepts that are then readily mapped into an implementation. Accordingly, we have two distinct implementations: an OCaml stream library, staged via MetaOCaml, and a Scala library for the JVM, staged via LMS. In both cases, we derive libraries richer and simultaneously many tens of times faster than past work. We greatly exceed in performance the standard stream libraries available in Java, Scala and OCaml, including the well-optimized Java 8 streams.},
journal = {SIGPLAN Not.},
month = jan,
pages = {285–299},
numpages = {15},
keywords = {Code generation, multi-stage programming, optimization, stream fusion, streams}
}

@misc{scalamacros,
  title = {Macros},
  howpublished = {\url{https://dotty.epfl.ch/docs/reference/metaprogramming/macros.html}},
  note = {Accessed: 2025-03-17}
}

@inproceedings{templatehaskell,
author = {Sheard, Tim and Jones, Simon Peyton},
title = {Template meta-programming for Haskell},
year = {2002},
isbn = {1581136056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581690.581691},
doi = {10.1145/581690.581691},
abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.},
booktitle = {Proceedings of the 2002 ACM SIGPLAN Workshop on Haskell},
pages = {1–16},
numpages = {16},
keywords = {meta programming, templates},
location = {Pittsburgh, Pennsylvania},
series = {Haskell '02}
}

@inproceedings{parreaux17,
author = {Parreaux, Lionel and Shaikhha, Amir and Koch, Christoph E.},
title = {Quoted staged rewriting: a practical approach to library-defined optimizations},
year = {2017},
isbn = {9781450355247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136040.3136043},
doi = {10.1145/3136040.3136043},
abstract = {Staging has proved a successful technique for programmatically removing code abstractions, thereby allowing for faster program execution while retaining a high-level interface for the programmer. Unfortunately, techniques based on staging suffer from a number of problems — ranging from practicalities to fundamental limitations — which have prevented their widespread adoption. We introduce Quoted Staged Rewriting (QSR), an approach that uses type-safe, pattern matching-enabled quasiquotes to define optimizations. The approach is “staged” in two ways: first, rewrite rules can execute arbitrary code during pattern matching and code reconstruction, leveraging the power and flexibility of staging; second, library designers can orchestrate the application of successive rewriting phases (stages). The advantages of using quasiquote-based rewriting are that library designers never have to deal directly with the intermediate representation (IR), and that it allows for non-intrusive optimizations — in contrast with staging, it is not necessary to adapt the entire library and user programs to accommodate optimizations. We show how Squid, a Scala macro-based framework, enables QSR and renders library-defined optimizations more practical than ever before: library designers write domain-specific optimizers that users invoke transparently on delimited portions of their code base. As a motivating example we describe an implementation of stream fusion (a well-known deforestation technique) that is both simpler and more powerful than the state of the art, and can readily be used by Scala programmers with no knowledge of metaprogramming.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {131–145},
numpages = {15},
keywords = {Staging, Rewrite Rules, Optimization},
location = {Vancouver, BC, Canada},
series = {GPCE 2017}
}

@phdthesis{parreauxthesis,
	abstractnote = {Software engineering practices have been steadily moving towards higher-level programming languages and away from lower-level ones. High-level languages tend to greatly improve safety, productivity, and code maintainability because they handle various implementation details automatically, allowing programmers to focus on their problem domains. However, the gains offered by high-level languages are usually made at the cost of reduced performance: higher-level languages usually consume more memory, run more slowly and require expensive garbage-collecting runtime systems. This trend has been worsening with the increasing adoption of the functional programming paradigm by the industry. Modern programmers are thus faced with a dilemma: should they favor productivity and lower maintenance costs at the expense of performance, or should they focus on performance, to the detriments of almost everything else? The main idea behind this thesis is that we can help solve this dilemma by making advances in type systems, metaprogramming, and compilers technology. In particular, we study how metaprogramming via statically-typed quasiquotation can let programmers define their own domain-specific optimizations in a safe way, while leveraging the latest advances in intermediate program representations. We present the design and implementation of the Squid metaprogramming framework, which extends the Scala programming language with multi-staged programming capabilities and more. We also present different application examples for Squid, including a polymorphic yet efficient library for linear algebra, a stream fusion engine improving on the state of the art, a demonstration of query compilation by rewriting, a staged SQL database system prototype, and a new embedded domain-specific language for expressing queries over collections of data.},
	address = {Lausanne},
	author = {Parreaux, Lionel Emile Vincent},
	doi = {10.5075/epfl-thesis-10285},
	keywords = {type systems | metaprogramming | optimization | domain-specific languages},
	language = {en},
	school = {EPFL},
	title = {Type-Safe Metaprogramming and Compilation Techniques For Designing Efficient Systems in High-Level Languages},
	url = {https://infoscience.epfl.ch/handle/20.500.14299/173432},
	year = {2020},
	bdsk-url-1 = {https://infoscience.epfl.ch/handle/20.500.14299/173432},
	bdsk-url-2 = {https://doi.org/10.5075/epfl-thesis-10285}}


@misc{jet12,
	abstractnote = {Cluster computing systems today impose a trade-off between generality, performance and productivity. Hadoop and Dryad force programmers to write low level programs that are tedious to compose but easy to optimize. Systems like Dryad/LINQ and Spark allow concise modeling of user programs but do not apply relational optimizations. Pig and Hive restrict the language to achieve relational optimizations, making complex programs hard to express without user extensions. However, these extensions are cumbersome to write and disallow program optimizations. We present a distributed batch data processing framework called Jet. Jet uses deep language embedding in Scala, multi-stage programming and explicit side effect tracking to analyze the structure of user programs. The analysis is used to apply projection insertion, which eliminates unused data, as well as code motion and operation fusion to highly optimize the performance critical path of the program. The language embedding and a high-level interface allow Jet programs to be both expressive, resembling regular Scala code, and optimized. Its modular design allows users to extend Jet with modules that produce good performing code. Through a modular code generation scheme, Jet can generate programs for both Spark and Hadoop. Compared with na{\"\i}ve implementations we achieve 143% speedups on Spark and 126% on Hadoop.},
	author = {Ackermann, Stefan and Jovanovic, Vojin and Rompf, Tiark and Odersky, Martin},
	keywords = {Domain-specific Languages | Multi-stage Programming | MapReduce | Operation Fusion | Projection Insertion},
	title = {Jet: An Embedded DSL for High Performance Big Data Processing},
	url = {https://infoscience.epfl.ch/handle/20.500.14299/85985},
	year = {2012},
	bdsk-url-1 = {https://infoscience.epfl.ch/handle/20.500.14299/85985}}

@inproceedings{bawden99,
  abstract = {Quasiquotation is the technology commonly used in Lisp to write program-generating programs. This paper explains how quasiquotation works, why it works well, and what its limitations are. A brief history of quasiquotation is included.},
  added-at = {2007-02-14T20:23:55.000+0100},
  author = {Bawden, Alan},
  biburl = {https://www.bibsonomy.org/bibtex/2199c718239813697082c78606938e0ae/tmalsburg},
  booktitle = {Partial Evaluation and Semantic-Based Program Manipulation},
  description = {Quasiquotation in Lisp - Bawden (ResearchIndex)},
  interhash = {014f52b9318aa036f1ab5af5851c0e12},
  intrahash = {199c718239813697082c78606938e0ae},
  keywords = {lisp macros},
  pages = {4-12},
  timestamp = {2007-02-14T20:23:55.000+0100},
  title = {Quasiquotation in Lisp},
  url = {citeseer.ist.psu.edu/bawden99quasiquotation.html},
  year = 1999
}

@article{cmtt,
author = {Nanevski, Aleksandar and Pfenning, Frank and Pientka, Brigitte},
title = {Contextual modal type theory},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1529-3785},
url = {https://doi.org/10.1145/1352582.1352591},
doi = {10.1145/1352582.1352591},
abstract = {The intuitionistic modal logic of necessity is based on the judgmental notion of categorical truth. In this article we investigate the consequences of relativizing these concepts to explicitly specified contexts. We obtain contextual modal logic and its type-theoretic analogue. Contextual modal type theory provides an elegant, uniform foundation for understanding metavariables and explicit substitutions. We sketch some applications in functional programming and logical frameworks.},
journal = {ACM Trans. Comput. Logic},
month = jun,
articleno = {23},
numpages = {49},
keywords = {logical frameworks, intuitionistic modal logic, Type theory}
}

@article{davies-pfenning,
author = {Davies, Rowan and Pfenning, Frank},
title = {A modal analysis of staged computation},
year = {2001},
issue_date = {May 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/382780.382785},
doi = {10.1145/382780.382785},
abstract = {We show that a type system based on the intuitionistic modal logic S4 provides an expressive framework for specifying and analyzing computation stages in the context of typed λ-calculi and functional languages. We directly demonstrate the sense in which our λe→□-calculus captures staging, and also give a conservative embeddng of Nielson and Nielson's two-level functional language in our functional language Mini-ML□, thus proving that binding-time correctness is equivalent to modal correctness on this fragment. In addition, Mini-ML□ can also express immediate evaluation and sharing of code across multiple stages, thus supporting run-time code generation as well as partial evaluation.},
journal = {J. ACM},
month = may,
pages = {555–604},
numpages = {50},
keywords = {staged computation, run-time code generation, binding times}
}


@article{flatt12,
author = {Flatt, Matthew},
title = {Creating languages in Racket},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2063176.2063195},
doi = {10.1145/2063176.2063195},
abstract = {Sometimes you just have to make a better mousetrap.},
journal = {Commun. ACM},
month = jan,
pages = {48–56},
numpages = {9}
}


@inproceedings{samth11,
author = {Tobin-Hochstadt, Sam and St-Amour, Vincent and Culpepper, Ryan and Flatt, Matthew and Felleisen, Matthias},
title = {Languages as libraries},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993514},
doi = {10.1145/1993498.1993514},
abstract = {Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's string-based macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope. The design of Racket---a descendant of Scheme---goes even further with the introduction of a full-fledged interface to the static semantics of the language. A Racket extension programmer can thus add constructs that are indistinguishable from "native" notation, large and complex embedded domain-specific languages, and even optimizing transformations for the compiler backend. This power to experiment with language design has been used to create a series of sub-languages for programming with first-class classes and modules, numerous languages for implementing the Racket system, and the creation of a complete and fully integrated typed sister language to Racket's untyped base language.This paper explains Racket's language extension API via an implementation of a small typed sister language. The new language provides a rich type system that accommodates the idioms of untyped Racket. Furthermore, modules in this typed language can safely exchange values with untyped modules. Last but not least, the implementation includes a type-based optimizer that achieves promising speedups. Although these extensions are complex, their Racket implementation is just a library, like any other library, requiring no changes to the Racket implementation.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {132–141},
numpages = {10},
keywords = {extensible languages, macros, modules, typed racket},
location = {San Jose, California, USA},
series = {PLDI '11}
}


@phdthesis{skthesis,
author = {Krishnamurthi, Shriram and Felleisen, Matthias},
title = {Linguistic reuse},
year = {2001},
isbn = {0493326707},
abstract = {Programmers employ a multitude of languages to build systems. Some are general-purpose languages. Others are specific to individual domains. These assist programmers with at least three different tasks: domain modeling, system validation and representing the structure of their general purpose program. As a result, programming languages have become key factors in the software engineering process. They are, however, rarely codified into the process and treated systematically. My dissertation develops a framework to treat programming languages as software engineering artifacts. In this framework, languages are identifiable, reusable entities that programmers can compose and link to produce larger languages; furthermore, languages themselves meet the properties of software components. Programmers can augment this lateral growth of languages with vertical growth, by producing languages that synthesize languages. Thus, software construction becomes a multi-phase process. In later phases, programmers use languages to build programs; in earlier phases, they employ languages to construct languages. This treatment of languages as artifacts addresses several open questions.},
note = {AAI3021152}
}


@article{davies17,
author = {Davies, Rowan},
title = {A Temporal Logic Approach to Binding-Time Analysis},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/3011069},
doi = {10.1145/3011069},
abstract = {This article demonstrates that there is a fundamental relationship between temporal logic and languages that involve multiple stages, such as those used to analyze binding times in the context of partial evaluation. This relationship is based on an extension of the Curry-Howard isomorphism, which identifies proofs with programs, and propositions with types. Our extension involves the “next time” (○) operator from linear-time temporal logic and yields a λ-calculus λ° with types of the form ○ A for expressions in the subsequent stage, with appropriate introduction and elimination forms. We demonstrate that λ° is equivalent to the core of a previously studied multilevel binding-time analysis. This is similar to work by Davies and Pfenning on staged computation based on the necessity (□) operator of modal logic, but □ only allows closed code, and naturally supports a code evaluation construct, whereas ○ captures open code, thus is more flexible, but is incompatible with such a construct. Instead, code evaluation is an external global operation that is validated by the proof theory regarding closed proofs of ○ formulas. We demonstrate the relevance of λ° to staged computation directly by showing that that normalization can be done in an order strictly following the times of the logic. We also extend λ° to small functional language and show that it would serve as a suitable basis for directly programming with multiple stages by presenting some example programs.},
journal = {J. ACM},
month = mar,
articleno = {1},
numpages = {45},
keywords = {Propositions as types, partial evaluation, staged computation}
}

@misc{lemiremicrobenchmarks,
  author = {Lemire, Daniel},
  title = {testingRNG},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/lemire/testingRNG}},
  commit = {bfd776b}
}

@article{lehmerrng,
author = {Payne, W. H. and Rabung, J. R. and Bogyo, T. P.},
title = {Coding the Lehmer pseudo-random number generator},
year = {1969},
issue_date = {Feb. 1969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/362848.362860},
doi = {10.1145/362848.362860},
abstract = {An algorithm and coding technique is presented for quick evaluation of the Lehmer pseudo-random number generator modulo 2 ** 31 - 1, a prime Mersenne number which produces 2 ** 31 - 2 numbers, on a p-bit (greater than 31) computer. The computation method is extendible to limited problems in modular arithmetic. Prime factorization for 2 ** 61 - 2 and a primitive root for 2 ** 61 - 1, the next largest prime Mersenne number, are given for possible construction of a pseudo-random number generator of increased cycle length.},
journal = {Commun. ACM},
month = feb,
pages = {85–86},
numpages = {2},
keywords = {uniform probability density, uniform frequency function, simulation, random number, pseudo-random number, primitive roots, prime factorization, modular arithmetic}
}

@misc{wyrand,
  author = {Yi, Wang},
  title = {wyhash},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/wangyi-fudan/wyhash}},
  commit = {46cebe9}
}

@misc{xorshiro,
      title={Scrambled Linear Pseudorandom Number Generators}, 
      author={David Blackman and Sebastiano Vigna},
      year={2022},
      eprint={1805.01407},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/1805.01407}, 
}

@misc{bq,
  author = {Jane Street},
  title = {Base Quickcheck},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/janestreet/base_quickcheck}},
  commit = {e429ad8}
}

@misc{sc,
  title = {ScalaCheck},
  howpublished = {\url{https://scalacheck.org/}},
  note = {Accessed: 2025-03-24}
}

@article{moggi91,
  title={Notions of computation and monads},
  author={Moggi, Eugenio},
  journal={Information and computation},
  volume={93},
  number={1},
  pages={55--92},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{quickcheck00,
author = {Claessen, Koen and Hughes, John},
title = {QuickCheck: a lightweight tool for random testing of Haskell programs},
year = {2000},
isbn = {1581132026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/351240.351266},
doi = {10.1145/351240.351266},
abstract = {Quick Check is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.},
booktitle = {Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming},
pages = {268–279},
numpages = {12},
series = {ICFP '00}
}

@inproceedings{gill93,
author = {Gill, Andrew and Launchbury, John and Peyton Jones, Simon L.},
title = {A short cut to deforestation},
year = {1993},
isbn = {089791595X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/165180.165214},
doi = {10.1145/165180.165214},
booktitle = {Proceedings of the Conference on Functional Programming Languages and Computer Architecture},
pages = {223–232},
numpages = {10},
location = {Copenhagen, Denmark},
series = {FPCA '93}
}

@book{geneticalgorithm,
author = {Holland, John H.},
title = {Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence},
year = {1992},
isbn = {0262082136},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {From the Publisher:Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications. In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics. Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements. John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and isDirector of the University of Michigan/Santa Fe Institute Advanced Research Program.}
}

@inproceedings{splitmix,
author = {Steele, Guy L. and Lea, Doug and Flood, Christine H.},
title = {Fast splittable pseudorandom number generators},
year = {2014},
isbn = {9781450325851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660193.2660195},
doi = {10.1145/2660193.2660195},
abstract = {We describe a new algorithm SplitMix for an object-oriented and splittable pseudorandom number generator (PRNG) that is quite fast: 9 64-bit arithmetic/logical operations per 64 bits generated. A conventional linear PRNG object provides a generate method that returns one pseudorandom value and updates the state of the PRNG, but a splittable PRNG object also has a second operation, split, that replaces the original PRNG object with two (seemingly) independent PRNG objects, by creating and returning a new such object and updating the state of the original object. Splittable PRNG objects make it easy to organize the use of pseudorandom numbers in multithreaded programs structured using fork-join parallelism. No locking or synchronization is required (other than the usual memory fence immediately after object creation). Because the generate method has no loops or conditionals, it is suitable for SIMD or GPU implementation.We derive SplitMix from the DotMix algorithm of Leiserson, Schardl, and Sukha by making a series of program transformations and engineering improvements. The end result is an object-oriented version of the purely functional API used in the Haskell library for over a decade, but SplitMix is faster and produces pseudorandom sequences of higher quality; it is also far superior in quality and speed to java.util.Random, and has been included in Java JDK8 as the class java.util.SplittableRandom.We have tested the pseudorandom sequences produced by SplitMix using two standard statistical test suites (DieHarder and TestU01) and they appear to be adequate for "everyday" use, such as in Monte Carlo algorithms and randomized data structures where speed is important.},
booktitle = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications},
pages = {453–472},
numpages = {20},
keywords = {streams, splittable data structures, spliterator, scala, recursive splitting, random number generator, pseudorandom, pedigree, parallel computing, object-oriented, nondeterminism, multithreading, java, determinism, collections},
location = {Portland, Oregon, USA},
series = {OOPSLA '14}
}

@misc{jsf,
  title = {A small noncryptographic pseudorandom number generator},
  howpublished = {\url{https://burtleburtle.net/bob/rand/smallprng.html}},
  note = {Accessed: 2025-03-24}
}

@article{vn51,
  title={Various Techniques Used in Connection with Random Digits},
  author={Von Neumann, John},
  journal={Appl. Math Ser},
  volume={12},
  number={36-38},
  pages={3},
  year={1951}
}