@inproceedings{abrialFormalMethodsIndustry2006,
  title = {Formal Methods in Industry: Achievements, Problems, Future},
  shorttitle = {Formal Methods in Industry},
  booktitle = {Proceedings of the 28th International Conference on {{Software}} Engineering},
  author = {Abrial, Jean-Raymond},
  year = {2006},
  month = may,
  series = {{{ICSE}} '06},
  pages = {761--768},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1134285.1134406},
  urldate = {2023-06-26},
  abstract = {Two real projects using the B formal method are quickly presented. They show how some important parts of complex systems can be developed in such a way that the outcome is "correct by construction". A number of factors are then analyzed relating the pros, the cons, and the difficulties in applying this approach in Industry.},
  isbn = {978-1-59593-375-1},
  keywords = {B,correctness,development process,formal method,train system},
  file = {/Users/harrison/Zotero/storage/Z45XK7IK/Abrial - 2006 - Formal methods in industry achievements, problems.pdf}
}

@inproceedings{acarSelfadjustingComputationOverview2009,
  title = {Self-Adjusting Computation: (An Overview)},
  shorttitle = {Self-Adjusting Computation},
  booktitle = {Proceedings of the 2009 {{ACM SIGPLAN}} Workshop on {{Partial}} Evaluation and Program Manipulation},
  author = {Acar, Umut A.},
  year = {2009},
  month = jan,
  series = {{{PEPM}} '09},
  pages = {1--6},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1480945.1480946},
  urldate = {2024-03-25},
  abstract = {Many applications need to respond to incremental modifications to data. Being incremental, such modification often require incremental modifications to the output, making it possible to respond to them asymptotically faster than recomputing from scratch. In many cases, taking advantage of incrementality therefore dramatically improves performance, especially as the input size increases. As a frame of reference, note that in parallel computing speedups are bounded by the number of processors, often a (small) constant. Designing and developing applications that respond to incremental modifications, however, is challenging: it often involves developing highly specific, complex algorithms. Self-adjusting computation offers a linguistic approach to this problem. In self-adjusting computation, programs respond automatically and efficiently to modifications to their data by tracking the dynamic data dependences of the computation and incrementally updating their output as needed. In this invited talk, I present an overview of self-adjusting computation and briefly discuss the progress in developing the approach and present some recent advances.},
  isbn = {978-1-60558-327-3},
  keywords = {asymptotic complexity,change propagation,compilers,continuations,dependence graphs,incremental modification,language design,performance,self-adjusting computation},
  file = {/Users/harrison/Zotero/storage/AZJ8ZK9M/Acar - 2009 - Self-adjusting computation (an overview).pdf}
}

@inproceedings{adamsRefactoringProofsTactician2015,
  title = {Refactoring {{Proofs}} with {{Tactician}}},
  booktitle = {Software {{Engineering}} and {{Formal Methods}}},
  author = {Adams, Mark},
  editor = {Bianculli, Domenico and Calinescu, Radu and Rumpe, Bernhard},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {53--67},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-49224-6_6},
  abstract = {Tactician is a tool for refactoring tactic proof scripts for the HOL Light theorem prover. Its core operations are packaging up a series of tactic steps into a compact proof with tactical connectives, and the reverse operation of unravelling compact proofs into interactive steps. This can be useful for novices learning from legacy proof scripts, as well as for experienced users maintaining their proofs. In this paper, we give an overview of Tactician's core capabilities and provide insight into how it is implemented.},
  isbn = {978-3-662-49224-6},
  langid = {english},
  keywords = {Connecting Maneuvers,Hip Roof,Proof Script,Proof Tactics,Tactical Steps},
  file = {/Users/harrison/Zotero/storage/UZC3SGVZ/Adams - 2015 - Refactoring Proofs with Tactician.pdf}
}

@inproceedings{ahmanDijkstraMonadsFree2017,
  title = {Dijkstra Monads for Free},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Ahman, Danel and Hri{\c t}cu, C{\u a}t{\u a}lin and Maillard, Kenji and Mart{\'i}nez, Guido and Plotkin, Gordon and Protzenko, Jonathan and Rastogi, Aseem and Swamy, Nikhil},
  year = {2017},
  month = jan,
  series = {{{POPL}} '17},
  pages = {515--529},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3009837.3009878},
  urldate = {2023-07-13},
  abstract = {Dijkstra monads enable a dependent type theory to be enhanced with support for specifying and verifying effectful code via weakest preconditions. Together with their closely related counterparts, Hoare monads, they provide the basis on which verification tools like F*, Hoare Type Theory (HTT), and Ynot are built. We show that Dijkstra monads can be derived "for free" by applying a continuation-passing style (CPS) translation to the standard monadic definitions of the underlying computational effects. Automatically deriving Dijkstra monads in this way provides a correct-by-construction and efficient way of reasoning about user-defined effects in dependent type theories. We demonstrate these ideas in EMF*, a new dependently typed calculus, validating it via both formal proof and a prototype implementation within F*. Besides equipping F* with a more uniform and extensible effect system, EMF* enables a novel mixture of intrinsic and extrinsic proofs within F*.},
  isbn = {978-1-4503-4660-3},
  keywords = {dependent types,effectful programming,proof assistants,verification},
  file = {/Users/harrison/Zotero/storage/5BAPKRK3/Ahman et al. - 2017 - Dijkstra monads for free.pdf}
}

@inproceedings{alaboudiHypothesizerHypothesisBasedDebugger2023,
  title = {Hypothesizer: {{A Hypothesis-Based Debugger}} to {{Find}} and {{Test Debugging Hypotheses}}},
  shorttitle = {Hypothesizer},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Alaboudi, Abdulaziz and Latoza, Thomas D.},
  year = {2023},
  month = oct,
  series = {{{UIST}} '23},
  pages = {1--14},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3586183.3606781},
  urldate = {2023-10-31},
  abstract = {When software defects occur, developers begin the debugging process by formulating hypotheses to explain the cause. These hypotheses guide the investigation process, determining which evidence developers gather to accept or reject the hypothesis, such as parts of the code and program state developers examine. However, existing debugging techniques do not offer support in finding relevant hypotheses, leading to wasted time testing hypotheses and examining code that ultimately does not lead to a fix. To address this issue, we introduce a new type of debugging tool, the hypothesis-based debugger, and an implementation of this tool in Hypothesizer. Hypothesis-based debuggers support developers from the beginning of the debugging process by finding relevant hypotheses until the defect is fixed. To debug using Hypothesizer, developers first demonstrate the defect, generating a recording of the program behavior with code execution, user interface events, network communications, and user interface changes. Based on this information and the developer's descriptions of the symptoms, Hypothesizer finds relevant hypotheses, analyzes the code to identify relevant evidence to test the hypothesis, and generates an investigation plan through a timeline view. This summarizes all evidence items related to the hypothesis, indicates whether the hypothesis is likely to be true by showing which evidence items were confirmed in the recording, and enables the developer to quickly check evidence in the recording by viewing code snippets for each evidence item. A randomized controlled experiment with 16 professional developers found that, compared to traditional debugging tools and techniques such as breakpoint debuggers and Stack Overflow, Hypothesizer dramatically improved the success rate of fixing defects by a factor of five and decreased the time to debug by a factor of three.},
  isbn = {9798400701320},
  keywords = {debugging,debugging hypotheses,debugging tools},
  file = {/Users/harrison/Zotero/storage/YXHGRJDN/Alaboudi and Latoza - 2023 - Hypothesizer A Hypothesis-Based Debugger to Find .pdf}
}

@article{algebCOMMENTATIONESMATHEMATICAEUNIVERSITATIS,
  title = {{{COMMENTATIONES MATHEMATICAE UNIVERSITATIS CAROLINAE}} 16,2 (19715)},
  author = {Algeb, Ee},
  abstract = {Given a functor F: X---{$\bullet$} \% , a category of F-algebras is defined and the existence of free F-algebras is discussed. This yields, under general conditions a characterization of input processes in the sense of Arbib, Manes or of free monads in the sense of Barr. The characterization is very simple: if F preserves monies then it is an input process iff for every object A there exists an object B with B a A v FB .},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/4FAX3DUW/Algeb - COMMENTATIONES MATHEMATICAE UNIVERSITATIS CAROLINA.pdf}
}

@misc{amazonwebservicesCedarLanguage2023,
  title = {Cedar {{Language}}},
  author = {Amazon Web Services},
  year = {2023},
  journal = {Cedar},
  urldate = {2023-10-05},
  howpublished = {https://www.cedarpolicy.com/en},
  file = {/Users/harrison/Zotero/storage/62XS4TSN/en.html}
}

@unpublished{andoniEvaluatingSmallScope2002,
  type = {Paper},
  title = {Evaluating the ``{{Small Scope Hypothesis}}''},
  author = {Andoni, Alexandr and Daniliuc, Dumitru and Khurshid, Sarfraz and Marinov, Darko},
  year = {2002},
  abstract = {The ``small scope hypothesis'' argues that a high proportion of bugs can be found by testing the program for all test inputs within some small scope. In object-oriented programs, a test input is constructed from objects of different {\texttimes} {\texttimes} classes; a test input is within a scope of if at most objects of any given class appear in it. If the hypothesis holds, it follows that it is more effective to do systematic testing within a small scope than to generate fewer test inputs of a larger scope.},
  langid = {english},
  annotation = {Accessed online},
  file = {/Users/harrison/Zotero/storage/FD6FHXGM/Andoni et al. - Evaluating the “Small Scope Hypothesis”.pdf}
}

@inproceedings{andriushchenkoPAYNTToolInductive2021,
  title = {{{PAYNT}}: {{A Tool}} for {{Inductive Synthesis}} of {{Probabilistic Programs}}},
  shorttitle = {{{PAYNT}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Andriushchenko, Roman and {\v C}e{\v s}ka, Milan and Junges, Sebastian and Katoen, Joost-Pieter and Stupinsk{\'y}, {\v S}imon},
  editor = {Silva, Alexandra and Leino, K. Rustan M.},
  year = {2021},
  pages = {856--869},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-81685-8_40},
  abstract = {This paper presents PAYNT, a tool to automatically synthesise probabilistic programs. PAYNT enables the synthesis of finite-state probabilistic programs from a program sketch representing a finite family of program candidates. A tight interaction between inductive oracle-guided methods with state-of-the-art probabilistic model checking is at the heart of PAYNT. These oracle-guided methods effectively reason about all possible candidates and synthesise programs that meet a given specification formulated as a conjunction of temporal logic constraints and possibly including an optimising objective. We demonstrate the performance and usefulness of PAYNT using several case studies from different application domains; e.g., we find the optimal randomized protocol for network stabilisation among 3M potential programs within minutes, whereas alternative approaches would need days to do so.},
  isbn = {978-3-030-81685-8},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/4XR5NJTM/Andriushchenko et al. - 2021 - PAYNT A Tool for Inductive Synthesis of Probabili.pdf}
}

@inproceedings{andronickLargescaleFormalVerification2012,
  title = {Large-Scale Formal Verification in Practice: {{A}} Process Perspective},
  shorttitle = {Large-Scale Formal Verification in Practice},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Andronick, June and Jeffery, Ross and Klein, Gerwin and Kolanski, Rafal and Staples, Mark and Zhang, He and Zhu, Liming},
  year = {2012},
  month = jun,
  pages = {1002--1011},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2012.6227120},
  abstract = {The L4.verified project was a rare success in large-scale, formal verification: it provided a formal, machine-checked, code-level proof of the full functional correctness of the seL4 microkernel. In this paper we report on the development process and management issues of this project, highlighting key success factors. We formulate a detailed descriptive model of its middle-out development process, and analyze the evolution and dependencies of code and proof artifacts. We compare our key findings on verification and re-verification with insights from other verification efforts in the literature. Our analysis of the project is based on complete access to project logs, meeting notes, and version control data over its entire history, including its long-term, ongoing maintenance phase. The aim of this work is to aid understanding of how to successfully run large-scale formal software verification projects.},
  keywords = {Abstracts,Analytical models,Computer bugs,formal methods,Kernel,L4,Maintenance engineering,microkernel,program verification,Prototypes,software process},
  file = {/Users/harrison/Zotero/storage/XFPSJ73R/Andronick et al. - 2012 - Large-scale formal verification in practice A pro.pdf;/Users/harrison/Zotero/storage/Q4FZE96V/6227120.html}
}

@article{anicheHowDevelopersEngineer2022,
  title = {How {{Developers Engineer Test Cases}}: {{An Observational Study}}},
  shorttitle = {How {{Developers Engineer Test Cases}}},
  author = {Aniche, Maur{\'i}cio and Treude, Christoph and Zaidman, Andy},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {12},
  pages = {4925--4946},
  issn = {1939-3520},
  doi = {10.1109/TSE.2021.3129889},
  abstract = {One of the main challenges that developers face when testing their systems lies in engineering test cases that are good enough to reveal bugs. And while our body of knowledge on software testing and automated test case generation is already quite significant, in practice, developers are still the ones responsible for engineering test cases manually. Therefore, understanding the developers' thought- and decision-making processes while engineering test cases is a fundamental step in making developers better at testing software. In this paper, we observe 13 developers thinking-aloud while testing different real-world open-source methods, and use these observations to explain how developers engineer test cases. We then challenge and augment our main findings by surveying 72 software developers on their testing practices. We discuss our results from three different angles. First, we propose a general framework that explains how developers reason about testing. Second, we propose and describe in detail the three different overarching strategies that developers apply when testing. Third, we compare and relate our observations with the existing body of knowledge and propose future studies that would advance our knowledge on the topic.},
  keywords = {Codes,Computer bugs,developer testing,Documentation,Software,software engineering,software testing,Software testing,Task analysis,Tools},
  file = {/Users/harrison/Zotero/storage/AF9A75Q9/Aniche et al. - 2022 - How Developers Engineer Test Cases An Observation.pdf;/Users/harrison/Zotero/storage/ZP4ZAE6A/9625808.html}
}

@misc{anysphereCursor2024,
  title = {Cursor},
  author = {Anysphere},
  year = {2024},
  urldate = {2024-09-11},
  abstract = {The AI Code Editor},
  howpublished = {https://www.cursor.com/},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/R3L3TT58/www.cursor.com.html}
}

@article{appelSolutionFourColorMapProblem1977,
  title = {The {{Solution}} of the {{Four-Color-Map Problem}}},
  author = {Appel, Kenneth and Haken, Wolfgang},
  year = {1977},
  journal = {Scientific American},
  volume = {237},
  number = {4},
  eprint = {24953967},
  eprinttype = {jstor},
  pages = {108--121},
  publisher = {Scientific American, a division of Nature America, Inc.},
  issn = {0036-8733},
  urldate = {2024-07-08},
  file = {/Users/harrison/Zotero/storage/YZBHL6EL/Appel and Haken - 1977 - The Solution of the Four-Color-Map Problem.pdf}
}

@book{arcainiSearchBasedSoftwareEngineering2024,
  title = {Search-{{Based Software Engineering}}: 15th {{International Symposium}}, {{SSBSE}} 2023, {{San Francisco}}, {{CA}}, {{USA}}, {{December}} 8, 2023, {{Proceedings}}},
  shorttitle = {Search-{{Based Software Engineering}}},
  author = {Arcaini, Paolo and Yue, Tao and Fredericks, Erik M.},
  year = {2024},
  month = jan,
  publisher = {Springer Nature},
  abstract = {This book constitutes the refereed proceedings of the 15th International Symposium on Search-Based Software Engineering, SSBSE 2023, which took place in San Francisco, CA, USA, during December 8, 2023.The 7 full and 7 short papers included in this book were carefully reviewed and selected from 23 submissions. They focus on formulating various optimization problems in software engineering as search problems, addressing them with search techniques, intending to automate complex software engineering tasks.},
  googlebooks = {JnPnEAAAQBAJ},
  isbn = {978-3-031-48796-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Information Technology,Computers / Networking / Hardware,Computers / Operating Systems / General,Computers / Software Development & Engineering / General}
}

@inproceedings{arcuriAdaptiveRandomTesting2011,
  title = {Adaptive Random Testing: An Illusion of Effectiveness?},
  booktitle = {Proceedings of the 20th {{International Symposium}} on {{Software Testing}} and {{Analysis}}, {{ISSTA}} 2011, {{Toronto}}, {{ON}}, {{Canada}}, {{July}} 17-21, 2011},
  author = {Arcuri, Andrea and Briand, Lionel C.},
  year = {2011},
  pages = {265--275},
  doi = {10.1145/2001420.2001452},
  file = {/Users/harrison/Zotero/storage/R7D7EYXB/Arcuri and Briand - 2011 - Adaptive random testing an illusion of effectiven.pdf}
}

@inproceedings{arntzeniusDatafunFunctionalDatalog2016,
  title = {Datafun: A Functional {{Datalog}}},
  shorttitle = {Datafun},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  author = {Arntzenius, Michael and Krishnaswami, Neelakantan R.},
  year = {2016},
  month = sep,
  series = {{{ICFP}} 2016},
  pages = {214--227},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2951913.2951948},
  urldate = {2023-02-01},
  abstract = {Datalog may be considered either an unusually powerful query language or a carefully limited logic programming language. Datalog is declarative, expressive, and optimizable, and has been applied successfully in a wide variety of problem domains. However, most use-cases require extending Datalog in an application-specific manner. In this paper we define Datafun, an analogue of Datalog supporting higher-order functional programming. The key idea is to track monotonicity with types.},
  isbn = {978-1-4503-4219-3},
  keywords = {adjoint logic,Datalog,denotational semantics,domain-specific languages,functional programming,logic programming,operational semantics,Prolog,type theory},
  file = {/Users/harrison/Zotero/storage/Z26VGKSD/Arntzenius and Krishnaswami - 2016 - Datafun a functional Datalog.pdf}
}

@inproceedings{artsShrinkingRandomlyGenerated2014,
  title = {On Shrinking Randomly Generated Load Tests},
  booktitle = {Proceedings of the {{Thirteenth ACM SIGPLAN}} Workshop on {{Erlang}}},
  author = {Arts, Thomas},
  year = {2014},
  month = sep,
  series = {Erlang '14},
  pages = {25--31},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2633448.2633452},
  urldate = {2022-12-13},
  abstract = {Running a load test is a time consuming undertaking for which normally a complete system should be configured. In contrast to running tests to find deviations from functional requirements, the goal of load testing is to find defects that only appear when the system has a lot of load to handle during a longer time. Load testing is typically performed by increasing the load and observing the effect of doing so. It is plausible that more defects can be detected by a wider variety of scenarios to create load. This makes it an attractive idea to use QuickCheck for the generation of user scenarios to perform load testing with randomly behaving users. In this paper we show that QuickCheck can be used as a framework for doing so by introducing and discussing load generators.},
  isbn = {978-1-4503-3038-1},
  keywords = {load testing,model-based testing,property-based testing,quickcheck},
  file = {/Users/harrison/Zotero/storage/WCBJ4UKK/Arts - 2014 - On shrinking randomly generated load tests.pdf}
}

@inproceedings{artsTestingAUTOSARSoftware2015,
  title = {Testing {{AUTOSAR}} Software with {{QuickCheck}}},
  booktitle = {2015 {{IEEE Eighth International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  author = {Arts, Thomas and Hughes, John and Norell, Ulf and Svensson, Hans},
  year = {2015},
  month = apr,
  pages = {1--4},
  doi = {10.1109/ICSTW.2015.7107466},
  abstract = {AUTOSAR (AUTomotive Open System ARchitecture) is an evolving standard for embedded software in vehicles, defined by the automotive industry, and implemented by many different vendors. On behalf of Volvo Cars, we have developed model-based acceptance tests for some critical AUTOSAR components, to guarantee that implementations from different vendors are compatible. We translated over 3000 pages of textual specifications into QuickCheck models, and tested many different implementations using large volumes of generated tests. This exposed over 200 issues, which we raised with Volvo and the software vendors. Compared to an earlier manual approach, ours is more efficient, more effective, and more correct.},
  keywords = {Automotive engineering,Conferences,Production,Protocols,Software,Standards,Testing},
  file = {/Users/harrison/Zotero/storage/Q3UIK2X5/Arts et al. - 2015 - Testing AUTOSAR software with QuickCheck.pdf;/Users/harrison/Zotero/storage/V2L6IYPZ/7107466.html}
}

@inproceedings{artsTestingTelecomsSoftware2006,
  title = {Testing Telecoms Software with Quviq {{QuickCheck}}},
  booktitle = {Proceedings of the 2006 {{ACM SIGPLAN}}  Workshop on {{Erlang}}},
  author = {Arts, Thomas and Hughes, John and Johansson, Joakim and Wiger, Ulf},
  year = {2006},
  month = sep,
  series = {{{ERLANG}} '06},
  pages = {2--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1159789.1159792},
  urldate = {2022-11-21},
  abstract = {We present a case study in which a novel testing tool, Quviq QuickCheck, is used to test an industrial implementation of the Megaco protocol. We considered positive and negative testing and we used our developed specification to test an old version in order to estimate how useful QuickCheck could potentially be when used early in development.The results of the case study indicate that, by using Quviq QuickCheck, we would have been able to detect faults early in the development.We detected faults that had not been detected by other testing techniques. We found unclarities in the specifications and potential faults when the software is used in a different setting. The results are considered promising enough to Ericsson that they are investing in an even larger case study, this time from the beginning of the development of a new product.},
  isbn = {978-1-59593-490-1},
  file = {/Users/harrison/Zotero/storage/RMHWCG8K/Arts et al. - 2006 - Testing telecoms software with quviq QuickCheck.pdf}
}

@inproceedings{aschermannNAUTILUSFishingDeep2019,
  title = {{{NAUTILUS}}: {{Fishing}} for {{Deep Bugs}} with {{Grammars}}},
  shorttitle = {{{NAUTILUS}}},
  booktitle = {Proceedings 2019 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Aschermann, Cornelius and Frassetto, Tommaso and Holz, Thorsten and Jauernig, Patrick and Sadeghi, Ahmad-Reza and Teuchert, Daniel},
  year = {2019},
  publisher = {Internet Society},
  address = {San Diego, CA},
  doi = {10.14722/ndss.2019.23412},
  urldate = {2023-01-04},
  abstract = {Fuzz testing is a well-known method for efficiently identifying bugs in programs. Unfortunately, when programs that require highly-structured inputs such as interpreters are fuzzed, many fuzzing methods struggle to pass the syntax checks: interpreters often process inputs in multiple stages, first syntactic and then semantic correctness is checked. Only if both checks are passed, the interpreted code gets executed. This prevents fuzzers from executing ``deeper'' --- and hence potentially more interesting --- code. Typically, two valid inputs that lead to the execution of different features in the target program require too many mutations for simple mutation-based fuzzers to discover: making small changes like bit flips usually only leads to the execution of error paths in the parsing engine. So-called grammar fuzzers are able to pass the syntax checks by using ContextFree Grammars. Feedback can significantly increase the efficiency of fuzzing engines and is commonly used in state-of-the-art mutational fuzzers which do not use grammars. Yet, current grammar fuzzers do not make use of code coverage, i.e., they do not know whether any input triggers new functionality.},
  isbn = {978-1-891562-55-6},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/YSLI63HR/Aschermann et al. - 2019 - NAUTILUS Fishing for Deep Bugs with Grammars.pdf}
}

@article{austinEffectsTimePressure2001,
  title = {The {{Effects}} of {{Time Pressure}} on {{Quality}} in {{Software Development}}: {{An Agency Model}}},
  shorttitle = {The {{Effects}} of {{Time Pressure}} on {{Quality}} in {{Software Development}}},
  author = {Austin, Robert D.},
  year = {2001},
  month = jun,
  journal = {Information Systems Research},
  publisher = {INFORMS},
  doi = {10.1287/isre.12.2.195.9699},
  urldate = {2024-04-23},
  abstract = {An agency framework is used to model the behavior of software developers as they weigh concerns about product quality against concerns about missing individual task deadlines. Developers who care a...},
  copyright = {{\copyright} 2001 INFORMS},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/ZLJ54XDG/Austin - 2001 - The Effects of Time Pressure on Quality in Softwar.pdf;/Users/harrison/Zotero/storage/XVDA8MR6/isre.12.2.195.html}
}

@inproceedings{aydemirMechanizedMetatheoryMasses2005a,
  title = {Mechanized {{Metatheory}} for the {{Masses}}: {{The PoplMark Challenge}}},
  shorttitle = {Mechanized {{Metatheory}} for the {{Masses}}},
  booktitle = {Theorem {{Proving}} in {{Higher Order Logics}}},
  author = {Aydemir, Brian E. and Bohannon, Aaron and Fairbairn, Matthew and Foster, J. Nathan and Pierce, Benjamin C. and Sewell, Peter and Vytiniotis, Dimitrios and Washburn, Geoffrey and Weirich, Stephanie and Zdancewic, Steve},
  editor = {Hurd, Joe and Melham, Tom},
  year = {2005},
  pages = {50--65},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11541868_4},
  abstract = {How close are we to a world where every paper on programming languages is accompanied by an electronic appendix with machine-checked proofs?},
  isbn = {978-3-540-31820-0},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/AMX5EYMD/Aydemir et al. - 2005 - Mechanized Metatheory for the Masses The PoplMark.pdf}
}

@article{bachPairwiseTestingBest,
  title = {Pairwise {{Testing}}: {{A Best Practice That Isn}}'t},
  author = {Bach, James and Schroeder, Patrick J},
  pages = {17},
  abstract = {Pairwise testing is a wildly popular approach to combinatorial testing problems. The number of articles and textbooks covering the topic continues to grow, as do the number of commercial and academic courses that teach the technique. Despite the technique's popularity and its reputation as a best practice, we find the technique to be over promoted and poorly understood. In this paper, we define pairwise testing and review many of the studies conducted using pairwise testing. Based on these studies and our experience with pairwise testing, we discuss weaknesses we perceive in pairwise testing. Knowledge of the weaknesses of the pairwise testing technique, or of any testing technique, is essential if we are to apply the technique wisely. We conclude by re-stating the story of pairwise testing and by warning testers against blindly accepting best practices.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/F5PG4VFS/Bach and Schroeder - Pairwise Testing A Best Practice That Isn’t.pdf}
}

@inproceedings{barbosaCvc5VersatileIndustrialStrength2022,
  title = {Cvc5: {{A Versatile}} and {{Industrial-Strength SMT Solver}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}} - 28th {{International Conference}}, {{TACAS}} 2022, {{Held}} as {{Part}} of the {{European Joint Conferences}} on {{Theory}} and {{Practice}} of {{Software}}, {{ETAPS}} 2022, {{Munich}}, {{Germany}}, {{April}} 2-7, 2022, {{Proceedings}}, {{Part I}}},
  author = {Barbosa, Haniel and Barrett, Clark W. and Brain, Martin and Kremer, Gereon and Lachnitt, Hanna and Mann, Makai and Mohamed, Abdalrhman and Mohamed, Mudathir and Niemetz, Aina and N{\"o}tzli, Andres and Ozdemir, Alex and Preiner, Mathias and Reynolds, Andrew and Sheng, Ying and Tinelli, Cesare and Zohar, Yoni},
  editor = {Fisman, Dana and Rosu, Grigore},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13243},
  pages = {415--442},
  publisher = {Springer},
  doi = {10.1007/978-3-030-99524-9_24}
}

@misc{barrett-wiltTrialsTribulationsAcademic2021,
  title = {The Trials and Tribulations of Academic Publishing -- and {{Fuzz Testing}}},
  author = {{Barrett-Wilt}, Karen},
  year = {2021},
  month = sep,
  journal = {UW. Madison Department of Computer Sciences}
}

@article{barrOracleProblemSoftware2015,
  title = {The {{Oracle Problem}} in {{Software Testing}}: {{A Survey}}},
  shorttitle = {The {{Oracle Problem}} in {{Software Testing}}},
  author = {Barr, Earl T. and Harman, Mark and McMinn, Phil and Shahbaz, Muzammil and Yoo, Shin},
  year = {2015},
  month = may,
  journal = {IEEE Transactions on Software Engineering},
  volume = {41},
  number = {5},
  pages = {507--525},
  issn = {1939-3520},
  doi = {10.1109/TSE.2014.2372785},
  abstract = {Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the ``test oracle problem''. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of test oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of test oracles, even the humble human, involve challenges of reducing cost and increasing benefit. This paper provides a comprehensive survey of current approaches to the test oracle problem and an analysis of trends in this important area of software testing research and practice.},
  keywords = {automatic testing,Automatic testing,Automation,Licenses,Market research,Probabilistic logic,Reliability,Software testing,Test oracle,testing formalism,Testing formalism},
  file = {/Users/harrison/Zotero/storage/Z3XKWJ8B/Barr et al. - 2015 - The Oracle Problem in Software Testing A Survey.pdf;/Users/harrison/Zotero/storage/6H93U4AA/6963470.html}
}

@misc{batchelderCoverageCodeCoverage2023,
  title = {Coverage: {{Code}} Coverage Measurement for {{Python}}},
  shorttitle = {Coverage},
  author = {Batchelder, Ned},
  year = {2023},
  urldate = {2023-07-18},
  copyright = {Apache Software License},
  keywords = {code,coverage,Software Development - Quality Assurance,Software Development - Testing,testing},
  file = {/Users/harrison/Zotero/storage/QZLUQIUT/coverage.html}
}

@article{bavishiAutoPandasNeuralbackedGenerators2019,
  title = {{{AutoPandas}}: Neural-Backed Generators for Program Synthesis},
  author = {Bavishi, Rohan and Lemieux, Caroline and Fox, Roy and Sen, Koushik and Stoica, Ion},
  year = {2019},
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {OOPSLA},
  pages = {1--27},
  publisher = {ACM New York, NY, USA},
  doi = {10.1145/3360594},
  file = {/Users/harrison/Zotero/storage/GJTATVWB/Bavishi et al. - 2019 - AutoPandas neural-backed generators for program s.pdf}
}

@inproceedings{beckertUsabilityEvaluationInteractive2015,
  title = {A {{Usability Evaluation}} of {{Interactive Theorem Provers Using Focus Groups}}},
  booktitle = {Software {{Engineering}} and {{Formal Methods}}},
  author = {Beckert, Bernhard and Grebing, Sarah and B{\"o}hl, Florian},
  editor = {Canal, Carlos and Idani, Akram},
  year = {2015},
  pages = {3--19},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-15201-1_1},
  abstract = {The effectiveness of interactive theorem provers (ITPs) increased such that the bottleneck in the proof process shifted from effectiveness to efficiency. While in principle large theorems are provable, it takes much effort for the user to interact with the system. A major obstacle for the user is to understand the proof state in order to guide the prover in successfully finding a proof. We conducted two~focus groups to evaluate the usability of ITPs. We wanted to evaluate the impact of the gap between the user's model of the proof and the actual proof performed by the provers' strategies. In addition, our goals are to explore which mechanisms already exist and to develop, based on the existing mechanisms, new mechanisms that help the user in bridging this gap.},
  isbn = {978-3-319-15201-1},
  langid = {english},
  keywords = {Focus Group,Java Modelling Language,Open Goal,Proof Obligation,Proof Tree},
  file = {/Users/harrison/Zotero/storage/TBYZ4836/Beckert et al. - 2015 - A Usability Evaluation of Interactive Theorem Prov.pdf}
}

@inproceedings{bellEffectivenessPairwiseMethodology2005,
  title = {On Effectiveness of Pairwise Methodology for Testing Network-Centric Software},
  booktitle = {2005 {{International Conference}} on {{Information}} and {{Communication Technology}}},
  author = {Bell, K.Z. and Vouk, M.A.},
  year = {2005},
  month = dec,
  pages = {221--235},
  issn = {2329-6372},
  doi = {10.1109/ITICT.2005.1609626},
  abstract = {Pairwise testing, which can be complemented with partial or full N-wise testing, is a technique which guarantees that all important parametric value pairs are included in a test suite. A percentage of N-wise testing is also included. We conjecture that N-wise enhanced pairwise testing can be used as a black-boxed testing method to increase effectiveness of random testing in exposing unusual or unexpected behaviors, such as security failures in network-centric software. This testing can also be quite cost-efficient since small N test suites grow linearly with the number of parameters. This paper explains the results of random testing of a simulation in which about 20\% of the defects with probabilities of occurrence less than 50\% are never exposed. This supports the premise that if the unusual or unexpected behaviors are based on defects which are less likely to occur, then random testing needs to be enhanced, especially if those unexposed defects could cause erratic or even critical behaviors to the system. Higher system complexities may indicate higher numbers of unusual or unexpected behaviors. It may be difficult to use the traditional operational profile information to determine the amount of testing for unusual behaviors since the operational usage may be 0 or close to it. Another interesting problem is that some testers lack the experience necessary to effectively analyze the results of a test run. It is important to compensate for the lack of experience so that novice testers are able to test comparatively as effectively as more experienced testers. It is believed that if the size of the test suite is relatively small, then it may be easier to pinpoint the source of a failure. The research presented in this paper is aimed at addressing some of these issues of random testing via enhanced pairwise testing and N-wise testing in general. It is possible that more complex systems, such as those that rely a great deal on a network, would require higher numbers of interactions to combat unexpected combinations for use in some testing instances such as security testing or high assurance testing. A tool is being developed concurrently to help automate a part of the test generation process.},
  file = {/Users/harrison/Zotero/storage/X6GD54P5/Bell and Vouk - 2005 - On effectiveness of pairwise methodology for testi.pdf;/Users/harrison/Zotero/storage/7GRUHI74/1609626.html}
}

@article{bellerDeveloperTestingIde2017,
  title = {Developer Testing in the Ide: {{Patterns}}, Beliefs, and Behavior},
  author = {Beller, Moritz and Gousios, Georgios and Panichella, Annibale and Proksch, Sebastian and Amann, Sven and Zaidman, Andy},
  year = {2017},
  journal = {IEEE Transactions on Software Engineering},
  volume = {45},
  number = {3},
  pages = {261--284},
  publisher = {IEEE}
}

@article{bellerDeveloperTestingIDE2019,
  title = {Developer {{Testing}} in the {{IDE}}: {{Patterns}}, {{Beliefs}}, and {{Behavior}}},
  shorttitle = {Developer {{Testing}} in the {{IDE}}},
  author = {Beller, Moritz and Gousios, Georgios and Panichella, Annibale and Proksch, Sebastian and Amann, Sven and Zaidman, Andy},
  year = {2019},
  month = mar,
  journal = {IEEE Transactions on Software Engineering},
  volume = {45},
  number = {3},
  pages = {261--284},
  issn = {1939-3520},
  doi = {10.1109/TSE.2017.2776152},
  abstract = {Software testing is one of the key activities to achieve software quality in practice. Despite its importance, however, we have a remarkable lack of knowledge on how developers test in real-world projects. In this paper, we report on a large-scale field study with 2,443 software engineers whose development activities we closely monitored over 2.5 years in four integrated development environments (IDEs). Our findings, which largely generalized across the studied IDEs and programming languages Java and C\#, question several commonly shared assumptions and beliefs about developer testing: half of the developers in our study do not test; developers rarely run their tests in the IDE; most programming sessions end without any test execution; only once they start testing, do they do it extensively; a quarter of test cases is responsible for three quarters of all test failures; 12 percent of tests show flaky behavior; Test-Driven Development (TDD) is not widely practiced; and software developers only spend a quarter of their time engineering tests, whereas they think they test half of their time. We summarize these practices of loosely guiding one's development efforts with the help of testing in an initial summary on Test-Guided Development (TGD), a behavior we argue to be closer to the development reality of most developers than TDD.},
  keywords = {Androids,Developer testing,field study,Humanoid robots,Java,JUnit,KaVE FeedBag++,Servers,Software,test-driven development (TDD),Testing,testing effort,TestRoots WatchDog,unit tests,Visualization},
  file = {/Users/harrison/Zotero/storage/6ZGIUP9C/Beller et al. - 2019 - Developer Testing in the IDE Patterns, Beliefs, a.pdf;/Users/harrison/Zotero/storage/5QK4VU9C/8116886.html}
}

@inproceedings{bellerWhenHowWhy2015,
  title = {When, How, and Why Developers (Do Not) Test in Their {{IDEs}}},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Beller, Moritz and Gousios, Georgios and Panichella, Annibale and Zaidman, Andy},
  year = {2015},
  month = aug,
  series = {{{ESEC}}/{{FSE}} 2015},
  pages = {179--190},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2786805.2786843},
  urldate = {2023-06-19},
  abstract = {The research community in Software Engineering and Software Testing in particular builds many of its contributions on a set of mutually shared expectations. Despite the fact that they form the basis of many publications as well as open-source and commercial testing applications, these common expectations and beliefs are rarely ever questioned. For example, Frederic Brooks' statement that testing takes half of the development time seems to have manifested itself within the community since he first made it in the ``Mythical Man Month'' in 1975. With this paper, we report on the surprising results of a large-scale field study with 416 software engineers whose development activity we closely monitored over the course of five months, resulting in over 13 years of recorded work time in their integrated development environments (IDEs). Our findings question several commonly shared assumptions and beliefs about testing and might be contributing factors to the observed bug proneness of software in practice: the majority of developers in our study does not test; developers rarely run their tests in the IDE; Test-Driven Development (TDD) is not widely practiced; and, last but not least, software developers only spend a quarter of their work time engineering tests, whereas they think they test half of their time.},
  isbn = {978-1-4503-3675-8},
  keywords = {Developer Testing,Field Study,Test-Driven Development (TDD),Testing Effort,Unit Tests},
  file = {/Users/harrison/Zotero/storage/LLF5EAWZ/Beller et al. - 2015 - When, how, and why developers (do not) test in the.pdf}
}

@inproceedings{berghoferRandomTestingIsabelle2004,
  title = {Random Testing in {{Isabelle}}/{{HOL}}},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Software Engineering}} and {{Formal Methods}}, 2004. {{SEFM}} 2004.},
  author = {Berghofer, S. and Nipkow, T.},
  year = {2004},
  month = sep,
  pages = {230--239},
  doi = {10.1109/SEFM.2004.1347524},
  urldate = {2023-10-05},
  abstract = {When developing non-trivial formalizations in a theorem prover, a considerable amount of time is devoted to "debugging" specifications and conjectures by failed proof attempts. To detect such problems early in the proof and save development time, we have extended the Isabelle theorem prover with a tool for testing specifications by evaluating propositions under an assignment of random values to free variables. Distribution of the test data is optimized via mutation testing. The technical contributions are an extension of earlier work with inductive definitions and a generic method for randomly generating elements of recursive datatypes.},
  file = {/Users/harrison/Zotero/storage/FHDHWDUE/1347524.html}
}

@inproceedings{blaauwbroekTactician2020,
  title = {The {{Tactician}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Blaauwbroek, Lasse and Urban, Josef and Geuvers, Herman},
  editor = {Benzm{\"u}ller, Christoph and Miller, Bruce},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {271--277},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-53518-6_17},
  abstract = {We present Tactician, a tactic learner and prover for the Coq Proof Assistant. Tactician helps users make tactical proof decisions while they retain control over the general proof strategy. To this end, Tactician learns from previously written tactic scripts and gives users either suggestions about the next tactic to be executed or altogether takes over the burden of proof synthesis. Tactician's goal is to provide users with a seamless, interactive, and intuitive experience together with robust and adaptive proof automation.},
  isbn = {978-3-030-53518-6},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/IUAFNJJN/Blaauwbroek et al. - 2020 - The Tactician.pdf}
}

@inproceedings{blackwellCognitiveDimensionsNotations2001,
  title = {Cognitive {{Dimensions}} of {{Notations}}: {{Design Tools}} for {{Cognitive Technology}}},
  shorttitle = {Cognitive {{Dimensions}} of {{Notations}}},
  booktitle = {Cognitive {{Technology}}: {{Instruments}} of {{Mind}}},
  author = {Blackwell, A. F. and Britton, C. and Cox, A. and Green, T. R. G. and Gurr, C. and Kadoda, G. and Kutar, M. S. and Loomes, M. and Nehaniv, C. L. and Petre, M. and Roast, C. and Roe, C. and Wong, A. and Young, R. M.},
  editor = {Beynon, Meurig and Nehaniv, Chrystopher L. and Dautenhahn, Kerstin},
  year = {2001},
  pages = {325--341},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44617-6_31},
  abstract = {The Cognitive Dimensions of Notations framework has been created to assist the designers of notational systems and information artifacts to evaluate their designs with respect to the impact that they will have on the users of those designs. The framework emphasizes the design choices available to such designers, including characterization of the user's activity, and the inevitable tradeoffs that will occur between potential design options. The resulting framework has been under development for over 10 years, and now has an active community of researchers devoted to it. This paper first introduces Cognitive Dimensions. It then summarizes the current activity, especially the results of a one-day workshop devoted to Cognitive Dimensions in December 2000, and reviews the ways in which it applies to the field of Cognitive Technology.},
  isbn = {978-3-540-44617-0},
  langid = {english},
  keywords = {Cognitive Dimension,Cognitive Technology,Design Choice,Design Tool,Door Knocker},
  file = {/Users/harrison/Zotero/storage/8KY2255M/Blackwell et al. - 2001 - Cognitive Dimensions of Notations Design Tools fo.pdf}
}

@incollection{blandfordAnalysingData2016,
  title = {Analysing {{Data}}},
  booktitle = {Qualitative {{HCI Research}}: {{Going Behind}} the {{Scenes}}},
  author = {Blandford, Ann and Furniss, Dominic and Makri, Stephann},
  editor = {Blandford, Ann and Furniss, Dominic and Makri, Stephann},
  year = {2016},
  series = {Synthesis {{Lectures}} on {{Human-Centered Informatics}}},
  pages = {51--60},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-02217-3_5},
  urldate = {2023-06-27},
  abstract = {Once the film footage has been gathered, then the editor's job is to organise and structure the footage to give a clear narrative and take-home message (journalists often call this the ``angle'' of a narrative). In ethnographic documentary making the specifics of the narrative are undecided until the footage has been reviewed, i.e., the themes emerge from the raw materials. The same is true of data analysis. This involves several iterations through the data to reorganise and structure it. In this chapter, for simplicity, we consider analysis independent of data gathering; in Chapter 6 we discuss interleaving of these activities.},
  isbn = {978-3-031-02217-3},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/M39JYFZK/Blandford et al. - 2016 - Analysing Data.pdf}
}

@book{blandfordQualitativeHCIResearch2016,
  title = {Qualitative {{HCI Research}}: {{Going Behind}} the {{Scenes}}},
  shorttitle = {Qualitative {{HCI Research}}},
  author = {Blandford, Ann and Furniss, Dominic and Makri, Stephann},
  year = {2016},
  series = {Synthesis {{Lectures}} on {{Human-Centered Informatics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-02217-3},
  urldate = {2023-06-27},
  isbn = {978-3-031-01089-7 978-3-031-02217-3},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/LHIKJU4T/Blandford et al. - 2016 - Qualitative HCI Research Going Behind the Scenes.pdf}
}

@inproceedings{bohmeDirectedGreyboxFuzzing2017,
  title = {Directed {{Greybox Fuzzing}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {B{\"o}hme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
  year = {2017},
  series = {{{CCS}} '17},
  pages = {2329--2344},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3133956.3134020},
  abstract = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
  isbn = {978-1-4503-4946-8},
  keywords = {coverage-based greybox fuzzing,crash reproduction,directed testing,patch testing,reachability,verifying true positives},
  file = {/Users/harrison/Zotero/storage/KHS9Y8NF/Böhme et al. - 2017 - Directed Greybox Fuzzing.pdf}
}

@inproceedings{bohmeEstimatingResidualRisk2021,
  title = {Estimating Residual Risk in Greybox Fuzzing},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {B{\"o}hme, Marcel and Liyanage, Danushka and W{\"u}stholz, Valentin},
  year = {2021},
  month = aug,
  pages = {230--241},
  publisher = {ACM},
  address = {Athens Greece},
  doi = {10.1145/3468264.3468570},
  urldate = {2024-02-07},
  abstract = {For any errorless fuzzing campaign, no matter how long, there is always some residual risk that a software error would be discovered if only the campaign was run for just a bit longer. Recently, greybox fuzzing tools have found widespread adoption. Yet, practitioners can only guess when the residual risk of a greybox fuzzing campaign falls below a specific, maximum allowable threshold.},
  isbn = {978-1-4503-8562-6},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/ZYP3PVXH/Böhme et al. - 2021 - Estimating residual risk in greybox fuzzing.pdf}
}

@article{borgesWhatGitHubStar2018,
  title = {What's in a {{GitHub Star}}? {{Understanding Repository Starring Practices}} in a {{Social Coding Platform}}},
  shorttitle = {What's in a {{GitHub Star}}?},
  author = {Borges, Hudson and Tulio Valente, Marco},
  year = {2018},
  month = dec,
  journal = {Journal of Systems and Software},
  volume = {146},
  pages = {112--129},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2018.09.016},
  urldate = {2023-02-23},
  abstract = {Besides a git-based version control system, GitHub integrates several social coding features. Particularly, GitHub users can star a repository, presumably to manifest interest or satisfaction with an open source project. However, the real and practical meaning of starring a project was never the subject of an in-depth and well-founded empirical investigation. Therefore, we provide in this paper a throughout study on the meaning, characteristics, and dynamic growth of GitHub stars. First, by surveying 791 developers, we report that three out of four developers consider the number of stars before using or contributing to a GitHub project. Then, we report a quantitative analysis on the characteristics of the top-5,000 most starred GitHub repositories. We propose four patterns to describe stars growth, which are derived after clustering the time series representing the number of stars of the studied repositories; we also reveal the perception of 115 developers about these growth patterns. To conclude, we provide a list of recommendations to open source project managers (e.g.,~on the importance of social media promotion) and to GitHub users and Software Engineering researchers (e.g., on the risks faced when selecting projects by GitHub stars).},
  langid = {english},
  keywords = {GitHub stars,Social coding,Software popularity},
  file = {/Users/harrison/Zotero/storage/BCL2LDGA/Borges and Tulio Valente - 2018 - What’s in a GitHub Star Understanding Repository .pdf;/Users/harrison/Zotero/storage/59BAW2I4/S0164121218301961.html}
}

@inproceedings{bornholtUsingLightweightFormal2021,
  title = {Using {{Lightweight Formal Methods}} to {{Validate}} a {{Key-Value Storage Node}} in {{Amazon S3}}},
  booktitle = {Proceedings of the {{ACM SIGOPS}} 28th {{Symposium}} on {{Operating Systems Principles}}},
  author = {Bornholt, James and Joshi, Rajeev and Astrauskas, Vytautas and Cully, Brendan and Kragl, Bernhard and Markle, Seth and Sauri, Kyle and Schleit, Drew and Slatton, Grant and Tasiran, Serdar and Van Geffen, Jacob and Warfield, Andrew},
  year = {2021},
  month = oct,
  series = {{{SOSP}} '21},
  pages = {836--850},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3477132.3483540},
  urldate = {2023-06-26},
  abstract = {This paper reports our experience applying lightweight formal methods to validate the correctness of ShardStore, a new key-value storage node implementation for the Amazon S3 cloud object storage service. By "lightweight formal methods" we mean a pragmatic approach to verifying the correctness of a production storage node that is under ongoing feature development by a full-time engineering team. We do not aim to achieve full formal verification, but instead emphasize automation, usability, and the ability to continually ensure correctness as both software and its specification evolve over time. Our approach decomposes correctness into independent properties, each checked by the most appropriate tool, and develops executable reference models as specifications to be checked against the implementation. Our work has prevented 16 issues from reaching production, including subtle crash consistency and concurrency problems, and has been extended by non-formal-methods experts to check new features and properties as ShardStore has evolved.},
  isbn = {978-1-4503-8709-5},
  keywords = {cloud storage,lightweight formal methods},
  file = {/Users/harrison/Zotero/storage/RZNWVWVP/Bornholt et al. - 2021 - Using Lightweight Formal Methods to Validate a Key.pdf}
}

@book{brachthauserRepresentingMonadsCapabilities2021,
  title = {Representing {{Monads}} with {{Capabilities}}},
  editor = {Brachth{\"a}user, Jonathan Immanuel and {Boruch-Gruszecki}, Aleksander Slawomir and Odersky, Martin},
  year = {2021},
  abstract = {Programming with monads can be advantageous even in imperative languages with builtin support for side effects. However, in these languages composing monadic programs is different from composing side effecting imperative programs. This does not need to be the case, as already noticed by Filinski [1994]. We revive the well-known technique of monadic reflection in the context of modern programming languages with support for fibers, generators, or coroutines. In particular, we show how (layered) monadic reflection can be implemented in a stack safe manner and how effect safety can conveniently be approximated by capability passing},
  keywords = {Capabilities,Functional Programming,Monads}
}

@techreport{braibantArtiCheckWelltypedGeneric2017,
  title = {{{ArtiCheck}}: Well-Typed Generic Fuzzing for Module Interfaces},
  author = {Braibant, Thomas and Protzenko, Jonathan and Scherer, Gabriel},
  year = {2017},
  abstract = {In spite of recent advances in program certification, testing remains a widely-used component of the software development cycle. Various flavors of testing exist: popular ones include unit testing, which consists in manually crafting test cases for specific parts of the code base, as well as quickcheck-style testing, where instances of a type are automatically generated to serve as test inputs.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/9MKYJFI7/Braibant et al. - ArtiCheck well-typed generic fuzzing for module i.pdf}
}

@article{brandtRehearseHelpingProgrammers,
  title = {Rehearse: {{Helping Programmers Adapt Examples}} by {{Visualizing Execution}} and {{Highlighting Related Code}}},
  author = {Brandt, Joel and Pattamatta, Vignan and Choi, William and Hsieh, Ben and Klemmer, Scott R},
  abstract = {Instructive example code is a central part of programming. Web search enables programmers to quickly locate relevant examples. However, existing code editors offer little support for helping users interactively explore examples. This paper proposes that effective use of examples hinges on the programmer's ability to quickly identify a small number of relevant lines interleaved among a larger body of boilerplate code. This insight is manifest in Rehearse, a code editing environment with two unique features: First, Rehearse links program execution to source code by highlighting each line of code as it is executed. This enables programmers to quickly determine which lines of code are involved in producing a particular interaction. Second, after a programmer has found a single line applicable to her task, Rehearse automatically identifies other lines that are also likely to be relevant. In a controlled experiment, participants using visualization and highlighting adapted example code significantly faster than those using an identical editor without these features.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/7IEG9UJX/Brandt et al. - Rehearse Helping Programmers Adapt Examples by Vi.pdf}
}

@unpublished{braquehaisToolsDiscoveryRefinement2017,
  title = {Tools for {{Discovery}}, {{Refinement}} and {{Generalization}} of {{Functional Properties}} by {{Enumerative Testing}}},
  author = {Braquehais, Rudy Matela},
  year = {2017},
  month = oct,
  publisher = {University of York},
  abstract = {This thesis presents techniques for discovery, refinement and generalization of properties about functional programs. These techniques work by reasoning from test results: their results are surprisingly accurate in practice, despite an inherent uncertainty in principle. These techniques are validated by corresponding implementations in Haskell and for Haskell programs: Speculate, FitSpec and Extrapolate. Speculate discovers properties given a collection of black-box function signatures. Properties discovered by Speculate include inequalities and conditional equations. These properties can contribute to program understanding, documentation and regression testing. FitSpec guides refinements of properties based on results of black-box mutation testing. These refinements include completion and minimization of property sets. Extrapolate generalizes counterexamples of test properties. Generalized counterexamples include repeated variables and side-conditions and can inform the programmer what characterizes failures. Several example applications demonstrate the effectiveness of Speculate, FitSpec and Extrapolate.},
  file = {/Users/harrison/Zotero/storage/GSI9LF8F/Braquehais - 2017 - Tools for Discovery, Refinement and Generalization.pdf}
}

@misc{bronsonBidict2023,
  title = {Bidict},
  author = {Bronson, Joshua},
  year = {2023},
  journal = {bidict},
  urldate = {2023-07-06},
  howpublished = {https://bidict.readthedocs.io/en/main/},
  file = {/Users/harrison/Zotero/storage/M7QDSP5M/main.html}
}

@inproceedings{brotherstonCyclicProofsFirstOrder2005,
  title = {Cyclic {{Proofs}} for {{First-Order Logic}} with {{Inductive Definitions}}},
  booktitle = {Automated {{Reasoning}} with {{Analytic Tableaux}} and {{Related Methods}}},
  author = {Brotherston, James},
  editor = {Beckert, Bernhard},
  year = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {78--92},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11554554_8},
  abstract = {We consider a cyclic approach to inductive reasoning in the setting of first-order logic with inductive definitions. We present a proof system for this language in which proofs are represented as finite, locally sound derivation trees with a ``repeat function'' identifying cyclic proof sections. Soundness is guaranteed by a well-foundedness condition formulated globally in terms of traces over the proof tree, following an idea due to Sprenger and Dam. However, in contrast to their work, our proof system does not require an extension of logical syntax by ordinal variables.},
  isbn = {978-3-540-31822-4},
  langid = {english},
  keywords = {Induction Rule,Predicate Symbol,Proof System,Sequent Calculus,Trace Condition}
}

@inproceedings{brownGeneralizingWYSIWYTVisual2003,
  title = {Generalizing {{WYSIWYT}} Visual Testing to Screen Transition Languages},
  booktitle = {{{IEEE Symposium}} on {{Human Centric Computing Languages}} and {{Environments}}, 2003. {{Proceedings}}. 2003},
  author = {Brown, Darren and Burnett, Margaret and Rothermel, Gregg and Fujita, Hamido and Negoro, Fumio},
  year = {2003},
  pages = {203--210},
  publisher = {IEEE}
}

@article{brzozowskiDerivativesRegularExpressions1964,
  title = {Derivatives of Regular Expressions},
  author = {Brzozowski, Janusz A},
  year = {1964},
  journal = {Journal of the ACM (JACM)},
  volume = {11},
  number = {4},
  pages = {481--494},
  publisher = {ACM New York, NY, USA},
  doi = {10.1145/321239.321249},
  file = {/Users/harrison/Zotero/storage/8KE2LAVL/Brzozowski - 1964 - Derivatives of Regular Expressions.pdf}
}

@inproceedings{buesodebarrioMakinaNewQuickCheck2021,
  title = {Makina: A New {{QuickCheck}} State Machine Library},
  shorttitle = {Makina},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Workshop}} on {{Erlang}}},
  author = {{Bueso de Barrio}, Luis Eduardo and Fredlund, Lars-{\AA}ke and Herranz, {\'A}ngel and Earle, Clara Benac and Mari{\~n}o, Julio},
  year = {2021},
  month = aug,
  series = {Erlang 2021},
  pages = {41--53},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3471871.3472964},
  urldate = {2023-04-06},
  abstract = {This article presents Makina, a new library and a domain specific language for writing property-based testing models for stateful programs. Models written in the new domain specific language are, using Elixir macros, rewritten into normal QuickCheck state machines. Our main goals with Makina are to facilitate the task of developing correct and maintainable models, and to encourage model reuse. To meet these goals, Makina provides a declarative syntax for defining model states and model commands. In particular, Makina encourages the typing of specifications, and ensures through its rewrite rules that such type information can be used by, e.g., the Dialyzer tool, to effectively typecheck models. Moreover, to promote model reuse, the domain specific language provides constructs to permit models to be defined in terms of collections of previously defined models.},
  isbn = {978-1-4503-8612-8},
  keywords = {Elixir,Property-based Testing,State machines},
  file = {/Users/harrison/Zotero/storage/ICGBGMRW/Bueso de Barrio et al. - 2021 - Makina a new QuickCheck state machine library.pdf}
}

@inproceedings{burgInteractiveRecordReplay2013,
  title = {Interactive Record/Replay for Web Application Debugging},
  booktitle = {Proceedings of the 26th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Burg, Brian and Bailey, Richard and Ko, Amy J. and Ernst, Michael D.},
  year = {2013},
  month = oct,
  series = {{{UIST}} '13},
  pages = {473--484},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2501988.2502050},
  urldate = {2022-12-21},
  abstract = {During debugging, a developer must repeatedly and manually reproduce faulty behavior in order to inspect different facets of the program's execution. Existing tools for reproducing such behaviors prevent the use of debugging aids such as breakpoints and logging, and are not designed for interactive, random-access exploration of recorded behavior. This paper presents Timelapse, a tool for quickly recording, reproducing, and debugging interactive behaviors in web applications. Developers can use Timelapse to browse, visualize, and seek within recorded program executions while simultaneously using familiar debugging tools such as breakpoints and logging. Testers and end-users can use Timelapse to demonstrate failures in situ and share recorded behaviors with developers, improving bug report quality by obviating the need for detailed reproduction steps. Timelapse is built on Dolos, a novel record/replay infrastructure that ensures deterministic execution by capturing and reusing program inputs both from the user and from external sources such as the network. Dolos introduces negligible overhead and does not interfere with breakpoints and logging. In a small user evaluation, participants used Timelapse to accelerate existing reproduction activities, but were not significantly faster or more successful in completing the larger tasks at hand. Together, the Dolos infrastructure and Timelapse developer tool support systematic bug reporting and debugging practices.},
  isbn = {978-1-4503-2268-3},
  keywords = {debugging,deterministic replay,web applications},
  file = {/Users/harrison/Zotero/storage/UBTACQ2T/Burg et al. - 2013 - Interactive recordreplay for web application debu.pdf}
}

@article{burnettGenderMagMethodEvaluating2016,
  title = {{{GenderMag}}: {{A Method}} for {{Evaluating Software}}'s {{Gender Inclusiveness}}},
  shorttitle = {{{GenderMag}}},
  author = {Burnett, Margaret and Stumpf, Simone and Macbeth, Jamie and Makri, Stephann and Beckwith, Laura and Kwan, Irwin and Peters, Anicia and Jernigan, William},
  year = {2016},
  month = nov,
  journal = {Interacting with Computers},
  volume = {28},
  number = {6},
  pages = {760--787},
  issn = {0953-5438},
  doi = {10.1093/iwc/iwv046},
  urldate = {2024-03-16},
  abstract = {In recent years, research into gender differences has established that individual differences in how people problem-solve often cluster by gender. Research also shows that these differences have direct implications for software that aims to support users' problem-solving activities, and that much of this software is more supportive of problem-solving processes favored (statistically) more by males than by females. However, there is almost no work considering how software practitioners---such as User Experience (UX) professionals or software developers---can find gender-inclusiveness issues like these in their software. To address this gap, we devised the GenderMag method for evaluating problem-solving software from a gender-inclusiveness perspective. The method includes a set of faceted personas that bring five facets of gender difference research to life, and embeds use of the personas into a concrete process through a gender-specialized Cognitive Walkthrough. Our empirical results show that a variety of practitioners who design software---without needing any background in gender research---were able to use the GenderMag method to find gender-inclusiveness issues in problem-solving software. Our results also show that the issues the practitioners found were real and fixable. This work is the first systematic method to find gender-inclusiveness issues in software, so that practitioners can design and produce problem-solving software that is more usable by everyone.},
  file = {/Users/harrison/Zotero/storage/S2NSYARQ/Burnett et al. - 2016 - GenderMag A Method for Evaluating Software's Gend.pdf;/Users/harrison/Zotero/storage/2RSML7UL/2417082.html}
}

@misc{bythewayBoleroBook2023,
  title = {Bolero {{Book}}},
  author = {Bytheway, Cameron},
  year = {2023},
  journal = {Bolero},
  urldate = {2023-10-05},
  howpublished = {https://camshaft.github.io/bolero/},
  file = {/Users/harrison/Zotero/storage/FNV8AB2K/bolero.html}
}

@article{capriottiFreeApplicativeFunctors2014,
  title = {Free Applicative Functors},
  author = {Capriotti, Paolo and Kaposi, Ambrus},
  year = {2014},
  journal = {arXiv preprint arXiv:1403.0749},
  eprint = {1403.0749},
  archiveprefix = {arXiv},
  file = {/Users/harrison/Zotero/storage/JFUCTD7Q/Capriotti and Kaposi - 2014 - Free Applicative Functors.pdf;/Users/harrison/Zotero/storage/84B7KPTE/1403.html}
}

@inproceedings{changGUITestingUsing2010,
  title = {{{GUI}} Testing Using Computer Vision},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chang, Tsung-Hsiang and Yeh, Tom and Miller, Robert C},
  year = {2010},
  pages = {1535--1544}
}

@article{chasinsPLHCIBetter2021,
  title = {{{PL}} and {{HCI}}: Better Together},
  shorttitle = {{{PL}} and {{HCI}}},
  author = {Chasins, Sarah E. and Glassman, Elena L. and Sunshine, Joshua},
  year = {2021},
  month = aug,
  journal = {Communications of the ACM},
  volume = {64},
  number = {8},
  pages = {98--106},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3469279},
  urldate = {2022-12-05},
  abstract = {Collaborations between two communities have unearthed a sweet spot for future programming efforts.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/S9C9HTDP/Chasins et al. - 2021 - PL and HCI better together.pdf}
}

@inproceedings{chenAdaptiveRandomTesting2004,
  title = {Adaptive {{Random Testing}}},
  booktitle = {Advances in {{Computer Science}} - {{ASIAN}} 2004, {{Higher-Level Decision Making}}, 9th {{Asian Computing Science Conference}}, {{Dedicated}} to {{Jean-Louis Lassez}} on the {{Occasion}} of {{His}} 5th {{Cycle Birthday}}, {{Chiang Mai}}, {{Thailand}}, {{December}} 8-10, 2004, {{Proceedings}}},
  author = {Chen, Tsong Yueh and Leung, Hing and Mak, I. K.},
  year = {2004},
  pages = {320--329},
  doi = {10.1007/978-3-540-30502-6_23},
  file = {/Users/harrison/Zotero/storage/D9JQJ5H6/art.pdf}
}

@article{chenNearlinearTimeSamplers,
  title = {Near-Linear Time Samplers for Matroid Independent Sets with Applications},
  author = {Chen, Xiaoyu and Guo, Heng and Zhang, Xinyuan and Zou, Zongrui},
  abstract = {We give a O(n) time almost uniform sampler for independent sets of a matroid, whose ground set has n elements and is given by an independence oracle. As a consequence, one can sample connected spanning subgraphs of a given graph G = (V, E) in O({\textbar}E{\textbar}) time, whereas the previous best algorithm takes O({\textbar}E{\textbar} {\textbar}V{\textbar}) time. This improvement, in turn, leads to a faster running time on estimating all-terminal network reliability. Furthermore, we generalise this near-linear time sampler to the random cluster model with q {$\leq$} 1.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/G29QUQVJ/Chen et al. - Near-linear time samplers for matroid independent .pdf}
}

@article{chomskyCertainFormalProperties1959,
  title = {On Certain Formal Properties of Grammars},
  author = {Chomsky, Noam},
  year = {1959},
  month = jun,
  journal = {Information and Control},
  volume = {2},
  number = {2},
  pages = {137--167},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(59)90362-6},
  urldate = {2024-03-17},
  abstract = {A grammar can be regarded as a device that enumerates the sentences of a language. We study a sequence of restrictions that limit grammars first to Turing machines, then to two types of system from which a phrase structure description of the generated language can be drawn, and finally to finite state Markov sources (finite automata). These restrictions are shown to be increasingly heavy in the sense that the languages that can be generated by grammars meeting a given restriction constitute a proper subset of those that can be generated by grammars meeting the preceding restriction. Various formulations of phrase structure description are considered, and the source of their excess generative power over finite state sources is investigated in greater detail.},
  file = {/Users/harrison/Zotero/storage/TINYQBXI/S0019995859903626.html}
}

@inproceedings{citoInteractiveProductionPerformance2019,
  title = {Interactive {{Production Performance Feedback}} in the {{IDE}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Cito, J{\"u}rgen and Leitner, Philipp and Rinard, Martin and Gall, Harald C.},
  year = {2019},
  month = may,
  pages = {971--981},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2019.00102},
  urldate = {2024-02-26},
  abstract = {Because of differences between development and production environments, many software performance problems are detected only after software enters production. We present PerformanceHat, a new system that uses profiling information from production executions to develop a global performance model suitable for integration into interactive development environments. PerformanceHat's ability to incrementally update this global model as the software is changed in the development environment enables it to deliver near real-time predictions of performance consequences reflecting the impact on the production environment. We implement PerformanceHat as an Eclipse plugin and evaluate it in a controlled experiment with 20 professional software developers implementing several software maintenance tasks using our approach and a representative baseline (Kibana). Our results indicate that developers using PerformanceHat were significantly faster in (1) detecting the performance problem, and (2) finding the root-cause of the problem. These results provide encouraging evidence that our approach helps developers detect, prevent, and debug production performance problems during development before the problem manifests in production.},
  keywords = {Data models,Monitoring,Runtime,Software,software performance engineering IDE user study,Task analysis,Tools},
  file = {/Users/harrison/Zotero/storage/LMKFE7MR/Cito et al. - 2019 - Interactive Production Performance Feedback in the.pdf;/Users/harrison/Zotero/storage/EG79MEZZ/8811928.html}
}

@inproceedings{ciupaARTOOAdaptiveRandom2008,
  title = {{{ARTOO}}: Adaptive Random Testing for Object-Oriented Software},
  booktitle = {30th {{International Conference}} on {{Software Engineering}} ({{ICSE}} 2008), {{Leipzig}}, {{Germany}}, {{May}} 10-18, 2008},
  author = {Ciupa, Ilinca and Leitner, Andreas and Oriol, Manuel and Meyer, Bertrand},
  year = {2008},
  pages = {71--80},
  doi = {10.1145/1368088.1368099},
  file = {/Users/harrison/Zotero/storage/AGPF4JLQ/Ciupa et al. - 2008 - ARTOO adaptive random testing for object-oriented.pdf}
}

@article{claessenGeneratingConstrainedRandom2015,
  title = {Generating Constrained Random Data with Uniform Distribution},
  author = {Claessen, Koen and Dureg{\aa}rd, Jonas and Palka, Michal H.},
  year = {2015},
  journal = {J. Funct. Program.},
  volume = {25},
  doi = {10.1017/S0956796815000143},
  file = {/Users/harrison/Zotero/storage/U8A62N9T/Claessen et al. - 2015 - Generating constrained random data with uniform di.pdf}
}

@inproceedings{claessenQuickCheckLightweightTool2000,
  title = {{{QuickCheck}}: A Lightweight Tool for Random Testing of {{Haskell}} Programs},
  booktitle = {Proceedings of the {{Fifth ACM SIGPLAN International Conference}} on {{Functional Programming}} ({{ICFP}} '00), {{Montreal}}, {{Canada}}, {{September}} 18-21, 2000},
  author = {Claessen, Koen and Hughes, John},
  editor = {Odersky, Martin and Wadler, Philip},
  year = {2000},
  pages = {268--279},
  publisher = {ACM},
  address = {Montreal, Canada},
  doi = {10.1145/351240.351266},
  file = {/Users/harrison/Zotero/storage/44JZEYR9/Claessen and Hughes - 2000 - QuickCheck a lightweight tool for random testing .pdf}
}

@inproceedings{claessenQuickSpecGuessingFormal2010,
  title = {{{QuickSpec}}: {{Guessing Formal Specifications Using Testing}}},
  shorttitle = {{{QuickSpec}}},
  booktitle = {Tests and {{Proofs}}},
  author = {Claessen, Koen and Smallbone, Nicholas and Hughes, John},
  editor = {Fraser, Gordon and Gargantini, Angelo},
  year = {2010},
  pages = {6--21},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-13977-2_3},
  abstract = {We present QuickSpec, a tool that automatically generates algebraic specifications for sets of pure functions. The tool is based on testing, rather than static analysis or theorem proving. The main challenge QuickSpec faces is to keep the number of generated equations to a minimum while maintaining completeness. We demonstrate how QuickSpec can improve one's understanding of a program module by exploring the laws that are generated using two case studies: a heap library for Haskell and a fixed-point arithmetic library for Erlang.},
  isbn = {978-3-642-13977-2},
  langid = {english},
  keywords = {Congruence Relation,Depth Limit,Depth Optimisation,Equivalence Class,Inductive Logic Programming},
  file = {/Users/harrison/Zotero/storage/CSXDGQIS/Claessen et al. - 2010 - QuickSpec Guessing Formal Specifications Using Te.pdf}
}

@inproceedings{claessenShrinkingShowingFunctions2012,
  title = {Shrinking and Showing Functions: (Functional Pearl)},
  shorttitle = {Shrinking and Showing Functions},
  booktitle = {Proceedings of the 2012 {{Haskell Symposium}}},
  author = {Claessen, Koen},
  year = {2012},
  month = sep,
  series = {Haskell '12},
  pages = {73--80},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2364506.2364516},
  urldate = {2023-10-05},
  abstract = {Although quantification over functions in QuickCheck properties has been supported from the beginning, displaying and shrinking them as counter examples has not. The reason is that in general, functions are infinite objects, which means that there is no sensible show function for them, and shrinking an infinite object within a finite number of steps seems impossible. This paper presents a general technique with which functions as counter examples can be shrunk to finite objects, which can then be displayed to the user. The approach turns out to be practically usable, which is shown by a number of examples. The two main limitations are that higher-order functions cannot be dealt with, and it is hard to deal with terms that contain functions as subterms.},
  isbn = {978-1-4503-1574-6},
  keywords = {counter example,quickcheck,testing},
  file = {/Users/harrison/Zotero/storage/UCAF6TLM/Claessen - 2012 - Shrinking and showing functions (functional pearl.pdf}
}

@article{clarkeBoundedModelChecking2001,
  title = {Bounded {{Model Checking Using Satisfiability Solving}}},
  author = {Clarke, Edmund and Biere, Armin and Raimi, Richard and Zhu, Yunshan},
  year = {2001},
  month = jul,
  journal = {Formal Methods in System Design},
  volume = {19},
  number = {1},
  pages = {7--34},
  issn = {1572-8102},
  doi = {10.1023/A:1011276507260},
  urldate = {2024-10-04},
  abstract = {The phrase model checking refers to algorithms for exploring the state space of a transition system to determine if it obeys a specification of its intended behavior. These algorithms can perform exhaustive verification in a highly automatic manner, and, thus, have attracted much interest in industry. Model checking programs are now being commercially marketed. However, model checking has been held back by the state explosion problem, which is the problem that the number of states in a system grows exponentially in the number of system components. Much research has been devoted to ameliorating this problem.},
  langid = {english},
  keywords = {bounded model checking,cone of influence reduction,model checking,processor verification,satisfiability},
  file = {/Users/harrison/Zotero/storage/T9KL73QD/Clarke et al. - 2001 - Bounded Model Checking Using Satisfiability Solvin.pdf}
}

@article{clarkeFormalMethodsState1996,
  title = {Formal Methods: State of the Art and Future Directions},
  shorttitle = {Formal Methods},
  author = {Clarke, Edmund M. and Wing, Jeannette M.},
  year = {1996},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {28},
  number = {4},
  pages = {626--643},
  issn = {0360-0300},
  doi = {10.1145/242223.242257},
  urldate = {2024-04-23},
  file = {/Users/harrison/Zotero/storage/NR9ZRY7X/Clarke and Wing - 1996 - Formal methods state of the art and future direct.pdf}
}

@incollection{clarkeOwnershipTypesSurvey2013,
  title = {Ownership {{Types}}: {{A Survey}}},
  shorttitle = {Ownership {{Types}}},
  booktitle = {Aliasing in {{Object-Oriented Programming}}. {{Types}}, {{Analysis}} and {{Verification}}},
  author = {Clarke, Dave and {\"O}stlund, Johan and Sergey, Ilya and Wrigstad, Tobias},
  editor = {Clarke, Dave and Noble, James and Wrigstad, Tobias},
  year = {2013},
  pages = {15--58},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-36946-9_3},
  urldate = {2024-09-09},
  abstract = {Ownership types were devised nearly 15 years ago to provide a stronger notion of protection to object-oriented programming languages. Rather than simply protecting the fields of an object from external access, ownership types protect also the objects stored in the fields, thereby enabling an object to claim (exclusive) ownership of and access to other objects. Furthermore, this notion is statically enforced by now-standard type-checking techniques.},
  isbn = {978-3-642-36946-9},
  langid = {english}
}

@article{coblenzPLIERSProcessThat2021,
  title = {{{PLIERS}}: {{A Process}} That {{Integrates User-Centered Methods}} into {{Programming Language Design}}},
  shorttitle = {{{PLIERS}}},
  author = {Coblenz, Michael and Kambhatla, Gauri and Koronkevich, Paulette and Wise, Jenna L. and Barnaby, Celeste and Sunshine, Joshua and Aldrich, Jonathan and Myers, Brad A.},
  year = {2021},
  month = jul,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {28},
  number = {4},
  pages = {28:1--28:53},
  issn = {1073-0516},
  doi = {10.1145/3452379},
  urldate = {2023-01-27},
  abstract = {Programming language design requires making many usability-related design decisions. However, existing HCI methods can be impractical to apply to programming languages: languages have high iteration costs, programmers require significant learning time, and user performance has high variance. To address these problems, we adapted both formative and summative HCI methods to make them more suitable for programming language design. We integrated these methods into a new process, PLIERS, for designing programming languages in a user-centered way. We assessed PLIERS by using it to design two new programming languages. Glacier extends Java to enable programmers to express immutability properties effectively and easily. Obsidian is a language for blockchains that includes verification of critical safety properties. Empirical studies showed that the PLIERS process resulted in languages that could be used effectively by many programmers and revealed additional opportunities for language improvement.},
  keywords = {programming language design,Usability of programming languages},
  file = {/Users/harrison/Zotero/storage/F3YG4AHW/Coblenz et al. - 2021 - PLIERS A Process that Integrates User-Centered Me.pdf}
}

@article{CognitiveDimensionsNotations2023,
  title = {Cognitive Dimensions of Notations},
  year = {2023},
  month = jul,
  journal = {Wikipedia},
  urldate = {2024-12-08},
  abstract = {Cognitive dimensions or cognitive dimensions of notations are design principles for notations, user interfaces and programming languages, described by researcher Thomas R.G. Green and further researched with Marian Petre.  The dimensions can be used to evaluate the usability of an existing information artifact, or as heuristics to guide the design of a new one, and are useful in Human-Computer Interaction design. Cognitive dimensions are designed to provide a lightweight approach to analyse the quality of a design, rather than an in-depth, detailed description.  They provide a common vocabulary for discussing many factors in notation, UI or programming language design.  Also, cognitive dimensions help in exploring the space of possible designs through design maneuvers, changes intended to improve the design along one dimension.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1166977777},
  file = {/Users/harrison/Zotero/storage/R4C2UDGR/Cognitive_dimensions_of_notations.html}
}

@inproceedings{colbournDeterministicDensityAlgorithm2004,
  title = {A {{Deterministic Density Algorithm}} for {{Pairwise Interaction Coverage}}},
  booktitle = {{{IASTED International Conference}} on {{Software Engineering}}, Part of the 22nd {{Multi-Conference}} on {{Applied Informatics}}, {{Innsbruck}}, {{Austria}}, {{February}} 17-19, 2004},
  author = {Colbourn, Charles J. and Cohen, Myra B. and Turban, Ren{\'e}e},
  editor = {Hamza, M. H.},
  year = {2004},
  pages = {345--352},
  publisher = {IASTED/ACTA Press},
  file = {/Users/harrison/Zotero/storage/8MNQUAEL/Colbourn et al. - 2004 - A Deterministic Density Algorithm for Pairwise Int.pdf}
}

@article{comonTreeAutomataTechniques2007,
  title = {Tree {{Automata Techniques}} and {{Applications}}},
  author = {Comon, H. and Dauchet, M. and Gilleron, R. and L{\"o}ding, C. and Jacquemard, F. and Lugiez, D. and Tison, S. and Tommasi, M.},
  year = {2007},
  file = {/Users/harrison/Zotero/storage/H3C5HBZK/Comon et al. - Tree Automata Techniques and Applications.pdf}
}

@book{conferencePeopleComputersProceedings1989,
  title = {People and {{Computers V}}: {{Proceedings}} of the {{Fifth Conference}} of the {{British Computer Society}}},
  shorttitle = {People and {{Computers V}}},
  author = {Conference, British Computer Society Human Computer Interaction Specialist Group},
  year = {1989},
  month = oct,
  publisher = {Cambridge University Press},
  abstract = {These papers detail the theoretical basis and methodical practice of HCI, the interaction of HCI with other disciplines, and individual relevance. This book is a comprehensive guide to the current research in HCI which will be essential reading for all researchers, designers and manufacturers whose work impinges on this rapidly moving field. Contributions are included from leading researchers and designers in both industry and academia.},
  googlebooks = {BTxOtt4X920C},
  isbn = {978-0-521-38430-8},
  langid = {english},
  keywords = {Computers / Human-Computer Interaction (HCI),Computers / Interactive & Multimedia,Computers / Reference,Computers / Social Aspects}
}

@inproceedings{corbettBanderaExtractingFinitestate2000,
  title = {Bandera: Extracting Finite-State Models from {{Java}} Source Code},
  shorttitle = {Bandera},
  booktitle = {Proceedings of the 22nd International Conference on {{Software}} Engineering},
  author = {Corbett, James C. and Dwyer, Matthew B. and Hatcliff, John and Laubach, Shawn and P{\u a}s{\u a}reanu, Corina S. and Robby and Zheng, Hongjun},
  year = {2000},
  month = jun,
  series = {{{ICSE}} '00},
  pages = {439--448},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/337180.337234},
  urldate = {2023-10-26},
  abstract = {Finite-state verification techniques, such as model checking, have shown promise as a cost-effective means for finding defects in hardware designs. To date, the application of these techniques to software has been hindered by several obstacles. Chief among these is the problem of constructing a finite-state model that approximates the executable behavior of the software system of interest. Current best-practice involves hand-construction of models which is expensive (prohibitive for all but the smallest systems), prone to errors (which can result in misleading verification results), and difficult to optimize (which is necessary to combat the exponential complexity of verification algorithms). In this paper, we describe an integrated collection of program analysis and transformation components, called Bandera, that enables the automatic extraction of safe, compact finite-state models from program source code. Bandera takes as input Java source code and generates a program model in the input language of one of several existing verification tools; Bandera also maps verifier outputs back to the original source code. We discuss the major components of Bandera and give an overview of how it can be used to model check correctness properties of Java programs.},
  isbn = {978-1-58113-206-9},
  keywords = {abstract interpretation,model checking,model extraction,program specialization,program verification,slicing},
  file = {/Users/harrison/Zotero/storage/GPYXXQ7W/Corbett et al. - 2000 - Bandera extracting finite-state models from Java .pdf}
}

@inproceedings{corgozinhoHowDevelopersImplement2023,
  title = {How {{Developers Implement Property-Based Tests}}},
  booktitle = {Conference: 39th {{International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}} 2023)},
  author = {Corgozinho, Arthur and Valente, Marco and Rocha, Henrique},
  year = {2023},
  month = sep,
  abstract = {Property-based testing (PBT) is an interesting alternative to example-based testing where the inputs are randomly generated by the testing tool. In PBT, we check properties that always hold for any input. Despite being a promising testing category, to the best of our knowledge, we still lack studies that investigate in the wild how developers are using PBT in practice. In this paper, we report the preliminary results of a study we are conducting on the usage of PBT. We created a dataset of 30 popular Python repositories using Hypothesis (a PBT tool) and selected a random sample of 86 tests. We manually analyzed these tests to understand the most commonly implemented properties and also to reveal the most used features to create them.},
  file = {/Users/harrison/Zotero/storage/TN9NEE3P/Corgozinho et al. - 2023 - How Developers Implement Property-Based Tests.pdf}
}

@article{courcelleFundamentalPropertiesInfinite1983,
  title = {Fundamental {{Properties}} of {{Infinite Trees}}},
  author = {Courcelle, Bruno},
  year = {1983},
  journal = {Theor. Comput. Sci.},
  volume = {25},
  pages = {95--169},
  doi = {10.1016/0304-3975(83)90059-2},
  file = {/Users/harrison/Zotero/storage/TJUZU2FY/Courcelle - 1983 - Fundamental Properties of Infinite Trees.pdf}
}

@misc{crichtonEvaluatingHumanFactors2024,
  title = {Evaluating {{Human Factors Beyond Lines}} of {{Code}}},
  author = {Crichton, Will},
  year = {2024},
  month = nov,
  journal = {SIGPLAN Blog},
  urldate = {2024-12-17},
  abstract = {Software systems researchers want to make human-centered claims, but don't have the proper tools to do so. That's how we ended up with the ubiquitous lines-of-code comparison found in e{\dots}},
  langid = {american},
  file = {/Users/harrison/Zotero/storage/44NE5XKX/evaluating-human-factors-beyond-lines-of-code.html}
}

@article{cutlerCedarNewLanguage2024,
  title = {Cedar: {{A New Language}} for {{Expressive}}, {{Fast}}, {{Safe}}, and {{Analyzable Authorization}}},
  shorttitle = {Cedar},
  author = {Cutler, Joseph W. and Disselkoen, Craig and Eline, Aaron and He, Shaobo and Headley, Kyle and Hicks, Michael and Hietala, Kesha and Ioannidis, Eleftherios and Kastner, John and Mamat, Anwar and McAdams, Darin and McCutchen, Matt and Rungta, Neha and Torlak, Emina and Wells, Andrew M.},
  year = {2024},
  month = apr,
  journal = {Proc. ACM Program. Lang.},
  volume = {8},
  number = {OOPSLA1},
  pages = {118:670--118:697},
  doi = {10.1145/3649835},
  urldate = {2025-01-19},
  abstract = {Cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable. Rather than embed authorization logic in an application's code, developers can write that logic as Cedar policies and delegate access decisions to Cedar's evaluation engine. Cedar's simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models. Cedar's policy structure enables access requests to be decided quickly. Cedar's policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way. Cedar's design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized   permissions do not change. We have modeled Cedar in the Lean programming language, and used Lean's proof assistant to prove important properties of Cedar's design. We have implemented Cedar in Rust, and released it open-source. Comparing Cedar to two open-source languages, OpenFGA and Rego, we find (subjectively) that Cedar has equally or more readable policies, but (objectively) performs far better.},
  file = {/Users/harrison/Zotero/storage/4L3QTM95/Cutler et al. - 2024 - Cedar A New Language for Expressive, Fast, Safe, .pdf}
}

@article{cutlerStreamTypes2024,
  title = {Stream {{Types}}},
  author = {Cutler, Joseph W. and Watson, Christopher and Nkurumeh, Emeka and Hilliard, Phillip and Goldstein, Harrison and Stanford, Caleb and Pierce, Benjamin C.},
  year = {2024},
  month = jun,
  journal = {Proc. ACM Program. Lang.},
  volume = {8},
  number = {PLDI},
  pages = {204:1412--204:1436},
  doi = {10.1145/3656434},
  urldate = {2024-09-18},
  abstract = {We propose a rich foundational theory of typed data streams and stream transformers, motivated by two high-level goals. First, the type of a stream should be able to express complex sequential patterns of events over time. And second, it should describe the internal parallel structure of the stream, to support deterministic stream processing on parallel and distributed systems. To these ends, we introduce stream types, with operators capturing sequential composition, parallel composition, and iteration, plus a core calculus {$\lambda$}ST of transformers over typed streams that naturally supports a number of common streaming idioms, including punctuation, windowing, and parallel partitioning, as first-class constructions. {$\lambda$}ST exploits a Curry-Howard-like correspondence with an ordered variant of the Logic of Bunched Implication to program with streams compositionally and uses Brzozowski-style derivatives to enable an incremental, prefix-based operational semantics. To illustrate the programming style supported by the rich types of {$\lambda$}ST, we present a number of examples written in Delta, a prototype high-level language design based on {$\lambda$}ST.},
  file = {/Users/harrison/Zotero/storage/UHR6E4VL/Cutler et al. - 2024 - Stream Types.pdf}
}

@inproceedings{dakaSurveyUnitTesting2014,
  title = {A {{Survey}} on {{Unit Testing Practices}} and {{Problems}}},
  booktitle = {2014 {{IEEE}} 25th {{International Symposium}} on {{Software Reliability Engineering}}},
  author = {Daka, Ermira and Fraser, Gordon},
  year = {2014},
  month = nov,
  pages = {201--211},
  issn = {2332-6549},
  doi = {10.1109/ISSRE.2014.11},
  abstract = {Unit testing is a common practice where developers write test cases together with regular code. Automation frameworks such as JUnit for Java have popularised this approach, allowing frequent and automatic execution of unit test suites. Despite the appraisals of unit testing in practice, software engineering researchers see potential for improvement and investigate advanced techniques such as automated unit test generation. To align such research with the needs of practitioners, we conducted a survey amongst 225 software developers, covering different programming languages and 29 countries, using a global online marketing research platform. The survey responses confirm that unit testing is an important factor in software development, and suggest that there is indeed potential and need for research on automation of unit testing. The results help us to identify areas of importance on which further research will be necessary (e.g., Maintenance of unit tests), and also provide insights into the suitability of online marketing research platforms for software engineering surveys.},
  keywords = {Java,Reliability,Software,Software engineering,survey,test case generation,Testing,unit testing,Writing},
  file = {/Users/harrison/Zotero/storage/VGIA5SQX/Daka and Fraser - 2014 - A Survey on Unit Testing Practices and Problems.pdf;/Users/harrison/Zotero/storage/44YVNVKD/6982627.html}
}

@inproceedings{danasUserStudiesPrincipled2017,
  title = {User {{Studies}} of {{Principled Model Finder Output}}},
  booktitle = {Software {{Engineering}} and {{Formal Methods}}},
  author = {Danas, Natasha and Nelson, Tim and Harrison, Lane and Krishnamurthi, Shriram and Dougherty, Daniel J.},
  editor = {Cimatti, Alessandro and Sirjani, Marjan},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {168--184},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-66197-1_11},
  abstract = {Model-finders such as SAT-solvers are attractive for producing concrete models, either as sample instances or as counterexamples when properties fail. However, the generated model is arbitrary. To address this, several research efforts have proposed principled forms of output from model-finders. These include minimal and maximal models, unsat cores, and proof-based provenance of facts.},
  isbn = {978-3-319-66197-1},
  langid = {english},
  keywords = {HCI,Minimization,Models,Provenance,Unsat core,User studies},
  file = {/Users/harrison/Zotero/storage/IAS3XMUH/Danas et al. - 2017 - User Studies of Principled Model Finder Output.pdf}
}

@inproceedings{darraghClothoRacketLibrary2021,
  title = {Clotho: {{A Racket Library}} for {{Parametric Randomness}}},
  booktitle = {Functional {{Programming Workshop}}},
  author = {Darragh, Pierce and Hatch, William Gallard and Eide, Eric},
  year = {2021},
  pages = {3},
  file = {/Users/harrison/Zotero/storage/P6E9NFLM/Darragh et al. - 2021 - Clotho A Racket Library for Parametric Randomness.pdf}
}

@article{dashAffineMonadsLazy2023,
  title = {Affine {{Monads}} and {{Lazy Structures}} for {{Bayesian Programming}}},
  author = {Dash, Swaraj and Kaddar, Younesse and Paquet, Hugo and Staton, Sam},
  year = {2023},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {7},
  number = {POPL},
  pages = {1338--1368},
  issn = {2475-1421},
  doi = {10.1145/3571239},
  urldate = {2023-03-06},
  abstract = {We show that streams and lazy data structures are a natural idiom for programming with infinite-dimensional Bayesian methods such as Poisson processes, Gaussian processes, jump processes, Dirichlet processes, and Beta processes. The crucial semantic idea, inspired by developments in synthetic probability theory, is to work with two separate monads: an affine monad of probability, which supports laziness, and a commutative, non-affine monad of measures, which does not. (Affine means that               T               (1){$\cong$} 1.) We show that the separation is important from a decidability perspective, and that the recent model of quasi-Borel spaces supports these two monads.                          To perform Bayesian inference with these examples, we introduce new inference methods that are specially adapted to laziness; they are proven correct by reference to the Metropolis-Hastings-Green method. Our theoretical development is implemented as a Haskell library, LazyPPL.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/PGYTBH9F/Dash et al. - 2023 - Affine Monads and Lazy Structures for Bayesian Pro.pdf}
}

@inproceedings{davisNaNofuzzUsableTool2023,
  title = {{{NaNofuzz}}: {{A Usable Tool}} for {{Automatic Test Generation}}},
  shorttitle = {{{NaNofuzz}}},
  booktitle = {Proceedings of the 31st {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Davis, Matthew and Choi, Sangheon and Estep, Sam and Myers, Brad and Sunshine, Joshua},
  year = {2023},
  month = nov,
  series = {{{ESEC}}/{{FSE}} 2023},
  pages = {1114--1126},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3611643.3616327},
  urldate = {2024-04-29},
  abstract = {In the United States alone, software testing labor is estimated to cost \$48 billion USD per year. Despite widespread test execution automation and automation in other areas of software engineering, test suites continue to be created manually by software engineers. We have built a test generation tool, called NaNofuzz, that helps users find bugs in their code by suggesting tests where the output is likely indicative of a bug, e.g., that return NaN (not-a-number) values. NaNofuzz is an interactive tool embedded in a development environment to fit into the programmer's workflow. NaNofuzz tests a function with as little as one button press, analyses the program to determine inputs it should evaluate, executes the program on those inputs, and categorizes outputs to prioritize likely bugs. We conducted a randomized controlled trial with 28 professional software engineers using NaNofuzz as the intervention treatment and the popular manual testing tool, Jest, as the control treatment. Participants using NaNofuzz on average identified bugs more accurately (p {$<$} .05, by 30\%), were more confident in their tests (p {$<$} .03, by 20\%), and finished their tasks more quickly (p {$<$} .007, by 30\%).},
  isbn = {9798400703270},
  keywords = {automatic test generation,Empirical software engineering,experiments,human subjects,software testing,usable testing,user study},
  file = {/Users/harrison/Zotero/storage/Y9R9W4VI/Davis et al. - 2023 - NaNofuzz A Usable Tool for Automatic Test Generat.pdf}
}

@inproceedings{davisTestLoopProcessModel2024,
  title = {{{TestLoop}}: {{A Process Model}} of {{Automatic}} and {{Manual Test Suite Generation}}},
  shorttitle = {{{TestLoop}}},
  booktitle = {14th Annual Workshop on the Intersection of {{HCI}} and {{PL}} ({{PLATEAU}}'24)},
  author = {Davis, Matthew C. and Choi, Sangheon and Wei, Amy and Myers, Brad A. and Sunshine, Joshua},
  year = {2024},
  month = feb
}

@misc{delaatPyMonadDataStructures,
  title = {{{PyMonad}}: {{Data}} Structures and Utilities for Monadic Style Functional Programming.},
  shorttitle = {{{PyMonad}}},
  author = {DeLaat, Jason},
  urldate = {2023-02-21},
  copyright = {BSD License},
  keywords = {Software Development,Software Development - Libraries,Utilities},
  file = {/Users/harrison/Zotero/storage/G2GW236K/PyMonad.html}
}

@inproceedings{demouraZ3EfficientSMT2008,
  title = {Z3: {{An Efficient SMT Solver}}},
  shorttitle = {Z3},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {{de Moura}, Leonardo and Bj{\o}rner, Nikolaj},
  editor = {Ramakrishnan, C. R. and Rehof, Jakob},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {337--340},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-78800-3_24},
  abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
  isbn = {978-3-540-78800-3},
  langid = {english},
  keywords = {Bound Model Check,Linear Arithmetic,Predicate Abstraction,Symbolic Execution,Theory Solver},
  file = {/Users/harrison/Zotero/storage/Z5DRYWPD/de Moura and Bjørner - 2008 - Z3 An Efficient SMT Solver.pdf}
}

@phdthesis{derakhshanSessionTypedRecursiveProcesses2021,
  title = {Session-{{Typed Recursive Processes}} and {{Circular Proofs}}},
  author = {Derakhshan, Farzaneh},
  year = {2021},
  month = may,
  langid = {english},
  school = {Caregie Mellon University},
  file = {/Users/harrison/Zotero/storage/NQLLRVWT/Derakhshan - Session-Typed Recursive Processes and Circular Pro.pdf}
}

@inproceedings{devriesFalsifyInternalShrinking2023,
  title = {Falsify: {{Internal Shrinking Reimagined}} for {{Haskell}}},
  shorttitle = {Falsify},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Haskell Symposium}}},
  author = {{de Vries}, Edsko},
  year = {2023},
  month = aug,
  series = {Haskell 2023},
  pages = {97--109},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3609026.3609733},
  urldate = {2024-03-11},
  abstract = {In unit testing we apply the function under test to known inputs and check for known outputs. By contrast, in property based testing we state properties relating inputs and outputs, apply the function to random inputs, and verify that the property holds; if not, we found a bug. Randomly generated inputs tend to be large and should therefore be minimised. Traditionally this is done with an explicitly provided shrinker, but in this paper we propose a way to write generators that obsoletes the need to write a separate shrinker. Inspired by the Python library Hypothesis, the approach can work even across monadic bind. Compared to Hypothesis, our approach is more suitable to the Haskell setting: it depends on a minimal set of core principles, and handles generation and shrinking of infinite data structures, including functions.},
  isbn = {9798400702983},
  keywords = {functional programming,internal shrinking,property based testing},
  file = {/Users/harrison/Zotero/storage/ICAPFYJJ/de Vries - 2023 - falsify Internal Shrinking Reimagined for Haskell.pdf}
}

@book{deweyAutomatedBlackBox2017,
  title = {Automated {{Black Box Generation}} of {{Structured Inputs}} for {{Use}} in {{Software Testing}}},
  author = {Dewey, Kyle Thomas},
  year = {2017},
  publisher = {University of California, Santa Barbara},
  file = {/Users/harrison/Zotero/storage/RZZ2DGV5/DmWkH.pdf}
}

@inproceedings{deweyMiMIsSimpleEfficient2020,
  title = {{{MiMIs}}: {{Simple}}, {{Efficient}}, and {{Fast Bounded-Exhaustive Test Case Generators}}},
  shorttitle = {{{MiMIs}}},
  booktitle = {2020 {{IEEE}} 13th {{International Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Dewey, Kyle and Hairapetian, Shant and Gavrilov, Miroslav},
  year = {2020},
  month = oct,
  pages = {51--62},
  issn = {2159-4848},
  doi = {10.1109/ICST46399.2020.00016},
  abstract = {Bounded-exhaustive test case generators are important bug-finding tools, epitomized with tools like Korat, UDITA, and SciFe. All such tools are practically limited by scale; a given testing problem may be too large to generate all tests within reasonable time, or prohibitive amounts of memory may be required for this generation. The tools themselves may have lengthy implementations which hide their own bugs, or they may require users to familiarize themselves with many operations. In this paper, we strive to push the boundaries of what test case generators can scale to, while preserving a simple implementation and user interface. To this end, we introduce a novel programming abstraction: Memoized Monadic Iterators (MiMIs). MiMIs combine the disparate ideas of ASTGen's iterators, LogicT's additive monad interface, and SciFe's memoization into a single coherent programming abstraction. Our evaluation shows that MiMIs are typically faster than the state of the art with orders of magnitude less memory used, and that MiMIs overall scale better to big testing problems than any competitor. Thanks to their fundamental design, MiMIs are implemented in under 200 LOC (compared to thousands with most competitors), and require users to familiarize themselves with collectively only 7 operations and types (less than half as many as the state of the art), all without sacrificing generator conciseness.},
  keywords = {Additives,Computer bugs,Generators,Java,Memory management,Testing,Tools},
  file = {/Users/harrison/Zotero/storage/78HELJZR/Dewey et al. - 2020 - MiMIs Simple, Efficient, and Fast Bounded-Exhaust.pdf;/Users/harrison/Zotero/storage/YKY2GZBA/9159050.html}
}

@article{diatchkiDaedalusSaferDocument2024,
  title = {Daedalus: {{Safer Document Parsing}}},
  shorttitle = {Daedalus},
  author = {Diatchki, Iavor S. and Dodds, Mike and Goldstein, Harrison and Harris, Bill and Holland, David A. and Razet, Benoit and Schlesinger, Cole and Winwood, Simon},
  year = {2024},
  month = jun,
  journal = {Daedalus},
  volume = {8},
  number = {PLDI},
  pages = {180:816--180:840},
  doi = {10.1145/3656410},
  urldate = {2024-09-18},
  abstract = {Despite decades of contributions to the theoretical foundations of parsing and the many tools available to aid        in parser development, many security attacks in the wild still exploit parsers. The issues are myriad---flaws        in memory management in contexts lacking memory safety, flaws in syntactic or semantic validation of        input, and misinterpretation of hundred-page-plus standards documents. It remains challenging to build and        maintain parsers for common, mature data formats.        In response to these challenges, we present Daedalus, a new domain-specific language (DSL) and toolchain        for writing safe parsers. Daedalus is built around functional-style parser combinators, which suit the rich data        dependencies often found in complex data formats. It adds domain-specific constructs for stream manipulation,        allowing the natural expression of parsing noncontiguous formats. Balancing between expressivity and        domain-specific constructs lends Daedalus specifications simplicity and leaves them amenable to analysis. As        a stand-alone DSL, Daedalus is able to generate safe parsers in multiple languages, currently C++ and Haskell.        We have implemented 20 data formats with Daedalus, including two large, complex formats---PDF and        NITF---and our evaluation shows that Daedalus parsers are concise and performant. Our experience with PDF        forms our largest case study. We worked with the PDF Association to build a reference implementation, which        was subject to a red-teaming exercise along with a number of other PDF parsers and was the only parser to be        found free of defects.},
  file = {/Users/harrison/Zotero/storage/6FC36I4Q/Diatchki et al. - 2024 - Daedalus Safer Document Parsing.pdf}
}

@inproceedings{dinellaTOGANeuralMethod2022,
  title = {{{TOGA}}: A Neural Method for Test Oracle Generation},
  shorttitle = {{{TOGA}}},
  booktitle = {Proceedings of the 44th {{International Conference}} on {{Software Engineering}}},
  author = {Dinella, Elizabeth and Ryan, Gabriel and Mytkowicz, Todd and Lahiri, Shuvendu K.},
  year = {2022},
  month = jul,
  series = {{{ICSE}} '22},
  pages = {2130--2141},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3510003.3510141},
  urldate = {2023-10-05},
  abstract = {Testing is widely recognized as an important stage of the software development lifecycle. Effective software testing can provide benefits such as bug finding, preventing regressions, and documentation. In terms of documentation, unit tests express a unit's intended functionality, as conceived by the developer. A test oracle, typically expressed as an condition, documents the intended behavior of a unit under a given test prefix. Synthesizing a functional test oracle is a challenging problem, as it must capture the intended functionality rather than the implemented functionality. In this paper, we propose TOGA (a neural method for {$<$}u{$>$}T{$<$}/u{$>$}est {$<$}u{$>$}O{$<$}/u{$>$}racle {$<$}u{$>$}G{$<$}/u{$>$}ener{$<$}u{$>$}A{$<$}/u{$>$}tion), a unified transformer-based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method. Our approach can handle units with ambiguous or missing documentation, and even units with a missing implementation. We evaluate our approach on both oracle inference accuracy and functional bug-finding. Our technique improves accuracy by 33\% over existing oracle inference approaches, achieving 96\% overall accuracy on a held out test dataset. Furthermore, we show that when integrated with a automated test generation tool (EvoSuite), our approach finds 57 real world bugs in large-scale Java programs, including 30 bugs that are not found by any other automated testing method in our evaluation.},
  isbn = {978-1-4503-9221-1},
  file = {/Users/harrison/Zotero/storage/HSVDFP9H/Dinella et al. - 2022 - TOGA a neural method for test oracle generation.pdf}
}

@misc{doddsZacHatfieldDoddsPersonal2022,
  title = {Zac {{Hatfield-Dodds Personal Communication}}},
  author = {Dodds, Zac Hatfield},
  year = {2022},
  annotation = {Published: current maintainer of Hypothesis (https://github.com/HypothesisWorks/hypothesis). Personal communication}
}

@article{doerflerProfessorWhichIsn2021,
  title = {"{{I}}'m a {{Professor}}, Which Isn't Usually a Dangerous Job": {{Internet-facilitated Harassment}} and {{Its Impact}} on {{Researchers}}},
  shorttitle = {"{{I}}'m a {{Professor}}, Which Isn't Usually a Dangerous Job"},
  author = {Doerfler, Periwinkle and Forte, Andrea and De Cristofaro, Emiliano and Stringhini, Gianluca and Blackburn, Jeremy and McCoy, Damon},
  year = {2021},
  month = oct,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {5},
  number = {CSCW2},
  pages = {341:1--341:32},
  doi = {10.1145/3476082},
  urldate = {2024-05-16},
  abstract = {While the Internet has dramatically increased the exposure that research can receive, it has also facilitated harassment against scholars. To understand the impact that these attacks can have on the work of researchers, we perform a series of systematic interviews with researchers including academics, journalists, and activists, who have experienced targeted, Internet-facilitated harassment. We provide a framework for understanding the types of harassers that target researchers, the harassment that ensues, and the personal and professional impact on individuals and academic freedom. We then study preventative and remedial strategies available, and the institutions that prevent some of these strategies from being more effective. Finally, we discuss the ethical structures that could facilitate more equitable access to participating in research without serious personal suffering.},
  keywords = {harassment,researcher safety},
  file = {/Users/harrison/Zotero/storage/LVG2AVQC/Doerfler et al. - 2021 - I'm a Professor, which isn't usually a dangerous .pdf}
}

@inproceedings{dolan-gavittLAVALargeScaleAutomated2016,
  title = {{{LAVA}}: {{Large-Scale Automated Vulnerability Addition}}},
  shorttitle = {{{LAVA}}},
  booktitle = {2016 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {{Dolan-Gavitt}, Brendan and Hulin, Patrick and Kirda, Engin and Leek, Tim and Mambretti, Andrea and Robertson, Wil and Ulrich, Frederick and Whelan, Ryan},
  year = {2016},
  month = may,
  pages = {110--121},
  issn = {2375-1207},
  doi = {10.1109/SP.2016.15},
  abstract = {Work on automating vulnerability discovery has long been hampered by a shortage of ground-truth corpora with which to evaluate tools and techniques. This lack of ground truth prevents authors and users of tools alike from being able to measure such fundamental quantities as miss and false alarm rates. In this paper, we present LAVA, a novel dynamic taint analysis-based technique for producing ground-truth corpora by quickly and automatically injecting large numbers of realistic bugs into program source code. Every LAVA bug is accompanied by an input that triggers it whereas normal inputs are extremely unlikely to do so. These vulnerabilities are synthetic but, we argue, still realistic, in the sense that they are embedded deep within programs and are triggered by real inputs. Using LAVA, we have injected thousands of bugs into eight real-world programs, including bash, tshark, and the GNU coreutils. In a preliminary evaluation, we found that a prominent fuzzer and a symbolic execution-based bug finder were able to locate some but not all LAVA-injected bugs, and that interesting patterns and pathologies were already apparent in their performance. Our work forms the basis of an approach for generating large ground-truth vulnerability corpora on demand, enabling rigorous tool evaluation and providing a high-quality target for tool developers.},
  keywords = {Computer bugs,Geophysical measurement techniques,Ground penetrating radar,Length measurement,Pathology,Privacy,Security},
  file = {/Users/harrison/Zotero/storage/94EZH5ST/Dolan-Gavitt et al. - 2016 - LAVA Large-Scale Automated Vulnerability Addition.pdf;/Users/harrison/Zotero/storage/5ZF9BSZ9/7546498.html}
}

@inproceedings{dolanTestingCrowbar2017,
  title = {Testing with Crowbar},
  booktitle = {{{OCaml Workshop}}},
  author = {Dolan, Stephen and Preston, Mindy},
  year = {2017},
  file = {/Users/harrison/Zotero/storage/BI7RBCK3/Dolan and Preston - 2017 - Testing with crowbar.pdf}
}

@inproceedings{drewToastboardUbiquitousInstrumentation2016,
  title = {The Toastboard: {{Ubiquitous}} Instrumentation and Automated Checking of Breadboarded Circuits},
  booktitle = {Proceedings of the 29th {{Annual Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Drew, Daniel and Newcomb, Julie L and McGrath, William and Maksimovic, Filip and Mellis, David and Hartmann, Bj{\"o}rn},
  year = {2016},
  pages = {677--686}
}

@inproceedings{drososWrexUnifedProgrammingbyExample2020,
  title = {Wrex: {{A Unifed Programming-by-Example Interaction}} for {{Synthesizing Readable Code}} for {{Data Scientists}}},
  booktitle = {{\textbackslash}confchi},
  author = {Drosos, Ian and Barik, Titus and Guo, Philip J. and DeLine, Robert and Gulwani, Sumit},
  year = {2020},
  publisher = {{\textbackslash}pubacm}
}

@misc{dubienFastcheck2024,
  title = {Fast-Check},
  author = {Dubien, Nicolas},
  year = {2024},
  urldate = {2024-02-26},
  abstract = {Property-based testing for JavaScript and TypeScript},
  howpublished = {https://fast-check.dev/},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/XQZVH5WC/fast-check.dev.html}
}

@article{dunfieldBidirectionalTyping2022,
  title = {Bidirectional {{Typing}}},
  author = {Dunfield, Jana and Krishnaswami, Neel},
  year = {2022},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {5},
  eprint = {1908.05839},
  primaryclass = {cs},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3450952},
  urldate = {2024-01-29},
  abstract = {Bidirectional typing combines two modes of typing: type checking, which checks that a program satisfies a known type, and type synthesis, which determines a type from the program. Using checking enables bidirectional typing to support features for which inference is undecidable; using synthesis enables bidirectional typing to avoid the large annotation burden of explicitly typed languages. In addition, bidirectional typing improves error locality. We highlight the design principles that underlie bidirectional type systems, survey the development of bidirectional typing from the prehistoric period before Pierce and Turner's local type inference to the present day, and provide guidance for future investigations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  file = {/Users/harrison/Zotero/storage/J54QQQ2D/Dunfield and Krishnaswami - 2022 - Bidirectional Typing.pdf;/Users/harrison/Zotero/storage/V4WQE89V/1908.html}
}

@article{duregardFeatFunctionalEnumeration2012,
  title = {Feat: Functional Enumeration of Algebraic Types},
  shorttitle = {Feat},
  author = {Dureg{\aa}rd, Jonas and Jansson, Patrik and Wang, Meng},
  year = {2012},
  month = sep,
  journal = {ACM SIGPLAN Notices},
  volume = {47},
  number = {12},
  pages = {61--72},
  issn = {0362-1340},
  doi = {10.1145/2430532.2364515},
  urldate = {2023-01-09},
  abstract = {In mathematics, an enumeration of a set S is a bijective function from (an initial segment of) the natural numbers to S. We define "functional enumerations" as efficiently computable such bijections. This paper describes a theory of functional enumeration and provides an algebra of enumerations closed under sums, products, guarded recursion and bijections. We partition each enumerated set into numbered, finite subsets. We provide a generic enumeration such that the number of each part corresponds to the size of its values (measured in the number of constructors). We implement our ideas in a Haskell library called testing-feat, and make the source code freely available. Feat provides efficient "random access" to enumerated values. The primary application is property-based testing, where it is used to define both random sampling (for example QuickCheck generators) and exhaustive enumeration (in the style of SmallCheck). We claim that functional enumeration is the best option for automatically generating test cases from large groups of mutually recursive syntax tree types. As a case study we use Feat to test the pretty-printer of the Template Haskell library (uncovering several bugs).},
  keywords = {enumeration,memoisation,property-based testing},
  file = {/Users/harrison/Zotero/storage/LEJU679F/Duregård et al. - 2012 - Feat functional enumeration of algebraic types.pdf}
}

@article{dutraEfficientSamplingSAT2019,
  title = {Efficient {{Sampling}} of {{SAT}} and {{SMT Solutions}} for {{Testing}} and {{Verification}}},
  author = {Dutra, Rafael Tupynamb{\'a}},
  year = {2019},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/JZN64KMR/Dutra - Efficient Sampling of SAT and SMT Solutions for Te.pdf}
}

@article{dyerApplyingCognitivePrinciples2022,
  title = {Applying Cognitive Principles to Model-Finding Output: The Positive Value of Negative Information},
  shorttitle = {Applying Cognitive Principles to Model-Finding Output},
  author = {Dyer, Tristan and Nelson, Tim and Fisler, Kathi and Krishnamurthi, Shriram},
  year = {2022},
  month = apr,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {OOPSLA1},
  pages = {79:1--79:29},
  doi = {10.1145/3527323},
  urldate = {2023-07-26},
  abstract = {Model-finders, such as SAT/SMT-solvers and Alloy, are used widely both directly and embedded in domain-specific tools. They support both conventional verification and, unlike other verification tools, property-free exploration. To do this effectively, they must produce output that helps users with these tasks. Unfortunately, the output of model-finders has seen relatively little rigorous human-factors study. Conventionally, these tools tend to show one satisfying instance at a time. Drawing inspiration from the cognitive science literature, we investigate two aspects of model-finder output: how many instances to show at once, and whether all instances must actually satisfy the input constraints. Using both controlled studies and open-ended talk-alouds, we show that there is benefit to showing negative instances in certain settings; the impact of multiple instances is less clear. Our work is a first step in a theoretically grounded approach to understanding how users engage cognitively with model-finder output, and how those tools might better support users in doing so.},
  keywords = {Alloy,cognitive science,model finding,user studies},
  file = {/Users/harrison/Zotero/storage/HNC7FGG4/Dyer et al. - 2022 - Applying cognitive principles to model-finding out.pdf}
}

@misc{eastlundQuickcheckCore2015,
  title = {Quickcheck for {{Core}}},
  author = {Eastlund, Carl},
  year = {2015},
  month = oct,
  journal = {Jane Street Tech Blog},
  urldate = {2023-06-26},
  abstract = {Automated testing is a powerful tool for finding bugs and specifying correctnessproperties of code. Haskell's Quickcheck library is the most well-knownautoma...},
  howpublished = {https://blog.janestreet.com/quickcheck-for-core/},
  file = {/Users/harrison/Zotero/storage/YLH5KKCU/quickcheck-for-core.html}
}

@article{economouFocusingRefinementTyping2023,
  title = {Focusing on {{Refinement Typing}}},
  author = {Economou, Dimitrios J. and Krishnaswami, Neel and Dunfield, Jana},
  year = {2023},
  month = dec,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {45},
  number = {4},
  pages = {22:1--22:62},
  issn = {0164-0925},
  doi = {10.1145/3610408},
  urldate = {2024-01-29},
  abstract = {We present a logically principled foundation for systematizing, in a way that works with any computational effect and evaluation order, SMT constraint generation seen in refinement type systems for functional programming languages. By carefully combining a focalized variant of call-by-push-value, bidirectional typing, and our novel technique of value-determined indexes, our system generates solvable SMT constraints without existential (unification) variables. We design a polarized subtyping relation allowing us to prove our logically focused typing algorithm is sound, complete, and decidable. We prove type soundness of our declarative system with respect to an elementary domain-theoretic denotational semantics. Type soundness implies, relatively simply, the total correctness and logical consistency of our system. The relative ease with which we obtain both algorithmic and semantic results ultimately stems from the proof-theoretic technique of focalization.},
  keywords = {bidirectional typechecking,call-by-push-value,polarity,Refinement types},
  file = {/Users/harrison/Zotero/storage/KJDW5WFR/Economou et al. - 2023 - Focusing on Refinement Typing.pdf}
}

@inproceedings{eguchiAutomatedSynthesisFunctional2018,
  title = {Automated {{Synthesis}} of {{Functional Programs}} with {{Auxiliary Functions}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Eguchi, Shingo and Kobayashi, Naoki and Tsukada, Takeshi},
  editor = {Ryu, Sukyoung},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {223--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-02768-1_13},
  abstract = {Polikarpova et al. have recently proposed a method for synthesizing functional programs from specifications expressed as refinement types, and implemented a program synthesis tool Synquid. Although Synquid can generate non-trivial programs on various data structures such as lists and binary search trees, it cannot automatically generate programs that require auxiliary functions, unless users provide the specifications of auxiliary functions. We propose an extension of Synquid to enable automatic synthesis of programs with auxiliary functions. The idea is to prepare a template of the target function containing unknown auxiliary functions, infer the types of auxiliary functions, and then use Synquid to synthesize the auxiliary functions. We have implemented a program synthesizer based on our method, and confirmed through experiments that our method can synthesize several programs with auxiliary functions, which Synquid is unable to automatically synthesize.},
  isbn = {978-3-030-02768-1},
  langid = {english},
  keywords = {Binary Search Tree,Polikarpov,Program Synthesis Tools,Refinement Types,Unknown Auxiliary Functions},
  file = {/Users/harrison/Zotero/storage/55L9H3YT/Eguchi et al. - 2018 - Automated Synthesis of Functional Programs with Au.pdf}
}

@inproceedings{elazarmittelmanDonGoRabbit2023,
  title = {Don't {{Go Down}} the {{Rabbit Hole}}: {{Reprioritizing Enumeration}} for {{Property-Based Testing}}},
  shorttitle = {Don't {{Go Down}} the {{Rabbit Hole}}},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Haskell Symposium}}},
  author = {Elazar Mittelman, Segev and Resnick, Aviel and Perez, Ivan and Goodloe, Alwyn E. and Lampropoulos, Leonidas},
  year = {2023},
  month = aug,
  series = {Haskell 2023},
  pages = {59--71},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3609026.3609730},
  urldate = {2023-11-28},
  abstract = {In our implementation, we integrate a state-of-the-art enumeration-based property-based testing framework, LazySearch, with a state-of-the-art combinatorial testing tool, NIST's ACTS, and demonstrate how it can significantly speed up the effectiveness of testing---up to more than 20{\texttimes} in the case of a prior System F case study from the literature.},
  isbn = {9798400702983},
  keywords = {combinatorial testing,enumeration,functional programming,generation,property-based testing},
  file = {/Users/harrison/Zotero/storage/NVPQ58VJ/Elazar Mittelman et al. - 2023 - Don’t Go Down the Rabbit Hole Reprioritizing Enum.pdf}
}

@inproceedings{ellsonGraphvizOpenSource2002,
  title = {Graphviz--- {{Open Source Graph Drawing Tools}}},
  booktitle = {Graph {{Drawing}}},
  author = {Ellson, John and Gansner, Emden and Koutsofios, Lefteris and North, Stephen C. and Woodhull, Gordon},
  editor = {Mutzel, Petra and J{\"u}nger, Michael and Leipert, Sebastian},
  year = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {483--484},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45848-4_57},
  abstract = {Graphviz is a heterogeneous collection of graph drawing tools containing batch layout programs (dot, neato, fdp, twopi); a platform for incremental layout (Dynagraph); customizable graph editors (dotty, Grappa); a server for including graphs in Web pages (WebDot); support for graphs as COM objects (Montage); utility programs useful in graph visualization; and libraries for attributed graphs. The software is available under an Open Source license. The article[1] provides a detailed description of the package.},
  isbn = {978-3-540-45848-7},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/YEPNC8XX/Ellson et al. - 2002 - Graphviz— Open Source Graph Drawing Tools.pdf}
}

@inproceedings{enoiuModelTestersCognitive2020,
  title = {Towards a Model of Testers' Cognitive Processes: {{Software}} Testing as a Problem Solving Approach},
  booktitle = {2020 {{IEEE}} 20th {{International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security Companion}} ({{QRS-C}})},
  author = {Enoiu, Eduard and Tukseferi, Gerald and Feldt, Robert},
  year = {2020},
  pages = {272--279},
  publisher = {IEEE}
}

@misc{facebookCreatereactapp2024,
  title = {Create-React-App},
  author = {{facebook}},
  year = {2024}
}

@article{fanHighThroughputFormalMethodsAssistedFuzzing,
  title = {High-{{Throughput}}, {{Formal-Methods-Assisted Fuzzing}} for {{LLVM}}},
  author = {Fan, Yuyou and Regehr, John},
  abstract = {It is very difficult to thoroughly test a compiler, and as a consequence it is common for released versions of production compilers to contain bugs that cause them to crash and to emit incorrect object code. We created alive-mutate, a mutation-based fuzzing tool that takes test cases written by humans and randomly modifies them, based on the hypothesis that while compiler developers are fundamentally good at writing tests, they also tend to miss corner cases. Alive-mutate is integrated with the Alive2 translation validation tool for LLVM, which is useful because it checks the behavior of optimizations for all possible values of input variables. Alive-mutate is also integrated with the LLVM middle-end, allowing it to perform mutations, optimizations, and formal verification of the optimizations all within a single program---avoiding numerous sources of overhead. Alive-mutate's fuzzing throughput is 12x higher, on average, than a fuzzing workflow that runs mutation, optimization, and formal verification in separate processes. So far we have used alive-mutate to find and report 33 previously unknown bugs in LLVM.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/L27RPI9T/Fan and Regehr - High-Throughput, Formal-Methods-Assisted Fuzzing f.pdf}
}

@article{fengProgramSynthesisUsing2018,
  title = {Program Synthesis Using Conflict-Driven Learning},
  author = {Feng, Yu and Martins, Ruben and Bastani, Osbert and Dillig, Isil},
  year = {2018},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {53},
  number = {4},
  pages = {420--435},
  issn = {0362-1340},
  doi = {10.1145/3296979.3192382},
  urldate = {2023-02-16},
  abstract = {We propose a new conflict-driven program synthesis technique that is capable of learning from past mistakes. Given a spurious program that violates the desired specification, our synthesis algorithm identifies the root cause of the conflict and learns new lemmas that can prevent similar mistakes in the future. Specifically, we introduce the notion of equivalence modulo conflict and show how this idea can be used to learn useful lemmas that allow the synthesizer to prune large parts of the search space. We have implemented a general-purpose CDCL-style program synthesizer called Neo and evaluate it in two different application domains, namely data wrangling in R and functional programming over lists. Our experiments demonstrate the substantial benefits of conflict-driven learning and show that Neo outperforms two state-of-the-art synthesis tools, Morpheus and Deepcoder, that target these respective domains.},
  keywords = {automated reasoning,conflict-driven learning,program synthesis},
  file = {/Users/harrison/Zotero/storage/7URT3I7Q/Feng et al. - 2018 - Program synthesis using conflict-driven learning.pdf}
}

@incollection{fernandesOrigamiUnfoldingAbstraction2024,
  title = {Origami: (Un)Folding the Abstraction of Recursion Schemes for Program Synthesis},
  shorttitle = {Origami},
  author = {Fernandes, Matheus Campos and de Franca, Fabricio Olivetti and Francesquini, Emilio},
  year = {2024},
  eprint = {2402.13828},
  primaryclass = {cs},
  pages = {263--281},
  doi = {10.1007/978-981-99-8413-8_14},
  urldate = {2025-03-12},
  abstract = {Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples. One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate. A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption. Recursion Schemes are very powerful as they allow the construction of programs that can summarize data, create sequences, and perform advanced calculations. The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized. In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results. To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB benchmark using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations. We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Programming Languages},
  file = {/Users/harrison/Zotero/storage/8HMNCLMY/Fernandes et al. - 2024 - Origami (un)folding the abstraction of recursion .pdf;/Users/harrison/Zotero/storage/PP6LC3KL/2402.html}
}

@inproceedings{fioraldiAFLCombiningIncremental2020,
  title = {\{\vphantom\}{{AFL}}++\vphantom\{\} : {{Combining Incremental Steps}} of {{Fuzzing Research}}},
  shorttitle = {\{\vphantom\}{{AFL}}++\vphantom\{\}},
  booktitle = {14th {{USENIX Workshop}} on {{Offensive Technologies}} ({{WOOT}} 20)},
  author = {Fioraldi, Andrea and Maier, Dominik and Ei{\ss}feldt, Heiko and Heuse, Marc},
  year = {2020},
  urldate = {2022-12-01},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/LRLUQQQK/Fioraldi et al. - 2020 - AFL++ Combining Incremental Steps of Fuzzing R.pdf;/Users/harrison/Zotero/storage/68IRVCYC/fioraldi.html}
}

@inproceedings{fortierCutsCircularProofs2013,
  title = {Cuts for Circular Proofs: Semantics and Cut-Elimination},
  shorttitle = {Cuts for Circular Proofs},
  booktitle = {Computer {{Science Logic}} 2013 ({{CSL}} 2013)},
  author = {Fortier, J{\'e}r{\^o}me and Santocanale, Luigi},
  editor = {Rocca, Simona Ronchi Della},
  year = {2013},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {23},
  pages = {248--262},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  address = {Dagstuhl, Germany},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.CSL.2013.248},
  urldate = {2023-07-11},
  isbn = {978-3-939897-60-6},
  keywords = {categorical proof-theory,fixpoints,inductive and coinductive types,initial and final (co)algebras},
  file = {/Users/harrison/Zotero/storage/L4PJZPWN/Fortier and Santocanale - 2013 - Cuts for circular proofs semantics and cut-elimin.pdf;/Users/harrison/Zotero/storage/JJV4N6HK/4201.html}
}

@phdthesis{fosterBidirectionalProgrammingLanguages2009,
  title = {Bidirectional Programming Languages},
  author = {Foster, John Nathan},
  year = {2009},
  address = {United States -- Pennsylvania},
  urldate = {2022-11-22},
  abstract = {The need to edit source data through a view arises in a host of applications across many different areas of computing. Unfortunately, few existing systems provide support for updatable views. In practice, when they are needed, updatable views are usually implemented using two separate programs: one that computes the view from the source and another that handles updates. This rudimentary design is tedious for programmers, difficult to reason about, and a nightmare to maintain. This dissertation presents bidirectional programming languages, which provide an elegant and effective mechanism for describing updatable views. Unlike programs written in an ordinary language, which only work in one direction, programs in a bidirectional language can be run both forwards and backwards: from left to right, they describe functions that map sources to views, and from right to left, they describe functions that map updated views back to updated sources. Besides eliminating redundancy, these languages can be designed to ensure correctness, guaranteeing by construction that the two functions work well together. Starting from the foundations, we define a general semantic space of well-behaved bidirectional transformations called lenses. Then, building on this foundation, we describe a particular language for defining lenses on strings with syntax based on the familiar regular operators (union, concatenation, and Kleene star). We present extensions to the basic framework that address the subtle complications that arise when lenses are used to manipulate, data containing unimportant details, ordered data, and confidential data.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781109710137},
  langid = {english},
  school = {University of Pennsylvania},
  file = {/Users/harrison/Zotero/storage/TFCYPJHJ/Foster - Bidirectional programming languages.pdf}
}

@article{frankGeneratingWellTypedTerms2024,
  title = {Generating {{Well-Typed Terms That Are Not}} ``{{Useless}}''},
  author = {Frank, Justin and Quiring, Benjamin and Lampropoulos, Leonidas},
  year = {2024},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {8},
  number = {POPL},
  pages = {77:2318--77:2339},
  doi = {10.1145/3632919},
  urldate = {2024-01-22},
  abstract = {Random generation of well-typed terms lies at the core of effective random testing of compilers for functional languages. Existing techniques have had success following a top-down type-oriented approach to generation that makes choices locally, which suffers from an inherent limitation: the type of an expression is often generated independently from the expression itself. Such generation frequently yields functions with argument types that cannot be used to produce a result in a meaningful way, leaving those arguments unused. Such "use-less" functions can hinder both performance, as the argument generation code is dead but still needs to be compiled, and effectiveness, as a lot of interesting optimizations are tested less frequently. In this paper, we introduce a novel algorithm that is significantly more effective at generating functions that use their arguments. We formalize both the "local" and the "nonlocal" algorithms as step-relations in an extension of the simply-typed lambda calculus with type and arguments holes, showing how delaying the generation of types for subexpressions by allowing nonlocal generation steps leads to "useful" functions.},
  keywords = {property-based testing,test generation,well-typed lambda terms},
  file = {/Users/harrison/Zotero/storage/5CBF84CU/Frank et al. - 2024 - Generating Well-Typed Terms That Are Not “Useless”.pdf}
}

@inproceedings{fraserDoesAutomatedWhitebox2013,
  title = {Does Automated White-Box Test Generation Really Help Software Testers?},
  booktitle = {Proceedings of the 2013 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Fraser, Gordon and Staats, Matt and McMinn, Phil and Arcuri, Andrea and Padberg, Frank},
  year = {2013},
  month = jul,
  series = {{{ISSTA}} 2013},
  pages = {291--301},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2483760.2483774},
  urldate = {2023-02-04},
  abstract = {Automated test generation techniques can efficiently produce test data that systematically cover structural aspects of a program. In the absence of a specification, a common assumption is that these tests relieve a developer of most of the work, as the act of testing is reduced to checking the results of the tests. Although this assumption has persisted for decades, there has been no conclusive evidence to date confirming it. However, the fact that the approach has only seen a limited uptake in industry suggests the contrary, and calls into question its practical usefulness. To investigate this issue, we performed a controlled experiment comparing a total of 49 subjects split between writing tests manually and writing tests with the aid of an automated unit test generation tool, EvoSuite. We found that, on one hand, tool support leads to clear improvements in commonly applied quality metrics such as code coverage (up to 300\% increase). However, on the other hand, there was no measurable improvement in the number of bugs actually found by developers. Our results not only cast some doubt on how the research community evaluates test generation tools, but also point to improvements and future work necessary before automated test generation tools will be widely adopted by practitioners.},
  isbn = {978-1-4503-2159-4},
  keywords = {automated test generation,branch coverage,empirical software engineering,Unit testing},
  file = {/Users/harrison/Zotero/storage/3ZKRSERU/Fraser et al. - 2013 - Does automated white-box test generation really he.pdf}
}

@inproceedings{fraserEvoSuiteAutomaticTest2011,
  title = {{{EvoSuite}}: Automatic Test Suite Generation for Object-Oriented Software},
  shorttitle = {{{EvoSuite}}},
  booktitle = {Proceedings of the 19th {{ACM SIGSOFT}} Symposium and the 13th {{European}} Conference on {{Foundations}} of Software Engineering},
  author = {Fraser, Gordon and Arcuri, Andrea},
  year = {2011},
  month = sep,
  series = {{{ESEC}}/{{FSE}} '11},
  pages = {416--419},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2025113.2025179},
  urldate = {2023-02-04},
  abstract = {To find defects in software, one needs test cases that execute the software systematically, and oracles that assess the correctness of the observed behavior when running these test cases. This paper presents EvoSuite, a tool that automatically generates test cases with assertions for classes written in Java code. To achieve this, EvoSuite applies a novel hybrid approach that generates and optimizes whole test suites towards satisfying a coverage criterion. For the produced test suites, EvoSuite suggests possible oracles by adding small and effective sets of assertions that concisely summarize the current behavior; these assertions allow the developer to detect deviations from expected behavior, and to capture the current behavior in order to protect against future defects breaking this behavior.},
  isbn = {978-1-4503-0443-6},
  keywords = {assertion generation,search based soft- ware testing,test case generation},
  file = {/Users/harrison/Zotero/storage/N8B55TX7/Fraser and Arcuri - 2011 - EvoSuite automatic test suite generation for obje.pdf}
}

@article{fraserLargeScaleEvaluationAutomated2014,
  title = {A {{Large-Scale Evaluation}} of {{Automated Unit Test Generation Using EvoSuite}}},
  author = {Fraser, Gordon and Arcuri, Andrea},
  year = {2014},
  month = dec,
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {24},
  number = {2},
  pages = {8:1--8:42},
  issn = {1049-331X},
  doi = {10.1145/2685612},
  urldate = {2023-02-04},
  abstract = {Research on software testing produces many innovative automated techniques, but because software testing is by necessity incomplete and approximate, any new technique faces the challenge of an empirical assessment. In the past, we have demonstrated scientific advance in automated unit test generation with the EVOSUITE tool by evaluating it on manually selected open-source projects or examples that represent a particular problem addressed by the underlying technique. However, demonstrating scientific advance is not necessarily the same as demonstrating practical value; even if VOSUITE worked well on the software projects we selected for evaluation, it might not scale up to the complexity of real systems. Ideally, one would use large ``real-world'' software systems to minimize the threats to external validity when evaluating research tools. However, neither choosing such software systems nor applying research prototypes to them are trivial tasks. In this article we present the results of a large experiment in unit test generation using the VOSUITE tool on 100 randomly chosen open-source projects, the 10 most popular open-source projects according to the SourceForge Web site, seven industrial projects, and 11 automatically generated software projects. The study confirms that VOSUITE can achieve good levels of branch coverage (on average, 71\% per class) in practice. However, the study also exemplifies how the choice of software systems for an empirical study can influence the results of the experiments, which can serve to inform researchers to make more conscious choices in the selection of software system subjects. Furthermore, our experiments demonstrate how practical limitations interfere with scientific advances, branch coverage on an unbiased sample is affected by predominant environmental dependencies. The surprisingly large effect of such practical engineering problems in unit testing will hopefully lead to a larger appreciation of work in this area, thus supporting transfer of knowledge from software testing research to practice.},
  keywords = {automated test generation,benchmark,branch coverage,empirical software engineering,Java,JUnit,Unit testing},
  file = {/Users/harrison/Zotero/storage/6NCTLSQ4/Fraser and Arcuri - 2014 - A Large-Scale Evaluation of Automated Unit Test Ge.pdf}
}

@misc{gallantBurntSushiQuickcheck2024,
  title = {{{BurntSushi}}/Quickcheck},
  author = {Gallant, Andrew},
  year = {2024},
  month = feb,
  urldate = {2024-02-26},
  abstract = {Automated property based testing for Rust (with shrinking).},
  copyright = {Unlicense}
}

@misc{garnock-jonesRecognisingGeneratingTerms2018,
  title = {Recognising and {{Generating Terms}} Using {{Derivatives}} of {{Parsing Expression Grammars}}},
  author = {{Garnock-Jones}, Tony and Eslamimehr, Mahdi and Warth, Alessandro},
  year = {2018},
  month = jan,
  number = {arXiv:1801.10490},
  eprint = {1801.10490},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.10490},
  urldate = {2022-11-22},
  abstract = {Grammar-based sentence generation has been thoroughly explored for Context-Free Grammars (CFGs), but remains unsolved for recognition-based approaches such as Parsing Expression Grammars (PEGs). Lacking tool support, language designers using PEGs have difficulty predicting the behaviour of their parsers. In this paper, we extend the idea of derivatives, originally formulated for regular expressions, to PEGs. We then present a novel technique for sentence generation based on derivatives, applicable to any grammatical formalism for which the derivative can be defined--now including PEGs. Finally, we propose applying derivatives more generally to other problems facing language designers and implementers.},
  archiveprefix = {arXiv},
  file = {/Users/harrison/Zotero/storage/RTSY69W2/Garnock-Jones et al. - 2018 - Recognising and Generating Terms using Derivatives.pdf;/Users/harrison/Zotero/storage/TM27ZD4P/1801.html}
}

@article{geuversProofAssistantsHistory2009,
  title = {Proof Assistants: {{History}}, Ideas and Future},
  shorttitle = {Proof Assistants},
  author = {Geuvers, H.},
  year = {2009},
  month = feb,
  journal = {Sadhana},
  volume = {34},
  number = {1},
  pages = {3--25},
  issn = {0973-7677},
  doi = {10.1007/s12046-009-0001-5},
  urldate = {2024-07-07},
  abstract = {In this paper I will discuss the fundamental ideas behind proof assistants: What are they and what is a proof anyway? I give a short history of the main ideas, emphasizing the way they ensure the correctness of the mathematics formalized. I will also briefly discuss the places where proof assistants are used and how we envision their extended use in the future. While being an introduction into the world of proof assistants and the main issues behind them, this paper is also a position paper that pushes the further use of proof assistants. We believe that these systems will become the future of mathematics, where definitions, statements, computations and proofs are all available in a computerized form. An important application is and will be in computer supported modelling and verification of systems. But there is still a long road ahead and I will indicate what we believe is needed for the further proliferation of proof assistants.},
  langid = {english},
  keywords = {formalized mathematics,logic,Proof assistant,software correctness,verification},
  file = {/Users/harrison/Zotero/storage/84WAIFNR/Geuvers - 2009 - Proof assistants History, ideas and future.pdf}
}

@article{ghaemiTransformersSourceCode2024,
  title = {Transformers in Source Code Generation: {{A}} Comprehensive Survey},
  shorttitle = {Transformers in Source Code Generation},
  author = {Ghaemi, Hadi and Alizadehsani, Zakieh and Shahraki, Amin and Corchado, Juan M.},
  year = {2024},
  month = jun,
  journal = {Journal of Systems Architecture},
  pages = {103193},
  issn = {1383-7621},
  doi = {10.1016/j.sysarc.2024.103193},
  urldate = {2024-06-09},
  abstract = {Transformers have revolutionized natural language processing (NLP) and have had a huge impact on automating tasks. Recently, transformers have led to the development of powerful large language models (LLMs), which have advanced automatic code generation. This study provides a review of code generation concepts and transformer applications in this field. First, the fundamental concepts of the attention mechanism embedded into transformers are explored. Then, predominant automated code generation approaches are briefly reviewed, including non-learning code generation (e.g., rule-based), shallow learning (e.g., heuristic rules, grammar-based), and deep learning models. Afterward, this survey reviews pre-training and fine-tuning techniques for code generation, focusing on the application of efficient transformer methods such as parameter-efficient tuning, instruction tuning, and prompt tuning. Additionally, this work briefly outlines resources for code generation (e.g., datasets, benchmarks, packages) and evaluation metrics utilized in code generation processes. Finally, the challenges and potential research directions (e.g., multimodal learning) are investigated in depth.},
  keywords = {Alignment-tuning,Code generation,Efficient fine-tuning,Large language models(LLMs),Pre-training,Prompt-tuning,Transformers},
  file = {/Users/harrison/Zotero/storage/U3MUAXCB/S1383762124001309.html}
}

@inproceedings{ghoshSystematicReviewProgram2020,
  title = {A {{Systematic Review}} on {{Program Debugging Techniques}}},
  booktitle = {Smart {{Computing Paradigms}}: {{New Progresses}} and {{Challenges}}},
  author = {Ghosh, Debolina and Singh, Jagannath},
  editor = {El{\c c}i, Atilla and Sa, Pankaj Kumar and Modi, Chirag N. and Olague, Gustavo and Sahoo, Manmath N. and Bakshi, Sambit},
  year = {2020},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {193--199},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-9680-9_16},
  abstract = {In softwareGhosh, Debolina~engineering, debugging is a most tedious job. Finding and correcting the bug takes much more time and effort than coding. Many researchers haveSingh,Jagannath~worked for making the debugging process easier. Many existing debugging techniques are available. Here, in this paper, we review various new emerging trends of software debugging techniques which is mostly used by the developers or testers for a particular application.},
  isbn = {9789811396809},
  langid = {english},
  keywords = {Breakpoint,Debugging,Java,Omniscient debugging,Testing},
  file = {/Users/harrison/Zotero/storage/E7ELMM87/Ghosh and Singh - 2020 - A Systematic Review on Program Debugging Technique.pdf}
}

@inproceedings{gillHaskellProgramCoverage2007,
  title = {Haskell Program Coverage},
  booktitle = {Proceedings of the {{ACM SIGPLAN Workshop}} on {{Haskell}}, {{Haskell}} 2007, {{Freiburg}}, {{Germany}}, {{September}} 30, 2007},
  author = {Gill, Andy and Runciman, Colin},
  editor = {Keller, Gabriele},
  year = {2007},
  pages = {1--12},
  publisher = {ACM},
  doi = {10.1145/1291201.1291203},
  file = {/Users/harrison/Zotero/storage/TNFZLVZT/Gill and Runciman - 2007 - Haskell program coverage.pdf}
}

@inproceedings{ginosarAuthoringMultistageCode2013,
  title = {Authoring Multi-Stage Code Examples with Editable Code Histories},
  booktitle = {Proceedings of the 26th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Ginosar, Shiry and De Pombo, Luis Fernando and Agrawala, Maneesh and Hartmann, Bjorn},
  year = {2013},
  month = oct,
  pages = {485--494},
  publisher = {ACM},
  address = {St. Andrews Scotland, United Kingdom},
  doi = {10.1145/2501988.2502053},
  urldate = {2024-11-02},
  abstract = {Multi-stage code examples present multiple versions of a program where each stage increases the overall complexity of the code. In order to acquire strategies of program construction using a new language or API, programmers consult multistage code examples in books, tutorials and online videos. Authoring multi-stage code examples is currently a tedious process, as it involves keeping several stages of code synchronized in the face of edits and error corrections. We document these difficulties with a formative study examining how programmers author multi-stage code examples. We then present an IDE extension that helps authors create multi-stage code examples by propagating changes (insertions, deletions and modifications) to multiple saved versions of their code. Our system adapts revision control algorithms to the specific task of evolving example code. An informal evaluation finds that taking snapshots of a program as it is being developed and editing these snapshots in hindsight help users in creating multi-stage code examples.},
  isbn = {978-1-4503-2268-3},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/EZCWIFIT/Ginosar et al. - 2013 - Authoring multi-stage code examples with editable .pdf}
}

@article{girardSystemVariableTypes1986,
  title = {The System {{F}} of Variable Types, Fifteen Years Later},
  author = {Girard, Jean-Yves},
  year = {1986},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {45},
  pages = {159--192},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(86)90044-7},
  urldate = {2023-02-26},
  abstract = {The semantic study of system F stumbles on the problem of variable types for which there was no convincing interpretation; we develop here a semantics based on the category-theoretic idea of direct limit, so that the behaviour of a variable type on any domain is determined by its behaviour on finite ones, thus getting rid of the circularity of variable types. To do so, one has first to simplify somehow the extant semantic ideas, replacing Scott domains by the simpler and more finitary qualitative domains. The interpretation obtained is extremely compact, as shown on simple examples. The paper also contains the definitions of a very small `universal model' of lambda-calculus, and investigates the concept totality.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/G3EAKLWE/Girard - 1986 - The system F of variable types, fifteen years late.pdf;/Users/harrison/Zotero/storage/AB3FA7UT/0304397586900447.html}
}

@incollection{giryCategoricalApproachProbability1982,
  title = {A {{Categorical Approach}} to {{Probability Theory}}},
  booktitle = {Categorical Aspects of Topology and Analysis},
  author = {Giry, Michele},
  year = {1982},
  pages = {68--85},
  publisher = {Springer},
  file = {/Users/harrison/Zotero/storage/74DWY32Y/Giry - A CATEGORICAL APPROACH TO PROBABILITY THEORY.pdf}
}

@misc{githubCopilot2024,
  title = {Copilot},
  author = {GitHub},
  year = {2024}
}

@article{glassmanOverCodeVisualizingVariation2015,
  title = {{{OverCode}}: {{Visualizing Variation}} in {{Student Solutions}} to {{Programming Problems}} at {{Scale}}},
  shorttitle = {{{OverCode}}},
  author = {Glassman, Elena L. and Scott, Jeremy and Singh, Rishabh and Guo, Philip J. and Miller, Robert C.},
  year = {2015},
  month = mar,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {22},
  number = {2},
  pages = {7:1--7:35},
  issn = {1073-0516},
  doi = {10.1145/2699751},
  urldate = {2024-02-25},
  abstract = {In MOOCs, a single programming exercise may produce thousands of solutions from learners. Understanding solution variation is important for providing appropriate feedback to students at scale. The wide variation among these solutions can be a source of pedagogically valuable examples and can be used to refine the autograder for the exercise by exposing corner cases. We present OverCode, a system for visualizing and exploring thousands of programming solutions. OverCode uses both static and dynamic analysis to cluster similar solutions, and lets teachers further filter and cluster solutions based on different criteria. We evaluated OverCode against a nonclustering baseline in a within-subjects study with 24 teaching assistants and found that the OverCode interface allows teachers to more quickly develop a high-level view of students' understanding and misconceptions, and to provide feedback that is relevant to more students' solutions.},
  keywords = {learning at scale,Programming education},
  file = {/Users/harrison/Zotero/storage/42ES74DD/Glassman et al. - 2015 - OverCode Visualizing Variation in Student Solutio.pdf}
}

@inproceedings{glassmanVisualizingAPIUsage2018,
  title = {Visualizing {{API Usage Examples}} at {{Scale}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Glassman, Elena L. and Zhang, Tianyi and Hartmann, Bj{\"o}rn and Kim, Miryung},
  year = {2018},
  month = apr,
  series = {{{CHI}} '18},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3173574.3174154},
  urldate = {2024-02-25},
  abstract = {Using existing APIs properly is a key challenge in programming, given that libraries and APIs are increasing in number and complexity. Programmers often search for online code examples in Q\&A forums and read tutorials and blog posts to learn how to use a given API. However, there are often a massive number of related code examples and it is difficult for a user to understand the commonalities and variances among them, while being able to drill down to concrete details. We introduce an interactive visualization for exploring a large collection of code examples mined from open-source repositories at scale. This visualization summarizes hundreds of code examples in one synthetic code skeleton with statistical distributions for canonicalized statements and structures enclosing an API call. We implemented this interactive visualization for a set of Java APIs and found that, in a lab study, it helped users (1) answer significantly more API usage questions correctly and comprehensively and (2) explore how other programmers have used an unfamiliar API.},
  isbn = {978-1-4503-5620-6},
  keywords = {api,code examples,interactive visualization,programming support},
  file = {/Users/harrison/Zotero/storage/TB5CI89L/Glassman et al. - 2018 - Visualizing API Usage Examples at Scale.pdf}
}

@inproceedings{gobertLorgnetteCreatingMalleable2023,
  title = {Lorgnette: {{Creating Malleable Code Projections}}},
  shorttitle = {Lorgnette},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Gobert, Camille and {Beaudouin-Lafon}, Michel},
  year = {2023},
  month = oct,
  series = {{{UIST}} '23},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3586183.3606817},
  urldate = {2024-01-23},
  abstract = {Projections of computer languages are tools that help users interact with representations that better fit their needs than plain text. We collected 62 projections from the literature and from a design workshop and found that 60\% of them can be implemented using a table, a graph or a form. However, projections are often hardcoded for specific languages and situations, and in most cases only the developers of a code editor can create or adapt projections, leaving no room for appropriation by their users. We introduce lorgnette, a new framework for letting programmers augment their code editor with projections. We demonstrate five examples that use lorgnette to create projections that can be reused in new contexts. We discuss how this approach could help democratise projections and conclude with future work.},
  isbn = {9798400701320},
  keywords = {Lorgnette,Projection,Semantic interaction},
  file = {/Users/harrison/Zotero/storage/IMRYQLH3/Gobert and Beaudouin-Lafon - 2023 - Lorgnette Creating Malleable Code Projections.pdf}
}

@inproceedings{godefroidGrammarbasedWhiteboxFuzzing2008,
  title = {Grammar-Based Whitebox Fuzzing},
  booktitle = {Proceedings of the 29th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
  year = {2008},
  month = jun,
  series = {{{PLDI}} '08},
  pages = {206--215},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1375581.1375607},
  urldate = {2023-02-26},
  abstract = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages. In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53\% to 81\% while using three times fewer tests.},
  isbn = {978-1-59593-860-2},
  keywords = {automatic test generation,grammars,program verification,software testing},
  file = {/Users/harrison/Zotero/storage/7XTRKTI3/Godefroid et al. - 2008 - Grammar-based whitebox fuzzing.pdf}
}

@inproceedings{godefroidLearnFuzzMachine2017,
  title = {Learn\&fuzz: {{Machine}} Learning for Input Fuzzing},
  booktitle = {2017 32nd {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Godefroid, Patrice and Peleg, Hila and Singh, Rishabh},
  year = {2017},
  pages = {50--59},
  publisher = {IEEE},
  file = {/Users/harrison/Zotero/storage/Y9HMFU7G/Godefroid et al. - 2017 - Learn&Fuzz Machine learning for input fuzzing.pdf}
}

@inproceedings{goldsteinJudgeTestIts2021,
  title = {Do {{Judge}} a {{Test}} by Its {{Cover}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Goldstein, Harrison and Hughes, John and Lampropoulos, Leonidas and Pierce, Benjamin C.},
  editor = {Yoshida, Nobuko},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {264--291},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-72019-3_10},
  abstract = {Property-based testing uses randomly generated inputs to validate high-level program specifications. It can be shockingly effective at finding bugs, but it often requires generating a very large number of inputs to do so. In this paper, we apply ideas from combinatorial testing, a powerful and widely studied testing methodology, to modify the distributions of our random generators so as to find bugs with fewer tests. The key concept is combinatorial coverage, which measures the degree to which a given set of tests exercises every possible choice of values for every small combination of input features.},
  copyright = {All rights reserved},
  isbn = {978-3-030-72019-3},
  langid = {english},
  keywords = {combinatorial testing},
  file = {/Users/harrison/Zotero/storage/WLSJ2GC9/Goldstein et al. - 2021 - Do Judge a Test by its Cover.pdf}
}

@article{goldsteinParsingRandomness2022,
  title = {Parsing {{Randomness}}},
  author = {Goldstein, Harrison and Pierce, Benjamin C.},
  year = {2022},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {OOPSLA2},
  pages = {128:89--128:113},
  doi = {10.1145/3563291},
  urldate = {2022-11-21},
  abstract = {Random data generators can be thought of as parsers of streams of randomness. This perspective on generators for random data structures is established folklore in the programming languages community, but it has never been formalized, nor have its consequences been deeply explored. We build on the idea of freer monads to develop free generators, which unify parsing and generation using a common structure that makes the relationship between the two concepts precise. Free generators lead naturally to a proof that a monadic generator can be factored into a parser plus a distribution over choice sequences. Free generators also support a notion of derivative, analogous to the familiar Brzozowski derivatives of formal languages, allowing analysis tools to "preview" the effect of a particular generator choice. This gives rise to a novel algorithm for generating data structures satisfying user-specified preconditions.},
  copyright = {All rights reserved},
  file = {/Users/harrison/Zotero/storage/7DHQCBVA/Goldstein and Pierce - 2022 - Parsing randomness.pdf}
}

@misc{goldsteinParsingRandomnessFree2022,
  title = {Parsing {{Randomness}}: {{Free Generators Development}}},
  author = {Goldstein, Harrison},
  year = {2022},
  month = oct,
  howpublished = {Zenodo}
}

@inproceedings{goldsteinProblemsProperties2022,
  title = {Some {{Problems}} with {{Properties}}},
  booktitle = {Workshop on {{Human Aspects}} of {{Types}} and {{Reasoning Assistants}}},
  author = {Goldstein, Harrison and Cutler, Joseph W and Stein, Adam and Pierce, Benjamin C and Head, Andrew},
  year = {2022},
  month = dec,
  volume = {1},
  copyright = {All rights reserved},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/QVA4RMQP/Goldstein et al. - Some Problems with Properties.pdf}
}

@phdthesis{goldsteinPropertyBasedTestingPeople2024,
  title = {Property-{{Based Testing}} for the {{People}}},
  author = {Goldstein, Harrison},
  year = {2024},
  month = may,
  school = {University of Pennsylvania}
}

@misc{goldsteinPropertyBasedTestingPractice2023,
  title = {Property-{{Based Testing}} in {{Practice}}: {{Codebook}}},
  shorttitle = {Property-{{Based Testing}} in {{Practice}}},
  author = {Goldstein, Harrison},
  year = {2023},
  month = dec,
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.10407686},
  urldate = {2024-01-09},
  abstract = {Property-based testing (PBT) is a testing methodology where users write executable formal specifications of software components and an automated harness checks these specifications against many automatically generated inputs. From its roots in the QuickCheck library in Haskell, PBT has made significant inroads in mainstream languages and industrial practice at companies such as Amazon, Volvo, and Stripe. As PBT extends its reach, it is important to understand how developers are using it in practice, where they see its strengths and weaknesses, and what innovations are needed to make it more effective.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english}
}

@inproceedings{goldsteinPropertyBasedTestingPractice2024,
  title = {Property-{{Based Testing}} in {{Practice}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 46th {{International Conference}} on {{Software Engineering}}},
  author = {Goldstein, Harrison and Cutler, Joseph W. and Dickstein, Daniel and Pierce, Benjamin C. and Head, Andrew},
  year = {2024},
  series = {{{ICSE}} '24},
  volume = {187},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {Lisbon, Portugal},
  doi = {10.1145/3597503.3639581},
  abstract = {Property-based testing (PBT) is a testing methodology where users write executable formal specifications of software components and an automated harness checks these specifications against many automatically generated inputs. From its roots in the QuickCheck library in Haskell, PBT has made significant inroads in mainstream languages and industrial practice at companies such as Amazon, Volvo, and Stripe. As PBT extends its reach, it is important to understand how developers are using it in practice, where they see its strengths and weaknesses, and what innovations are needed to make it more effective. We address these questions using data from 30 in-depth interviews with experienced users of PBT at Jane Street, a financial technology company making heavy and sophisticated use of PBT. These interviews provide empirical evidence that PBT's main strengths lie in testing complex code and in increasing confidence beyond what is available through conventional testing methodologies, and, moreover, that most uses fall into a relatively small number of high-leverage idioms. Its main weaknesses, on the other hand, lie in the relative complexity of writing properties and random data generators and in the difficulty of evaluating their effectiveness. From these observations, we identify a number of potentially high-impact areas for future exploration, including performance improvements, differential testing, additional high-leverage testing scenarios, better techniques for generating random input data, test-case reduction, and methods for evaluating the effectiveness of tests.},
  copyright = {All rights reserved},
  isbn = {9798400702174},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/XRH6XD6C/Goldstein - 2023 - Property-Based Testing in Practice Codebook.pdf}
}

@inproceedings{goldsteinReflectingRandomGeneration2023,
  title = {Reflecting on {{Random Generation}}},
  booktitle = {Proc. {{ACM Program}}. {{Lang}}.},
  author = {Goldstein, Harrison and Frohlich, Samantha and Wang, Meng and Pierce, Benjamin C.},
  year = {2023},
  month = aug,
  volume = {7},
  pages = {322--355},
  publisher = {Association for Computing Machinery},
  address = {Seattle, WA, USA},
  doi = {10.1145/3607842},
  abstract = {Expert users of property-based testing often labor to craft random generators that encode detailed knowledge about what it means for a test input to be valid and interesting. Fortunately, the fruits of this labor can also be put to other uses. In the bidirectional programming literature, for example, generators have been repurposed as validity checkers, while Python's Hypothesis library uses the same structures for shrinking and mutating test inputs. To unify and generalize these uses and many others, we propose reflective generators, a new foundation for random data generators that can "reflect" on an input value to calculate the random choices that could have been made to produce it. Reflective generators combine ideas from two existing abstractions: free generators and partial monadic profunctors. They can be used to implement and enhance the aforementioned shrinking and mutation algorithms, generalizing them to work for any values that can be produced by the generator, not just ones for which a trace of the generator's execution is available. Beyond shrinking and mutation, reflective generators generalize a published algorithm for example-based generation, and they can also be used as checkers, partial value completers, and other kinds of test data producers.},
  langid = {english},
  keywords = {bidirectional programming,property-based testing,random generation}
}

@misc{goldsteinReflectingRandomGeneration2023a,
  title = {Reflecting on {{Random Generation}}: {{Reflective Generators Development}}},
  author = {Goldstein, Harrison},
  year = {2023},
  howpublished = {Zenodo}
}

@inproceedings{goldsteinTycheMakingSense2024,
  title = {Tyche: {{Making Sense}} of {{Property-Based Testing Effectiveness}}},
  booktitle = {The 37th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}} ({{UIST}} '24)},
  author = {Goldstein, Harrison and Tao, Jeffrey and {Hatfield-Dodds}, Zac and Pierce, Benjamin C. and Head, Andrew},
  year = {2024},
  month = oct,
  pages = {16},
  doi = {10.1145/3654777.3676407},
  langid = {english}
}

@misc{goldsteinTycheSituExploration2023,
  type = {Demo},
  title = {Tyche: {{In Situ Exploration}} of {{Random Testing Effectiveness}} ({{Demo}})},
  author = {Goldstein, Harrison},
  year = {2023},
  month = oct,
  address = {San Francisco, CA, USA},
  collaborator = {Pierce, Benjamin C and Head, Andrew},
  copyright = {All rights reserved}
}

@inproceedings{goldsteinUngenerators2021,
  title = {Ungenerators},
  booktitle = {{{ICFP Student Research Competition}}},
  author = {Goldstein, Harrison},
  year = {2021},
  copyright = {All rights reserved},
  file = {/Users/harrison/Zotero/storage/LTSPBW8X/Goldstein - 2021 - Ungenerators.pdf}
}

@inproceedings{goodmanDeepStateSymbolicUnit2018,
  title = {{{DeepState}}: {{Symbolic Unit Testing}} for {{C}} and {{C}}++},
  shorttitle = {{{DeepState}}},
  booktitle = {Proceedings 2018 {{Workshop}} on {{Binary Analysis Research}}},
  author = {Goodman, Peter and Groce, Alex},
  year = {2018},
  publisher = {Internet Society},
  address = {San Diego, CA},
  doi = {10.14722/bar.2018.23009},
  urldate = {2023-11-27},
  abstract = {Unit testing is a popular software development methodology that can help developers detect functional regressions, explore boundary conditions, and document expected behavior. However, writing comprehensive unit tests is challenging and time-consuming, and developers seldom explore the obscure (and bug-hiding) corners of software behavior without assistance. DeepState is a tool that provides a Google Test-like API to give C and C++ developers push-button access to symbolic execution engines, such as Manticore and angr, and fuzzers, such as Dr. Fuzz. Rather than learning multiple complex tools, users learn one interface for defining a test harness, and can use various methods to automatically generate tests for software. In addition to providing a familiar interface to binary analysis and fuzzing for parameterized unit testing, DeepState also provides constructs that aid in the construction of API-sequence tests, where the tool chooses the functions or methods to call, allowing for even more diverse and powerful tests. By serving as a front-end to multiple tools, DeepState additionally provides a way to apply (novel) high-level strategies to test generation, and to compare effectiveness and efficiency of testing back-ends, including binary analysis tools.},
  isbn = {978-1-891562-50-1},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/7Q9JYQFU/Goodman and Groce - 2018 - DeepState Symbolic Unit Testing for C and C++.pdf}
}

@article{gopinathanMostlyAutomatedProof2023,
  title = {Mostly {{Automated Proof Repair}} for {{Verified Libraries}}},
  author = {Gopinathan, Kiran and Keoliya, Mayank and Sergey, Ilya},
  year = {2023},
  month = jun,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {7},
  number = {PLDI},
  pages = {107:25--107:49},
  doi = {10.1145/3591221},
  urldate = {2023-06-23},
  abstract = {The cost of maintaining formally specified and verified software is widely considered prohibitively high due to the need to constantly keep code and the proofs of its correctness in sync---the problem known as proof repair. One of the main challenges in automated proof repair for evolving code is to infer invariants for a new version of a once verified program that are strong enough to establish its full functional correctness. In this work, we present the first proof repair methodology for higher-order imperative functions, whose initial versions were verified in the Coq proof assistant and whose specifications remained unchanged. Our proof repair procedure is based on the combination of dynamic program alignment, enumerative invariant synthesis, and a novel technique for efficiently pruning the space of invariant candidates, dubbed proof-driven testing, enabled by the constructive nature of Coq's proof certificates. We have implemented our approach in a mostly-automated proof repair tool called Sisyphus. Given an OCaml function verified in Coq and its unverified new version, Sisyphus produces a Coq proof for the new version, discharging most of the new proof goals automatically and suggesting high-confidence obligations for the programmer to prove for the cases when automation fails. We have evaluated Sisyphus on 10 OCaml programs taken from popular libraries, that manipulate arrays and mutable data structures, considering their verified original and unverified evolved versions. Sisyphus has managed to repair proofs for all those functions, suggesting correct invariants and generating a small number of easy-to-prove residual obligations.},
  keywords = {invariant inference,mechanised proofs,proof repair,separation logic},
  file = {/Users/harrison/Zotero/storage/QPWG4VPJ/Gopinathan et al. - 2023 - Mostly Automated Proof Repair for Verified Librari.pdf}
}

@inproceedings{gordonProbabilisticProgramming2014,
  title = {Probabilistic Programming},
  booktitle = {Future of {{Software Engineering Proceedings}}},
  author = {Gordon, Andrew D. and Henzinger, Thomas A. and Nori, Aditya V. and Rajamani, Sriram K.},
  year = {2014},
  month = may,
  series = {{{FOSE}} 2014},
  pages = {167--181},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2593882.2593900},
  urldate = {2023-02-08},
  abstract = {Probabilistic programs are usual functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of variables in a program via observations. Models from diverse application areas such as computer vision, coding theory, cryptographic protocols, biology and reliability analysis can be written as probabilistic programs. Probabilistic inference is the problem of computing an explicit representation of the probability distribution implicitly specified by a probabilistic program. Depending on the application, the desired output from inference may vary---we may want to estimate the expected value of some function f with respect to the distribution, or the mode of the distribution, or simply a set of samples drawn from the distribution. In this paper, we describe connections this research area called ``Probabilistic Programming" has with programming languages and software engineering, and this includes language design, and the static and dynamic analysis of programs. We survey current state of the art and speculate on promising directions for future research.},
  isbn = {978-1-4503-2865-4},
  keywords = {Machine learning,Probabilistic programming,Program analysis},
  file = {/Users/harrison/Zotero/storage/85XIS6AI/Gordon et al. - 2014 - Probabilistic programming.pdf}
}

@inproceedings{gorinovaLiveMultipleRepresentationProbabilistic2016,
  title = {A {{Live}}, {{Multiple-Representation Probabilistic Programming Environment}} for {{Novices}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gorinova, Maria I. and Sarkar, Advait and Blackwell, Alan F. and Syme, Don},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {2533--2537},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858221},
  urldate = {2023-04-20},
  abstract = {We present a live, multiple-representation novice environment for probabilistic programming based on the Infer.NET language. When compared to a text-only editor in a controlled experiment on 16 participants, our system showed a significant reduction in keystrokes during introductory probabilistic programming exercises, and subsequently, a significant improvement in program description and debugging tasks as measured by task time, keystrokes and deletions.},
  isbn = {978-1-4503-3362-7},
  keywords = {computational thinking,development environment,multiple representation,probabilistic programming,visual languages},
  file = {/Users/harrison/Zotero/storage/2KNJS8V8/Gorinova et al. - 2016 - A Live, Multiple-Representation Probabilistic Prog.pdf}
}

@article{greenmanLittleTrickyLogic2022,
  title = {Little {{Tricky Logic}}: {{Misconceptions}} in the {{Understanding}} of {{LTL}}},
  shorttitle = {Little {{Tricky Logic}}},
  author = {Greenman, Ben and Saarinen, Sam and Nelson, Tim and Krishnamurthi, Shriram},
  year = {2022},
  month = oct,
  journal = {The Art, Science, and Engineering of Programming},
  volume = {7},
  number = {2},
  pages = {7},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2023/7/7},
  urldate = {2023-02-19},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/KS8K8Q7W/Greenman et al. - 2022 - Little Tricky Logic Misconceptions in the Underst.pdf}
}

@article{greenUsabilityAnalysisVisual1996,
  title = {Usability {{Analysis}} of {{Visual Programming Environments}}: {{A}} `{{Cognitive Dimensions}}' {{Framework}}},
  shorttitle = {Usability {{Analysis}} of {{Visual Programming Environments}}},
  author = {Green, T. R. G. and Petre, M.},
  year = {1996},
  month = jun,
  journal = {Journal of Visual Languages \& Computing},
  volume = {7},
  number = {2},
  pages = {131--174},
  issn = {1045-926X},
  doi = {10.1006/jvlc.1996.0009},
  urldate = {2024-12-05},
  abstract = {The cognitive dimensions framework is a broad-brush evaluation technique for interactive devices and for non-interactive notations. It sets out a small vocabulary of terms designed to capture the cognitively-relevant aspects of structure, and shows how they can be traded off against each other. The purpose of this paper is to propose the framework as an evaluation technique for visual programming environments. We apply it to two commercially-available dataflow languages (with further examples from other systems) and conclude that it is effective and insightful; other HCI-based evaluation techniques focus on different aspects and would make good complements. Insofar as the examples we used are representative, current VPLs are successful in achieving a good `closeness of match', but designers need to consider the `viscosity ' (resistance to local change) and the `secondary notation' (possibility of conveying extra meaning by choice of layout, colour, etc.).},
  file = {/Users/harrison/Zotero/storage/9Y9ZTVHB/S1045926X96900099.html}
}

@inproceedings{greilerTestConfessionsStudy2012,
  title = {Test Confessions: {{A}} Study of Testing Practices for Plug-in Systems},
  shorttitle = {Test Confessions},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Greiler, Michaela and {van Deursen}, Arie and Storey, Margaret-Anne},
  year = {2012},
  month = jun,
  pages = {244--254},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2012.6227189},
  abstract = {Testing plug-in-based systems is challenging due to complex interactions among many different plug-ins, and variations in version and configuration. The objective of this paper is to increase our understanding of what testers and developers think and do when it comes to testing plug-in-based systems. To that end, we conduct a qualitative (grounded theory) study, in which we interview 25 senior practitioners about how they test plug-in applications based on the Eclipse plug-in architecture. The outcome is an overview of the testing practices currently used, a set of identified barriers limiting test adoption, and an explanation of how limited testing is compensated by self-hosting of projects and by involving the community. These results are supported by a structured survey of more than 150 professionals. The study reveals that unit testing plays a key role, whereas plug-in specific integration problems are identified and resolved by the community. Based on our findings, we propose a series of recommendations and areas for future research.},
  keywords = {Communities,Companies,Computer architecture,Eclipse,Graphical user interfaces,grounded theory,Interviews,Manuals,open source software development,plug-in architectures,Testing},
  file = {/Users/harrison/Zotero/storage/UL5VMT4R/Greiler et al. - 2012 - Test confessions A study of testing practices for.pdf;/Users/harrison/Zotero/storage/DBHL5KKP/6227189.html}
}

@inproceedings{gulzarPerceptionPracticesDifferential2019,
  title = {Perception and {{Practices}} of {{Differential Testing}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}} ({{ICSE-SEIP}})},
  author = {Gulzar, Muhammad Ali and Zhu, Yongkang and Han, Xiaofeng},
  year = {2019},
  month = may,
  pages = {71--80},
  doi = {10.1109/ICSE-SEIP.2019.00016},
  abstract = {Tens of thousands engineers are contributing to Google's codebase that spans billions of lines of code. To ensure high code quality, tremendous amount of effort has been made with new testing techniques and frameworks. However, with increasingly complex data structures and software systems, traditional test case based testing strategies cannot scale well to achieve the desired level of test adequacy. Differential (Diff) is one of the new testing techniques adapted to fill this gap. It uses the same input to run two versions of a software system, namely base and test, where base is the verified/tested version of the system while test is the modified version. The output of two runs are then thoroughly compared to find abnormalities that may lead to possible bugs. Over the past few years, differential testing has been quickly adopted by hundreds of teams across all major product areas at Google. Meanwhile, many new differential testing frameworks were developed to simplify the creation, maintenance, and analysis of diff tests. Curious by this emerging popularity, we conducted the first empirical study on differential testing in practice at large scale. In this study, we investigated common practices and usage of diff tests. We further explore the features of diff tests that users value the most and the pain points of using diff tests. Through this user study, we discovered that differential testing does not replace fine-grained testing techniques such as unit tests. Instead it supplements existing testing suites. It helps users verify the impact on unmodified and unfamiliar components in the absence of a test oracle. In terms of limitations, diff tests often take long time to run and appear to generate noisy and flaky outcomes. Finally, we highlight problems (including smart data differencing, sampling, and traceability) to guide future research in differential testing.},
  keywords = {Computer bugs,Google,Interviews,Maintenance engineering,Software systems,Testing,Testing empirical study differential testing},
  file = {/Users/harrison/Zotero/storage/3ACVD38Y/Gulzar et al. - 2019 - Perception and Practices of Differential Testing.pdf;/Users/harrison/Zotero/storage/BXJITD7W/8804465.html}
}

@inproceedings{hamanaFreeSMonoidsHigherOrder2004,
  title = {Free {{$\Sigma$-Monoids}}: {{A Higher-Order Syntax}} with {{Metavariables}}},
  shorttitle = {Free {{$\Sigma$-Monoids}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Hamana, Makoto},
  editor = {Chin, Wei-Ngan},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {348--363},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30477-7_23},
  abstract = {The notion of {$\Sigma$}-monoids is proposed by Fiore, Plotkin and Turi, to give abstract algebraic model of languages with variable binding and substitutions. In this paper, we give a free construction of {$\Sigma$}-monoids. The free {$\Sigma$}-monoid over a given presheaf serves a well-structured term language involving binding and substitutions. Moreover, the free {$\Sigma$}-monoid naturally contains interesting syntactic objects which can be viewed as ``metavariables'' and ``environments''. We analyse the term language of the free {$\Sigma$}-monoid by relating it with several concrete systems, especially the {$\lambda$}-calculus extended with contexts.},
  isbn = {978-3-540-30477-7},
  langid = {english},
  keywords = {Binding Signature,Construction Rule,Function Symbol,Monoidal Category,Object Variable},
  file = {/Users/harrison/Zotero/storage/TY78KX6E/Hamana - 2004 - Free Σ-Monoids A Higher-Order Syntax with Metavar.pdf}
}

@article{hammerConfusingClaimsData2014,
  title = {Confusing {{Claims}} for {{Data}}: {{A Critique}} of {{Common Practices}} for {{Presenting Qualitative Research}} on {{Learning}}},
  shorttitle = {Confusing {{Claims}} for {{Data}}},
  author = {Hammer, David and Berland, Leema K.},
  year = {2014},
  month = jan,
  journal = {Journal of the Learning Sciences},
  volume = {23},
  number = {1},
  pages = {37--46},
  publisher = {Routledge},
  issn = {1050-8406},
  doi = {10.1080/10508406.2013.802652},
  urldate = {2023-09-12},
  abstract = {We question widely accepted practices of publishing articles that present quantified analyses of qualitative data. First, articles are often published that provide only very brief excerpts of the qualitative data themselves to illustrate the coding scheme, tacitly or explicitly treating the coding results as data. Second, articles are often published that treat interrater reliability solely as a matter of justifying the coding scheme, without further attention to the variance it makes evident in the process of coding. We argue that authors should not treat coding results as data but rather as tabulations of claims about data and that it is important to discuss the rates and substance of disagreements among coders. We propose publication guidelines for authors and reviewers of this form of research.},
  file = {/Users/harrison/Zotero/storage/L8574NLV/Hammer and Berland - 2014 - Confusing Claims for Data A Critique of Common Pr.pdf}
}

@inproceedings{hartiganMosaicsContingencyTables1981,
  title = {Mosaics for {{Contingency Tables}}},
  booktitle = {Computer {{Science}} and {{Statistics}}: {{Proceedings}} of the 13th {{Symposium}} on the {{Interface}}},
  author = {Hartigan, J. A. and Kleiner, B.},
  editor = {Eddy, William F.},
  year = {1981},
  pages = {268--273},
  publisher = {Springer US},
  address = {New York, NY},
  doi = {10.1007/978-1-4613-9464-8_37},
  abstract = {A contingency table specifies the joint distribution of a number of discrete variables. The numbers in a contingency table are represented by rectangles of areas proportional to the numbers, with shape and position chosen to expose deviations from independence models. The collection of rectangles for the contingency table is called a mosaic. Mosaics of various types are given for contingency tables of two and more variables.},
  isbn = {978-1-4613-9464-8},
  langid = {english},
  keywords = {Contingency Tables,Discrete Variables,Graphical Methods,Independence}
}

@inproceedings{hartmannDesignExplorationCreating2008,
  title = {Design as {{Exploration}}: {{Creating Interface Alternatives}} through {{Parallel Authoring}} and {{Runtime Tuning}}},
  booktitle = {{\textbackslash}confuist},
  author = {Hartmann, Bj{\"o}rn and Yu, Loren and Allison, Abel and Yang, Yeonsoo and Klemmer, Scott R.},
  year = {2008},
  pages = {91--100},
  publisher = {{\textbackslash}pubacm}
}

@misc{hatfield-doddsGhostwritingTestsYou2023,
  title = {Ghostwriting Tests for You --- {{Hypothesis}} 6.82.0 Documentation},
  author = {{Hatfield-Dodds}, Zac},
  year = {2023},
  urldate = {2023-07-26},
  howpublished = {https://hypothesis.readthedocs.io/en/latest/ghostwriter.html},
  file = {/Users/harrison/Zotero/storage/W9HFKSNL/ghostwriter.html}
}

@misc{hatfield-doddsHypoFuzz2022,
  title = {{{HypoFuzz}}},
  author = {{Hatfield-Dodds}, Zac},
  year = {2022},
  journal = {HypoFuzz},
  urldate = {2022-12-01},
  howpublished = {https://hypofuzz.com/},
  file = {/Users/harrison/Zotero/storage/CSKNT77U/hypofuzz.com.html}
}

@misc{hatfield-doddsObservabilityToolsHypothesis2024,
  title = {Observability {{Tools}} \& {{Hypothesis}} 6.99.13},
  author = {{Hatfield-Dodds}, Zac},
  year = {2024}
}

@inproceedings{havrikovSystematicallyCoveringInput2019,
  title = {Systematically {{Covering Input Structure}}},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Havrikov, Nikolas and Zeller, Andreas},
  year = {2019},
  month = nov,
  pages = {189--199},
  issn = {2643-1572},
  doi = {10.1109/ASE.2019.00027},
  abstract = {Grammar-based testing uses a given grammar to produce syntactically valid inputs. To cover program features, it is necessary to also cover input features-say, all URL variants for a URL parser. Our k-path algorithm for grammar production systematically covers syntactic elements as well as their combinations. In our evaluation, we show that this results in a significantly higher code coverage than state of the art.},
  file = {/Users/harrison/Zotero/storage/GAHMMNHH/Havrikov and Zeller - 2019 - Systematically Covering Input Structure.pdf;/Users/harrison/Zotero/storage/XLP2DTNF/8952419.html}
}

@article{hazimehMagmaGroundTruthFuzzing2021,
  title = {Magma: {{A Ground-Truth Fuzzing Benchmark}}},
  shorttitle = {Magma},
  author = {Hazimeh, Ahmad and Herrera, Adrian and Payer, Mathias},
  year = {2021},
  month = jun,
  journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume = {4},
  number = {3},
  pages = {49:1--49:29},
  doi = {10.1145/3428334},
  urldate = {2022-12-13},
  abstract = {High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash count---perhaps the most commonly-used performance metric---is inaccurate due to imperfections in deduplication techniques. Additionally, the lack of a unified set of targets results in ad hoc evaluations that hinder fair comparison. We tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. Magma is an open benchmark consisting of seven targets that perform a variety of input manipulations and complex computations, presenting a challenge to state-of-the-art fuzzers. We evaluate seven widely-used mutation-based fuzzers (AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, honggfuzz, and SymCC-AFL) against Magma over 200,000 CPU-hours. Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzers' exploration and detection capabilities. This provides insight into fuzzer performance evaluation, highlighting the importance of ground truth in performing more accurate and meaningful evaluations.},
  keywords = {benchmark,fuzzing,performance evaluation,software security},
  file = {/Users/harrison/Zotero/storage/AJ2ZEM4U/Hazimeh et al. - 2021 - Magma A Ground-Truth Fuzzing Benchmark.pdf}
}

@inproceedings{headWritingReusableCode2017,
  title = {Writing {{Reusable Code Feedback}} at {{Scale}} with {{Mixed-Initiative Program Synthesis}}},
  booktitle = {{\textbackslash}conflas},
  author = {Head, Andrew and Glassman, Elena and Soares, Gustavo and Suzuki, Ryo and Figueredo, Lucas and D'Antoni, Loris and Hartmann, Bj{\"o}rn},
  year = {2017},
  pages = {89--98},
  publisher = {{\textbackslash}pubacm}
}

@techreport{heitmeyerNeedPracticalFormal1998,
  title = {On the {{Need}} for {{Practical Formal Methods}}},
  author = {Heitmeyer, Constance},
  year = {1998},
  month = jan,
  address = {Washington, DC},
  institution = {Naval Research Lab},
  urldate = {2023-06-26},
  abstract = {A controversial issue in the formal methods community is the degree to which mathematical sophistication and theorem proving skills should be needed to apply a formal method. A fundamental assumption of this paper is that formal methods research has produced several classes of analysis that can prove useful in software development. However, to be useful to software practitioners, most of whom lack advanced mathematical training and theorem proving skills, current formal methods need a number of additional attributes, including more user- friendly notations, completely automatic i.e., push button analysis, and useful, easy to understand feedback. Moreover, formal methods need to be integrated into a standard development process. I discuss additional research and engineering that is needed to make the current set of formal methods more practical. To illustrate the ideas, I present several examples, many taken from the SCR Software Cost Reduction requirements method, a formal method that software developers can apply without theorem proving skills, knowledge of temporal and higher order logics, or consultation with formal methods experts.},
  chapter = {Technical Reports},
  langid = {english}
}

@inproceedings{hemmatiHowEffectiveAre2015,
  title = {How Effective Are Code Coverage Criteria?},
  booktitle = {2015 {{IEEE International Conference}} on {{Software Quality}}, {{Reliability}} and {{Security}}},
  author = {Hemmati, Hadi},
  year = {2015},
  pages = {151--156},
  publisher = {IEEE}
}

@inproceedings{hempelSketchnSketchOutputDirectedProgramming2019,
  title = {Sketch-n-{{Sketch}}: {{Output-Directed Programming}} for {{SVG}}},
  shorttitle = {Sketch-n-{{Sketch}}},
  booktitle = {Proceedings of the 32nd {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Hempel, Brian and Lubin, Justin and Chugh, Ravi},
  year = {2019},
  month = oct,
  series = {{{UIST}} '19},
  pages = {281--292},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3332165.3347925},
  urldate = {2022-12-21},
  abstract = {For creative tasks, programmers face a choice: Use a GUI and sacrifice flexibility, or write code and sacrifice ergonomics? To obtain both flexibility and ease of use, a number of systems have explored a workflow that we call output-directed programming. In this paradigm, direct manipulation of the program's graphical output corresponds to writing code in a general-purpose programming language, and edits not possible with the mouse can still be enacted through ordinary text edits to the program. Such capabilities provide hope for integrating graphical user interfaces into what are currently text-centric programming environments. To further advance this vision, we present a variety of new output-directed techniques that extend the expressive power of Sketch-n-Sketch, an output-directed programming system for creating programs that generate vector graphics. To enable output-directed interaction at more stages of program construction, we expose intermediate execution products for manipulation and we present a mechanism for contextual drawing. Looking forward to output-directed programming beyond vector graphics, we also offer generic refactorings through the GUI, and our techniques employ a domain-agnostic provenance tracing scheme. To demonstrate the improved expressiveness, we implement a dozen new parametric designs in Sketch-n-Sketch without text-based edits. Among these is the first demonstration of building a recursive function in an output-directed programming setting.},
  isbn = {978-1-4503-6816-2},
  keywords = {output-directed programming,sketch-n-sketch,svg},
  file = {/Users/harrison/Zotero/storage/JUSK3HWZ/Hempel et al. - 2019 - Sketch-n-Sketch Output-Directed Programming for S.pdf}
}

@article{hoangRandomTestingHigherorder2022,
  title = {Random Testing of a Higher-Order Blockchain Language (Experience Report)},
  author = {Hoang, Tram and Trunov, Anton and Lampropoulos, Leonidas and Sergey, Ilya},
  year = {2022},
  month = aug,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {ICFP},
  pages = {122:886--122:901},
  doi = {10.1145/3547653},
  urldate = {2023-11-28},
  abstract = {We describe our experience of using property-based testing---an approach for automatically generating random inputs to check executable program specifications---in a development of a higher-order smart contract language that powers a state-of-the-art blockchain with thousands of active daily users. We outline the process of integrating QuickChick---a framework for property-based testing built on top of the Coq proof assistant---into a real-world language implementation in OCaml. We discuss the challenges we have encountered when generating well-typed programs for a realistic higher-order smart contract language, which mixes purely functional and imperative computations and features runtime resource accounting. We describe the set of the language implementation properties that we tested, as well as the semantic harness required to enable their validation. The properties range from the standard type safety to the soundness of a control- and type-flow analysis used by the optimizing compiler. Finally, we present the list of bugs discovered and rediscovered with the help of QuickChick and discuss their severity and possible ramifications.},
  keywords = {definitional interpreters,higher-order control-flow analysis,property-based testing,QuickChick,random testing,Scilla,smart contracts},
  file = {/Users/harrison/Zotero/storage/PNPZNJ82/Hoang et al. - 2022 - Random testing of a higher-order blockchain langua.pdf}
}

@inproceedings{hoffswellAugmentingCodeSitu2018,
  title = {Augmenting {{Code}} with {{In Situ Visualizations}} to {{Aid Program Understanding}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hoffswell, Jane and Satyanarayan, Arvind and Heer, Jeffrey},
  year = {2018},
  month = apr,
  series = {{{CHI}} '18},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3173574.3174106},
  urldate = {2024-02-26},
  abstract = {Programmers must draw explicit connections between their code and runtime state to properly assess the correctness of their programs. However, debugging tools often decouple the program state from the source code and require explicitly invoked views to bridge the rift between program editing and program understanding. To unobtrusively reveal runtime behavior during both normal execution and debugging, we contribute techniques for visualizing program variables directly within the source code. We describe a design space and placement criteria for embedded visualizations. We evaluate our in situ visualizations in an editor for the Vega visualization grammar. Compared to a baseline development environment, novice Vega users improve their overall task grade by about 2 points when using the in situ visualizations and exhibit significant positive effects on their self-reported speed and accuracy.},
  isbn = {978-1-4503-5620-6},
  keywords = {code augmentation,debugging,program behavior,program understanding,visualization},
  file = {/Users/harrison/Zotero/storage/3LEYJXKF/Hoffswell et al. - 2018 - Augmenting Code with In Situ Visualizations to Aid.pdf}
}

@inproceedings{hohmanGamutDesignProbe2019,
  title = {Gamut: {{A Design Probe}} to {{Understand How Data Scientists Understand Machine Learning Models}}},
  shorttitle = {Gamut},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hohman, Fred and Head, Andrew and Caruana, Rich and DeLine, Robert and Drucker, Steven M.},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3290605.3300809},
  urldate = {2022-11-22},
  abstract = {Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability. Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools. Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, Gamut, to explore how interactive interfaces could better support model interpretation. Using Gamut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability. Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness. Participants also asked to use Gamut in their work, highlighting its potential to help data scientists understand their own data.},
  isbn = {978-1-4503-5970-2},
  keywords = {data visualization,design probe,interactive interfaces,machine learning interpretability,visual analytics},
  file = {/Users/harrison/Zotero/storage/2WWTJ3RP/Hohman et al. - 2019 - Gamut A Design Probe to Understand How Data Scien.pdf}
}

@inproceedings{hohmanUnderstandingVisualizingData2020,
  title = {Understanding and {{Visualizing Data Iteration}} in {{Machine Learning}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hohman, Fred and Wongsuphasawat, Kanit and Kery, Mary Beth and Patel, Kayur},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376177},
  urldate = {2022-11-21},
  abstract = {Successful machine learning (ML) applications require iterations on both modeling and the underlying data. While prior visualization tools for ML primarily focus on modeling, our interviews with 23 ML practitioners reveal that they improve model performance frequently by iterating on their data (e.g., collecting new data, adding labels) rather than their models. We also identify common types of data iterations and associated analysis tasks and challenges. To help attribute data iterations to model performance, we design a collection of interactive visualizations and integrate them into a prototype, Chameleon, that lets users compare data features, training/testing splits, and performance across data versions. We present two case studies where developers apply {\textbackslash}system to their own evolving datasets on production ML projects. Our interface helps them verify data collection efforts, find failure cases stretching across data versions, capture data processing changes that impacted performance, and identify opportunities for future data iterations.},
  isbn = {978-1-4503-6708-0},
  file = {/Users/harrison/Zotero/storage/U6ECSS6F/Hohman et al. - 2020 - Understanding and Visualizing Data Iteration in Ma.pdf}
}

@inproceedings{hollerFuzzingCodeFragments2012,
  title = {Fuzzing with Code Fragments},
  booktitle = {Proceedings of the 21st {{USENIX}} Conference on {{Security}} Symposium},
  author = {Holler, Christian and Herzig, Kim and Zeller, Andreas},
  year = {2012},
  month = aug,
  series = {Security'12},
  pages = {38},
  publisher = {USENIX Association},
  address = {USA},
  urldate = {2023-02-26},
  abstract = {Fuzz testing is an automated technique providing random data as input to a software system in the hope to expose a vulnerability. In order to be effective, the fuzzed input must be common enough to pass elementary consistency checks; a JavaScript interpreter, for instance, would only accept a semantically valid program. On the other hand, the fuzzed input must be uncommon enough to trigger exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using a grammar to randomly generate valid programs; the code fragments, however, partially stem from programs known to have caused invalid behavior before. LangFuzz is an effective tool for security testing: Applied on the Mozilla JavaScript interpreter, it discovered a total of 105 new severe vulnerabilities within three months of operation (and thus became one of the top security bug bounty collectors within this period); applied on the PHP interpreter, it discovered 18 new defects causing crashes.},
  file = {/Users/harrison/Zotero/storage/AM3D8E84/Holler et al. - Fuzzing with Code Fragments (-2).pdf}
}

@article{holmesTSTLTemplateScripting2018,
  title = {{{TSTL}}: The Template Scripting Testing Language},
  shorttitle = {{{TSTL}}},
  author = {Holmes, Josie and Groce, Alex and Pinto, Jervis and Mittal, Pranjal and Azimi, Pooria and Kellar, Kevin and O'brien, James},
  year = {2018},
  month = feb,
  journal = {International Journal on Software Tools for Technology Transfer (STTT)},
  volume = {20},
  number = {1},
  pages = {57--78},
  issn = {1433-2779},
  doi = {10.1007/s10009-016-0445-y},
  urldate = {2023-11-27},
  abstract = {A test harness, in automated test generation, defines the set of valid tests for a system, as well as their correctness properties. The difficulty of writing test harnesses is a major obstacle to the adoption of automated test generation and model checking. Languages for writing test harnesses are usually tied to a particular tool and unfamiliar to programmers, and often limit expressiveness. Writing test harnesses directly in the language of the software under test (SUT) is a tedious, repetitive, and error-prone task, offers little or no support for test case manipulation and debugging, and produces hard-to-read, hard-to-maintain code. Using existing harness languages or writing directly in the language of the SUT also tends to limit users to one algorithm for test generation, with little ability to explore alternative methods. In this paper, we present TSTL, the template scripting testing language, a domain-specific language (DSL) for writing test harnesses. TSTL compiles harness definitions into an interface for testing, making generic test generation and manipulation tools for all SUTs possible. TSTL includes tools for generating, manipulating, and analyzing test cases, including simple model checkers. This paper motivates TSTL via a large-scale testing effort, directed by an end-user, to find faults in the most widely used geographic information systems tool. This paper emphasizes a new approach to automated testing, where, rather than focus on developing a monolithic tool to extend, the aim is to convert a test harness into a language extension. This approach makes testing not a separate activity to be performed using a tool, but as natural to users of the language of the system under test as is the use of domain-specific libraries such as ArcPy, NumPy, or QIIME, in their domains. TSTL is a language and tool infrastructure, but is also a way to bring testing activities under the control of an existing programming language in a simple, natural way.},
  keywords = {Domain-specific languages,End-user testing,Explicit-state model checking,Geographic information systems,Software testing},
  file = {/Users/harrison/Zotero/storage/B78DDXG7/Holmes et al. - 2018 - TSTL the template scripting testing language.pdf}
}

@article{holmesUsingRelativeLines2020,
  title = {Using {{Relative Lines}} of {{Code}} to {{Guide Automated Test Generation}} for {{Python}}},
  author = {Holmes, Josie and Ahmed, Iftekhar and Brindescu, Caius and Gopinath, Rahul and Zhang, He and Groce, Alex},
  year = {2020},
  month = sep,
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {29},
  number = {4},
  pages = {28:1--28:38},
  issn = {1049-331X},
  doi = {10.1145/3408896},
  urldate = {2023-11-27},
  abstract = {Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation. It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another. However, relative LOC, between components of the same project, turns out to be a highly useful metric for automated testing. In this article, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation. This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead. We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool. In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20\%, up to 40\% or more) and improve fault detection by an even larger margin (usually more than 75\% and up to 400\% or more). The LOC heuristic is also easy to combine with other approaches and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing.},
  keywords = {Automated test generation,static code metrics,testing heuristics},
  file = {/Users/harrison/Zotero/storage/VU98DS4X/Holmes et al. - 2020 - Using Relative Lines of Code to Guide Automated Te.pdf}
}

@misc{holserPholserJunitquickcheck2024,
  title = {Pholser/Junit-Quickcheck},
  author = {Holser, Paul},
  year = {2024},
  month = jan,
  urldate = {2024-02-26},
  abstract = {Property-based testing, JUnit-style},
  copyright = {MIT},
  keywords = {java,junit,property-based-testing,quickcheck}
}

@article{holtzenScalingExactInference2020,
  title = {Scaling {{Exact Inference}} for {{Discrete Probabilistic Programs}}},
  author = {Holtzen, Steven and den Broeck, Guy Van and Millstein, Todd},
  year = {2020},
  month = nov,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  number = {OOPSLA},
  eprint = {2005.09089},
  primaryclass = {cs},
  pages = {1--31},
  issn = {2475-1421},
  doi = {10.1145/3428208},
  urldate = {2024-01-26},
  abstract = {Probabilistic programming languages (PPLs) are an expressive means of representing and reasoning about probabilistic models. The computational challenge of probabilistic inference remains the primary roadblock for applying PPLs in practice. Inference is fundamentally hard, so there is no one-size-fits all solution. In this work, we target scalable inference for an important class of probabilistic programs: those whose probability distributions are discrete. Discrete distributions are common in many fields, including text analysis, network verification, artificial intelligence, and graph analysis, but they prove to be challenging for existing PPLs. We develop a domain-specific probabilistic programming language called Dice that features a new approach to exact discrete probabilistic program inference. Dice exploits program structure in order to factorize inference, enabling us to perform exact inference on probabilistic programs with hundreds of thousands of random variables. Our key technical contribution is a new reduction from discrete probabilistic programs to weighted model counting (WMC). This reduction separates the structure of the distribution from its parameters, enabling logical reasoning tools to exploit that structure for probabilistic inference. We (1) show how to compositionally reduce Dice inference to WMC, (2) prove this compilation correct with respect to a denotational semantics, (3) empirically demonstrate the performance benefits over prior approaches, and (4) analyze the types of structure that allow Dice to scale to large probabilistic programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/harrison/Zotero/storage/9BYR2NIU/Holtzen et al. - 2020 - Scaling Exact Inference for Discrete Probabilistic.pdf;/Users/harrison/Zotero/storage/Z6S3QHXQ/2005.html}
}

@inproceedings{horowitzEngraftAPILive2023,
  title = {Engraft: {{An API}} for {{Live}}, {{Rich}}, and {{Composable Programming}}},
  shorttitle = {Engraft},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Horowitz, Joshua and Heer, Jeffrey},
  year = {2023},
  month = oct,
  series = {{{UIST}} '23},
  pages = {1--18},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3586183.3606733},
  urldate = {2024-01-23},
  abstract = {Live \& rich tools can support a diversity of domain-specific programming tasks, from visualization authoring to data wrangling. Real-world programming, however, requires performing multiple tasks in concert, calling for the use of multiple tools alongside conventional code. Programmers lack environments capable of composing live \& rich tools to support these situations. To enable this composition, we contribute Engraft, a component-based API that allows live \& rich tools to be embedded within larger environments like computational notebooks. Through recursive embedding of components, Engraft enables several new forms of composition: not only embedding tools inside environments, but also embedding environments within each other and embedding tools and environments in the outside world, including conventional codebases. We demonstrate Engraft with examples from diverse domains, including web-application development, command-line scripting, and physics education. By providing composability, Engraft can help cultivate a cycle of use and innovation in live \& rich programming.},
  isbn = {9798400701320},
  keywords = {composition,computational notebooks,end-user programming,GUIs,live programming,visual programming},
  file = {/Users/harrison/Zotero/storage/3M8SCMM7/Horowitz and Heer - 2023 - Engraft An API for Live, Rich, and Composable Pro.pdf}
}

@article{houfavoniaLogarithmProgramTesting2022,
  title = {Logarithm and Program Testing},
  author = {Hou (Favonia), Kuen-Bang and Wang, Zhuyang},
  year = {2022},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {POPL},
  pages = {64:1--64:26},
  doi = {10.1145/3498726},
  urldate = {2022-12-14},
  abstract = {Randomized property-based testing has gained much attention recently, but most frameworks stop short at polymorphic properties. Although Bernardy et al. have developed a theory to reduce a wide range of polymorphic properties to monomorphic ones, it relies upon ad-hoc embedding-projection pairs to massage the types into a particular form. This paper skips the embedding-projection pairs and presents a mechanical monomorphization for a general class of polymorphic functions, a step towards automatic testing for polymorphic properties. The calculation of suitable types for monomorphization turns out to be logarithm.},
  keywords = {logarithm,parametricity,polymorphism},
  file = {/Users/harrison/Zotero/storage/UZ5CKUE7/Hou (Favonia) and Wang - 2022 - Logarithm and program testing.pdf}
}

@misc{HowWeBuilt2023,
  title = {How We Built {{Cedar}} with Automated Reasoning and Differential Testing},
  year = {2023},
  month = may,
  journal = {Amazon Science},
  urldate = {2025-01-19},
  abstract = {The new development process behind Amazon Web Services' Cedar authorization-policy language.},
  howpublished = {https://www.amazon.science/blog/how-we-built-cedar-with-automated-reasoning-and-differential-testing},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/BNZHJPDG/how-we-built-cedar-with-automated-reasoning-and-differential-testing.html}
}

@inproceedings{huangReconcilingEnumerativeDeductive2020,
  title = {Reconciling Enumerative and Deductive Program Synthesis},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Huang, Kangjing and Qiu, Xiaokang and Shen, Peiyuan and Wang, Yanjun},
  year = {2020},
  month = jun,
  series = {{{PLDI}} 2020},
  pages = {1159--1174},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3385412.3386027},
  urldate = {2023-02-16},
  abstract = {Syntax-guided synthesis (SyGuS) aims to find a program satisfying semantic specification as well as user-provided structural hypotheses. There are two main synthesis approaches: enumerative synthesis, which repeatedly enumerates possible candidate programs and checks their correctness, and deductive synthesis, which leverages a symbolic procedure to construct implementations from specifications. Neither approach is strictly better than the other: automated deductive synthesis is usually very efficient but only works for special grammars or applications; enumerative synthesis is very generally applicable but limited in scalability. In this paper, we propose a cooperative synthesis technique for SyGuS problems with the conditional linear integer arithmetic (CLIA) background theory, as a novel integration of the two approaches, combining the best of the two worlds. The technique exploits several novel divide-and-conquer strategies to split a large synthesis problem to smaller subproblems. The subproblems are solved separately and their solutions are combined to form a final solution. The technique integrates two synthesis engines: a pure deductive component that can efficiently solve some problems, and a height-based enumeration algorithm that can handle arbitrary grammar. We implemented the cooperative synthesis technique, and evaluated it on a wide range of benchmarks. Experiments showed that our technique can solve many challenging synthesis problems not possible before, and tends to be more scalable than state-of-the-art synthesis algorithms.},
  isbn = {978-1-4503-7613-6},
  keywords = {deductive synthesis,divide-and-conquer,enumerative synthesis,syntax-guided synthesis},
  file = {/Users/harrison/Zotero/storage/3R6LNDQD/Huang et al. - 2020 - Reconciling enumerative and deductive program synt.pdf}
}

@article{hudakBuildingDomainspecificEmbedded1996,
  title = {Building Domain-Specific Embedded Languages},
  author = {Hudak, Paul},
  year = {1996},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {28},
  number = {4es},
  pages = {196},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/242224.242477},
  urldate = {2024-11-13},
  langid = {english}
}

@misc{hughesBuildingDevelopersIntuitions,
  title = {Building on {{Developers}}' {{Intuitions}} to {{Create Effective Property-Based Tests}}},
  author = {Hughes, John},
  journal = {InfoQ},
  urldate = {2023-01-16},
  abstract = {John Hughes shows how to build properties for testing, and also new features in Haskell QuickCheck that support them.},
  howpublished = {https://www.infoq.com/presentations/property-based-testing-haskell/},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/W85L68G8/property-based-testing-haskell.html}
}

@incollection{hughesExperiencesQuickCheckTesting2016,
  title = {Experiences with {{QuickCheck}}: {{Testing}} the {{Hard Stuff}} and {{Staying Sane}}},
  shorttitle = {Experiences with {{QuickCheck}}},
  booktitle = {A {{List}} of {{Successes That Can Change}} the {{World}}: {{Essays Dedicated}} to {{Philip Wadler}} on the {{Occasion}} of {{His}} 60th {{Birthday}}},
  author = {Hughes, John},
  editor = {Lindley, Sam and McBride, Conor and Trinder, Phil and Sannella, Don},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {169--186},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-30936-1_9},
  urldate = {2022-12-04},
  abstract = {This is not a typical scientific paper. It does not present a new method, with careful experiments to evaluate it, and detailed references to related work. Rather, it recounts some of my experiences over the last 15 years, working with QuickCheck, and its purpose is as much to entertain as to inform.},
  isbn = {978-3-319-30936-1},
  langid = {english},
  keywords = {Controller Area Network,Fail Test Case,Input Index,Race Condition,State Transition Function},
  file = {/Users/harrison/Zotero/storage/P4CR3FFW/Hughes - 2016 - Experiences with QuickCheck Testing the Hard Stuf.pdf}
}

@inproceedings{hughesHowSpecifyIt2019,
  title = {How to {{Specify It}}!},
  booktitle = {20th {{International Symposium}} on {{Trends}} in {{Functional Programming}}},
  author = {Hughes, John},
  year = {2019},
  doi = {10.1007/978-3-030-47147-7_4},
  file = {/Users/harrison/Zotero/storage/E543Q86R/Hughes - 2020 - How to Specify It!.pdf}
}

@inproceedings{hughesMysteriesDropBoxPropertyBased2016,
  title = {Mysteries of {{DropBox}}: {{Property-Based Testing}} of a {{Distributed Synchronization Service}}},
  shorttitle = {Mysteries of {{DropBox}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Hughes, John and Pierce, Benjamin C. and Arts, Thomas and Norell, Ulf},
  year = {2016},
  month = apr,
  pages = {135--145},
  doi = {10.1109/ICST.2016.37},
  abstract = {File synchronization services such as Dropbox are used by hundreds ofmillions of people to replicate vital data. Yet rigorous models of theirbehavior are lacking. We present the first formal -- and testable -- model ofthe core behavior of a modern file synchronizer, and we use it to discoversurprising behavior in two widely deployed synchronizers. Our model isbased on a technique for testing nondeterministic systems that avoidsrequiring that the system's internal choices be made visible to the testing framework.},
  keywords = {Cloud computing,Context modeling,Dropbox,File synchronization,Google,Property-based testing,QuickCheck,Servers,Synchronization,Testing,Virtual machining},
  file = {/Users/harrison/Zotero/storage/URSJNAIU/Hughes et al. - 2016 - Mysteries of DropBox Property-Based Testing of a .pdf;/Users/harrison/Zotero/storage/XYP9ZCH9/7515466.html}
}

@inproceedings{hughesQuickCheckTestingFun2007,
  title = {{{QuickCheck Testing}} for {{Fun}} and {{Profit}}},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  author = {Hughes, John},
  editor = {Hanus, Michael},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--32},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-69611-7_1},
  abstract = {One of the nice things about purely functional languages is that functions often satisfy simple properties, and enjoy simple algebraic relationships. Indeed, if the functions of an API satisfy elegant laws, that in itself is a sign of a good design---the laws not only indicate conceptual simplicity, but are useful in practice for simplifying programs that use the API, by equational reasoning or otherwise.},
  isbn = {978-3-540-69611-7},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/XVTJTA55/Hughes - 2007 - QuickCheck Testing for Fun and Profit.pdf}
}

@misc{icfpconference[@icfp_conference]FunctionalProgrammingHas2023,
  type = {Tweet},
  title = {Functional Programming Has Won. {{You}} Can't Fight It, but You Can Help Making It Even More Awesome by Supporting Basic {{PL}} Research. {{Here}}'s Your Chance to Do It and Also Put Your Company on the Radar of the World's Best Functional Programmers: {{https://icfp23.sigplan.org/attending/call-for-sponsorship}}},
  author = {{ICFP Conference [@icfp\_conference]}},
  year = {2023},
  month = jan,
  journal = {Twitter},
  urldate = {2024-12-12},
  langid = {english}
}

@inproceedings{inozemtsevaCoverageNotStrongly2014,
  title = {Coverage Is Not Strongly Correlated with Test Suite Effectiveness},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  author = {Inozemtseva, Laura and Holmes, Reid},
  year = {2014},
  month = may,
  series = {{{ICSE}} 2014},
  pages = {435--445},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2568225.2568271},
  urldate = {2024-04-20},
  abstract = {The coverage of a test suite is often used as a proxy for its ability to detect faults. However, previous studies that investigated the correlation between code coverage and test suite effectiveness have failed to reach a consensus about the nature and strength of the relationship between these test suite characteristics. Moreover, many of the studies were done with small or synthetic programs, making it unclear whether their results generalize to larger programs, and some of the studies did not account for the confounding influence of test suite size. In addition, most of the studies were done with adequate suites, which are are rare in practice, so the results may not generalize to typical test suites. We have extended these studies by evaluating the relationship between test suite size, coverage, and effectiveness for large Java programs. Our study is the largest to date in the literature: we generated 31,000 test suites for five systems consisting of up to 724,000 lines of source code. We measured the statement coverage, decision coverage, and modified condition coverage of these suites and used mutation testing to evaluate their fault detection effectiveness. We found that there is a low to moderate correlation between coverage and effectiveness when the number of test cases in the suite is controlled for. In addition, we found that stronger forms of coverage do not provide greater insight into the effectiveness of the suite. Our results suggest that coverage, while useful for identifying under-tested parts of a program, should not be used as a quality target because it is not a good indicator of test suite effectiveness.},
  isbn = {978-1-4503-2756-5},
  keywords = {Coverage,test suite effectiveness,test suite quality},
  file = {/Users/harrison/Zotero/storage/EZQJ25LC/Inozemtseva and Holmes - 2014 - Coverage is not strongly correlated with test suit.pdf}
}

@article{jamesDiggingFoldSynthesisaided2020,
  title = {Digging for Fold: Synthesis-Aided {{API}} Discovery for {{Haskell}}},
  shorttitle = {Digging for Fold},
  author = {James, Michael B. and Guo, Zheng and Wang, Ziteng and Doshi, Shivani and Peleg, Hila and Jhala, Ranjit and Polikarpova, Nadia},
  year = {2020},
  month = nov,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  number = {OOPSLA},
  pages = {205:1--205:27},
  doi = {10.1145/3428273},
  urldate = {2023-01-27},
  abstract = {We present Hoogle+, a web-based API discovery tool for Haskell. A Hoogle+ user can specify a programming task using either a type, a set of input-output tests, or both. Given a specification, the tool returns a list of matching programs composed from functions in popular Haskell libraries, and annotated with automatically-generated examples of their behavior. These features of Hoogle+ are powered by three novel techniques. First, to enable efficient type-directed synthesis from tests only, we develop an algorithm that infers likely type specifications from tests. Second, to return high-quality programs even with ambiguous specifications, we develop a technique that automatically eliminates meaningless and repetitive synthesis results. Finally, we show how to extend this elimination technique to automatically generate informative inputs that can be used to demonstrate program behavior to the user. To evaluate the effectiveness of Hoogle+ compared with traditional API search techniques, we perform a user study with 30 participants of varying Haskell proficiency. The study shows that programmers equipped with Hoogle+ generally solve tasks faster and were able to solve 50\% more tasks overall.},
  keywords = {Human-Computer Interaction,Program Synthesis,Type Inference},
  file = {/Users/harrison/Zotero/storage/PR545NX5/James et al. - 2020 - Digging for fold synthesis-aided API discovery fo.pdf}
}

@misc{jan29MechanizedProofsPL2020,
  title = {Mechanized {{Proofs}} for {{PL}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Mechanized {{Proofs}} for {{PL}}},
  author = {on Jan 29, Talia Ringer and {2020}},
  year = {2020},
  month = jan,
  journal = {SIGPLAN Blog},
  urldate = {2024-07-08},
  abstract = {The history of machine-checked proofs about programming languages offers valuable lessons for the future of programming languages research.},
  langid = {american},
  file = {/Users/harrison/Zotero/storage/5P6Y4XYA/mechanized-proofs-for-pl-past-present-and-future.html}
}

@misc{jetbrainsPythonDevelopersSurvey2021,
  title = {Python {{Developers Survey}} 2021 {{Results}}},
  author = {{JetBrains}},
  year = {2021},
  journal = {JetBrains: Developer Tools for Professionals and Teams},
  urldate = {2023-02-19},
  abstract = {Official Python Developers Survey 2021 Results by Python Software Foundation and JetBrains: more than 23k responses from more than 200 countries.},
  langid = {english}
}

@article{jiSurveyHallucinationNatural2023,
  title = {Survey of Hallucination in Natural Language Generation},
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  year = {2023},
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {12},
  pages = {1--38},
  publisher = {ACM New York, NY}
}

@misc{joeyespoPytestwatch2024,
  title = {Pytest-Watch},
  author = {{joeyespo}},
  year = {2024}
}

@inproceedings{johnsonWhyDonSoftware2013,
  title = {Why Don't Software Developers Use Static Analysis Tools to Find Bugs?},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Johnson, Brittany and Song, Yoonki and {Murphy-Hill}, Emerson and Bowdidge, Robert},
  year = {2013},
  month = may,
  pages = {672--681},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2013.6606613},
  abstract = {Using static analysis tools for automating code inspections can be beneficial for software engineers. Such tools can make finding bugs, or software defects, faster and cheaper than manual inspections. Despite the benefits of using static analysis tools to find bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneficial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers fix defects.},
  keywords = {Companies,Computer bugs,Encoding,Interviews,Software,Standards,Teamwork},
  file = {/Users/harrison/Zotero/storage/Y6PJ63KE/Johnson et al. - 2013 - Why don't software developers use static analysis .pdf;/Users/harrison/Zotero/storage/6PE3TZYK/6606613.html}
}

@inproceedings{jonesEmpiricalEvaluationTarantula2005,
  title = {Empirical Evaluation of the Tarantula Automatic Fault-Localization Technique},
  booktitle = {Proceedings of the 20th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Jones, James A. and Harrold, Mary Jean},
  year = {2005},
  month = nov,
  series = {{{ASE}} '05},
  pages = {273--282},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1101908.1101949},
  urldate = {2024-02-05},
  abstract = {The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied, describes our experiment, and reports and discusses the results. Our studies show that, on the same set of subjects, the Tarantula technique consistently outperforms the other four techniques in terms of effectiveness in fault localization, and is comparable in efficiency to the least expensive of the other four techniques.},
  isbn = {978-1-58113-993-8},
  keywords = {automated debugging,empirical study,fault localization,program analysis},
  file = {/Users/harrison/Zotero/storage/L6TMIIJ7/Jones and Harrold - 2005 - Empirical evaluation of the tarantula automatic fa.pdf}
}

@article{jungIrisGroundModular2018,
  title = {Iris from the Ground up: {{A}} Modular Foundation for Higher-Order Concurrent Separation Logic},
  shorttitle = {Iris from the Ground Up},
  author = {Jung, Ralf and Krebbers, Robbert and Jourdan, Jacques-Henri and Bizjak, Ale{\v s} and Birkedal, Lars and Dreyer, Derek},
  year = {2018/ed},
  journal = {Journal of Functional Programming},
  volume = {28},
  pages = {e20},
  publisher = {Cambridge University Press},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796818000151},
  urldate = {2023-02-21},
  abstract = {Iris is a framework for higher-order concurrent separation logic, which has been implemented in the Coq proof assistant and deployed very effectively in a wide variety of verification projects. Iris was designed with the express goal of simplifying and consolidating the foundations of modern separation logics, but it has evolved over time, and the design and semantic foundations of Iris itself have yet to be fully written down and explained together properly in one place. Here, we attempt to fill this gap, presenting a reasonably complete picture of the latest version of Iris (version 3.1), from first principles and in one coherent narrative.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/84LI6XHS/Jung et al. - 2018 - Iris from the ground up A modular foundation for .pdf}
}

@inproceedings{junTisaneAuthoringStatistical2022,
  title = {Tisane: {{Authoring Statistical Models}} via {{Formal Reasoning}} from {{Conceptual}} and {{Data Relationships}}},
  shorttitle = {Tisane},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jun, Eunice and Seo, Audrey and Heer, Jeffrey and Just, Ren{\'e}},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3491102.3501888},
  urldate = {2024-03-16},
  abstract = {Proper statistical modeling incorporates domain theory about how concepts relate and details of how data were measured. However, data analysts currently lack tool support for recording and reasoning about domain assumptions, data collection, and modeling choices in an integrated manner, leading to mistakes that can compromise scientific validity. For instance, generalized linear mixed-effects models (GLMMs) help answer complex research questions, but omitting random effects impairs the generalizability of results. To address this need, we present Tisane, a mixed-initiative system for authoring generalized linear models with and without mixed-effects. Tisane introduces a study design specification language for expressing and asking questions about relationships between variables. Tisane contributes an interactive compilation process that represents relationships in a graph, infers candidate statistical models, and asks follow-up questions to disambiguate user queries to construct a valid model. In case studies with three researchers, we find that Tisane helps them focus on their goals and assumptions while avoiding past mistakes.},
  isbn = {978-1-4503-9157-3},
  keywords = {domain-specific language,end-user elicitation,end-user programming,linear modeling,statistical analysis,transparent statistics,validity},
  file = {/Users/harrison/Zotero/storage/6VI75DLM/Jun et al. - 2022 - Tisane Authoring Statistical Models via Formal Re.pdf}
}

@inproceedings{justAreMutantsValid2014,
  title = {Are Mutants a Valid Substitute for Real Faults in Software Testing?},
  booktitle = {Proceedings of the 22nd {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Just, Ren{\'e} and Jalali, Darioush and Inozemtseva, Laura and Ernst, Michael D and Holmes, Reid and Fraser, Gordon},
  year = {2014},
  pages = {654--665}
}

@inproceedings{kangOmnicodeNoviceOrientedLive2017,
  title = {Omnicode: {{A Novice-Oriented Live Programming Environment}} with {{Always-On Run-Time Value Visualizations}}},
  shorttitle = {Omnicode},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Kang, Hyeonsu and Guo, Philip J.},
  year = {2017},
  month = oct,
  series = {{{UIST}} '17},
  pages = {737--745},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3126594.3126632},
  urldate = {2022-12-21},
  abstract = {Visualizations of run-time program state help novices form proper mental models and debug their code. We push this technique to the extreme by posing the following question: What if a live programming environment for an imperative language always displays the entire history of all run-time values for all program variables all the time? To explore this question, we built a prototype live IDE called Omnicode ("Omniscient Code") that continually runs the user's Python code and uses a scatterplot matrix to visualize the entire history of all of its numerical values, along with meaningful numbers derived from other data types. To filter the visualizations and hone in on specific points of interest, the user can brush and link over the scatterplots or select portions of code. They can also zoom in to view detailed stack and heap visualizations at each execution step. An exploratory study on 10 novice programmers discovered that they found Omnicode to be useful for debugging, forming mental models, explaining their code to others, and discovering moments of serendipity that would not have been likely within an ordinary IDE.},
  isbn = {978-1-4503-4981-9},
  keywords = {always-on visualizations,live programming},
  file = {/Users/harrison/Zotero/storage/RBQELB2F/Kang and Guo - 2017 - Omnicode A Novice-Oriented Live Programming Envir.pdf}
}

@inproceedings{karamTestingMethodologyDataflow2001,
  title = {A Testing Methodology for a Dataflow Based Visual Programming Language},
  booktitle = {Proceedings {{IEEE Symposia}} on {{Human-Centric Computing Languages}} and {{Environments}} ({{Cat}}. {{No}}. {{01TH8587}})},
  author = {Karam, Marcel R and Smedley, Trevor J},
  year = {2001},
  pages = {280--287},
  publisher = {IEEE}
}

@inproceedings{karkerRandomSamplingMatroids1993,
  title = {Random Sampling in Matroids, with Applications to Graph Connectivity and Minimum Spanning Trees},
  booktitle = {Proceedings of 1993 {{IEEE}} 34th {{Annual Foundations}} of {{Computer Science}}},
  author = {Karker, D.R.},
  year = {1993},
  month = nov,
  pages = {84--93},
  doi = {10.1109/SFCS.1993.366879},
  urldate = {2024-07-18},
  abstract = {Random sampling is a powerful way to gather information about a group by considering only a small part of it. We give a paradigm for applying this technique to optimization problems, and demonstrate its effectiveness on matroids. Matroids abstractly model many optimization problems that can be solved by greedy methods, such as the minimum spanning tree (MST) problem. Our results have several applications. We give an algorithm that uses simple data structures to construct an MST in O(m+n log n) time. We give bounds on the connectivity (minimum cut) of a graph suffering random edge failures. We give fast algorithms for packing matroid bases, with particular attention to packing spanning trees in graphs.{$<>$}},
  keywords = {Application software,Computer science,Data analysis,Data structures,Graphics,Greedy algorithms,Optimization methods,Sampling methods,Statistics,Tree graphs},
  file = {/Users/harrison/Zotero/storage/SRZNEGWK/Karker - 1993 - Random sampling in matroids, with applications to .pdf;/Users/harrison/Zotero/storage/RAHSVNVG/366879.html}
}

@article{kennedyFunctionalPearlPickler2004,
  title = {Functional {{Pearl}}: {{Pickler Combinators}}},
  author = {Kennedy, Andrew},
  year = {2004},
  month = jan,
  journal = {Journal of Functional Programming},
  edition = {Journal of Functional Programming},
  volume = {14},
  pages = {727--739},
  publisher = {CUP},
  abstract = {The tedium of writing pickling and unpickling functions by hand is relieved using a combinator library similar in spirit to the well-known parser combinators. Picklers for primitive types are combined to support tupling, alternation, recursion, and structure sharing. Code is presented in Haskell; an alternative implementation in ML is discussed.},
  file = {/Users/harrison/Zotero/storage/2HXI3LVE/Kennedy - 2004 - Functional Pearl Pickler Combinators.pdf}
}

@inproceedings{keryMageFluidMoves2020,
  title = {Mage: {{Fluid Moves Between Code}} and {{Graphical Work}} in {{Computational Notebooks}}},
  shorttitle = {Mage},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Kery, Mary Beth and Ren, Donghao and Hohman, Fred and Moritz, Dominik and Wongsuphasawat, Kanit and Patel, Kayur},
  year = {2020},
  month = oct,
  series = {{{UIST}} '20},
  pages = {140--151},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3379337.3415842},
  urldate = {2022-12-21},
  abstract = {We aim to increase the flexibility at which a data worker can choose the right tool for the job, regardless of whether the tool is a code library or an interactive graphical user interface (GUI). To achieve this flexibility, we extend computational notebooks with a new API mage, which supports tools that can represent themselves as both code and GUI as needed. We discuss the design of mage as well as design opportunities in the space of flexible code/GUI tools for data work. To understand tooling needs, we conduct a study with nine professional practitioners and elicit their feedback on mage and potential areas for flexible code/GUI tooling. We then implement six client tools for mage that illustrate the main themes of our study findings. Finally, we discuss open challenges in providing flexible code/GUI interactions for data workers.},
  isbn = {978-1-4503-7514-6},
  keywords = {computational notebooks,data science programming,handoff,machine learning programming},
  file = {/Users/harrison/Zotero/storage/DGP5B8MB/Kery et al. - 2020 - mage Fluid Moves Between Code and Graphical Work .pdf}
}

@article{kidneyAlgebrasWeightedSearch2021,
  title = {Algebras for Weighted Search},
  author = {Kidney, Donnacha Ois{\'i}n and Wu, Nicolas},
  year = {2021},
  month = aug,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {5},
  number = {ICFP},
  pages = {1--30},
  publisher = {Association for Computing Machinery (ACM)},
  issn = {2475-1421},
  doi = {10.1145/3473577},
  urldate = {2024-09-17},
  abstract = {Weighted search is an essential component of many fundamental and useful algorithms. Despite this, it is relatively under explored as a computational effect, receiving not nearly as much attention as either depth- or breadth-first search. This paper explores the algebraic underpinning of weighted search, and demonstrates how to implement it as a monad transformer.          The development first explores breadth-first search, which can be expressed as a polynomial over semirings. These polynomials are generalised to the free semimodule monad to capture a wide range of applications, including probability monads, polynomial monads, and monads for weighted search. Finally, a monad transformer based on the free semimodule monad is introduced. Applying optimisations to this type yields an implementation of pairing heaps, which is then used to implement Dijkstra's algorithm and efficient probabilistic sampling. The construction is formalised in Cubical Agda and implemented in Haskell.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/GMB2SGHP/Kidney and Wu - 2021 - Algebras for weighted search.pdf}
}

@article{kingSymbolicExecutionProgram1976,
  title = {Symbolic Execution and Program Testing},
  author = {King, James C.},
  year = {1976},
  month = jul,
  journal = {Commun. ACM},
  volume = {19},
  number = {7},
  pages = {385--394},
  issn = {0001-0782},
  doi = {10.1145/360248.360252},
  urldate = {2024-10-04},
  abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
  file = {/Users/harrison/Zotero/storage/N54MIMNR/King - 1976 - Symbolic execution and program testing.pdf}
}

@article{kiselyovBacktrackingInterleavingTerminating2005,
  title = {Backtracking, Interleaving, and Terminating Monad Transformers: (Functional Pearl)},
  shorttitle = {Backtracking, Interleaving, and Terminating Monad Transformers},
  author = {Kiselyov, Oleg and Shan, Chung-chieh and Friedman, Daniel P. and Sabry, Amr},
  year = {2005},
  month = sep,
  journal = {ACM SIGPLAN Notices},
  volume = {40},
  number = {9},
  pages = {192--203},
  issn = {0362-1340},
  doi = {10.1145/1090189.1086390},
  urldate = {2023-02-06},
  abstract = {We design and implement a library for adding backtracking computations to any Haskell monad. Inspired by logic programming, our library provides, in addition to the operations required by the MonadPlus interface, constructs for fair disjunctions, fair conjunctions, conditionals, pruning, and an expressive top-level interface. Implementing these additional constructs is easy in models of backtracking based on streams, but not known to be possible in continuation-based models. We show that all these additional constructs can be generically and monadically realized using a single primitive msplit. We present two implementations of the library: one using success and failure continuations; and the other using control operators for manipulating delimited continuations.},
  keywords = {continuations,control delimiters,Haskell,logic programming,Prolog,streams},
  file = {/Users/harrison/Zotero/storage/4QWV88QV/Kiselyov et al. - 2005 - Backtracking, interleaving, and terminating monad .pdf}
}

@article{kiselyovExtensibleEffectsAlternative2013,
  title = {Extensible {{Effects}}: {{An Alternative}} to {{Monad Transformers}}},
  author = {Kiselyov, Oleg and Sabry, Amr and Swords, Cameron},
  year = {2013},
  month = sep,
  journal = {SIGPLAN Not.},
  volume = {48},
  number = {12},
  pages = {59--70},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0362-1340},
  doi = {10.1145/2578854.2503791},
  abstract = {We design and implement a library that solves the long-standing problem of combining effects without imposing restrictions on their interactions (such as static ordering). Effects arise from interactions between a client and an effect handler (interpreter); interactions may vary throughout the program and dynamically adapt to execution conditions. Existing code that relies on monad transformers may be used with our library with minor changes, gaining efficiency over long monad stacks. In addition, our library has greater expressiveness, allowing for practical idioms that are inefficient, cumbersome, or outright impossible with monad transformers.Our alternative to a monad transformer stack is a single monad, for the coroutine-like communication of a client with its handler. Its type reflects possible requests, i.e., possible effects of a computation. To support arbitrary effects and their combinations, requests are values of an extensible union type, which allows adding and, notably, subtracting summands. Extending and, upon handling, shrinking of the union of possible requests is reflected in its type, yielding a type-and-effect system for Haskell. The library is lightweight, generalizing the extensible exception handling to other effects and accurately tracking them in types.},
  keywords = {coroutine,effect handler,effect interaction,monad,monad transformer,open union,type and effect system},
  file = {/Users/harrison/Zotero/storage/GQRD4973/Kiselyov et al. - 2013 - Extensible effects an alternative to monad transf.pdf}
}

@article{kiselyovFreerMonadsMore2015,
  title = {Freer Monads, More Extensible Effects},
  author = {Kiselyov, Oleg and Ishii, Hiromi},
  year = {2015},
  journal = {ACM SIGPLAN Notices},
  volume = {50},
  number = {12},
  pages = {94--105},
  publisher = {ACM New York, NY, USA},
  file = {/Users/harrison/Zotero/storage/JINXR9QL/Kiselyov and Ishii - 2015 - Freer monads, more extensible effects.pdf}
}

@incollection{kiselyovTypedTaglessFinal2012,
  title = {Typed {{Tagless Final Interpreters}}},
  booktitle = {Generic and {{Indexed Programming}}: {{International Spring School}}, {{SSGIP}} 2010, {{Oxford}}, {{UK}}, {{March}} 22-26, 2010, {{Revised Lectures}}},
  author = {Kiselyov, Oleg},
  editor = {Gibbons, Jeremy},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {130--174},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-32202-0_3},
  urldate = {2022-11-22},
  abstract = {The so-called `typed tagless final' approach of [6] has collected and polished a number of techniques for representing typed higher-order languages in a typed metalanguage, along with type-preserving interpretation, compilation and partial evaluation. The approach is an alternative to the traditional, or `initial' encoding of an object language as a (generalized) algebraic data type. Both approaches permit multiple interpretations of an expression, to evaluate it, pretty-print, etc. The final encoding represents all and only typed object terms without resorting to generalized algebraic data types, dependent or other fancy types. The final encoding lets us add new language forms and interpretations without breaking the existing terms and interpreters.},
  isbn = {978-3-642-32202-0},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/SP4B9DH7/Kiselyov - 2012 - Typed Tagless Final Interpreters.pdf}
}

@inproceedings{kleinSeL4FormalVerification2009,
  title = {{{seL4}}: Formal Verification of an {{OS}} Kernel},
  shorttitle = {{{seL4}}},
  booktitle = {Proceedings of the {{ACM SIGOPS}} 22nd Symposium on {{Operating}} Systems Principles},
  author = {Klein, Gerwin and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal and Norrish, Michael and Sewell, Thomas and Tuch, Harvey and Winwood, Simon},
  year = {2009},
  month = oct,
  series = {{{SOSP}} '09},
  pages = {207--220},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1629575.1629596},
  urldate = {2024-07-08},
  abstract = {Complete formal verification is the only known way to guarantee that a system is free of programming errors.We present our experience in performing the formal, machine-checked verification of the seL4 microkernel from an abstract specification down to its C implementation. We assume correctness of compiler, assembly code, and hardware, and we used a unique design approach that fuses formal and operating systems techniques. To our knowledge, this is the first formal proof of functional correctness of a complete, general-purpose operating-system kernel. Functional correctness means here that the implementation always strictly follows our high-level abstract specification of kernel behaviour. This encompasses traditional design and implementation safety properties such as the kernel will never crash, and it will never perform an unsafe operation. It also proves much more: we can predict precisely how the kernel will behave in every possible situation.seL4, a third-generation microkernel of L4 provenance, comprises 8,700 lines of C code and 600 lines of assembler. Its performance is comparable to other high-performance L4 kernels.},
  isbn = {978-1-60558-752-3}
}

@misc{kmettControlLens2024,
  title = {Control.{{Lens}}},
  author = {Kmett, Ed},
  year = {2024},
  urldate = {2024-04-30},
  howpublished = {https://hackage.haskell.org/package/lens-5.2.3/docs/Control-Lens.html},
  file = {/Users/harrison/Zotero/storage/ECLM5RPN/Control-Lens.html}
}

@misc{kmettFreeHaskellPackage2023,
  title = {Free: {{Haskell Package}}},
  author = {Kmett, Ed},
  year = {2023},
  journal = {Hackage},
  urldate = {2023-07-13},
  abstract = {Monads for free},
  howpublished = {//hackage.haskell.org/package/free},
  file = {/Users/harrison/Zotero/storage/HTWQFYZH/free.html}
}

@article{kochharCodeCoveragePostrelease2017,
  title = {Code {{Coverage}} and {{Postrelease Defects}}: {{A Large-Scale Study}} on {{Open Source Projects}}},
  shorttitle = {Code {{Coverage}} and {{Postrelease Defects}}},
  author = {Kochhar, Pavneet Singh and Lo, David and Lawall, Julia and Nagappan, Nachiappan},
  year = {2017},
  month = dec,
  journal = {IEEE Transactions on Reliability},
  volume = {66},
  number = {4},
  pages = {1213--1228},
  issn = {1558-1721},
  doi = {10.1109/TR.2017.2727062},
  urldate = {2024-02-27},
  abstract = {Testing is a pivotal activity in ensuring the quality of software. Code coverage is a common metric used as a yardstick to measure the efficacy and adequacy of testing. However, does higher coverage actually lead to a decline in postrelease bugs? Do files that have higher test coverage actually have fewer bug reports? The direct relationship between code coverage and actual bug reports has not yet been analyzed via a comprehensive empirical study on real bugs. Past studies only involve a few software systems or artificially injected bugs (mutants). In this empirical study, we examine these questions in the context of open-source software projects based on their actual reported bugs. We analyze 100 large open-source Java projects and measure the code coverage of the test cases that come along with these projects. We collect real bugs logged in the issue tracking system after the release of the software and analyze the correlations between code coverage and these bugs. We also collect other metrics such as cyclomatic complexity and lines of code, which are used to normalize the number of bugs and coverage to correlate with other metrics as well as use these metrics in regression analysis. Our results show that coverage has an insignificant correlation with the number of bugs that are found after the release of the software at the project level, and no such correlation at the file level.},
  keywords = {Code coverage,Computer bugs,empirical study,Open source software,open-source,postrelease defects,software testing,Software testing,Sonar measurements},
  file = {/Users/harrison/Zotero/storage/W9SJTHIY/Kochhar et al. - 2017 - Code Coverage and Postrelease Defects A Large-Sca.pdf;/Users/harrison/Zotero/storage/3GVKEJER/8031982.html}
}

@inproceedings{kochharPractitionersViewsGood2019,
  title = {Practitioners' Views on Good Software Testing Practices},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}} ({{ICSE-SEIP}})},
  author = {Kochhar, Pavneet Singh and Xia, Xin and Lo, David},
  year = {2019},
  pages = {61--70},
  publisher = {IEEE}
}

@inproceedings{koDesigningWhylineDebugging2004,
  title = {Designing the Whyline: A Debugging Interface for Asking Questions about Program Behavior},
  shorttitle = {Designing the Whyline},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Ko, Amy J. and Myers, Brad A.},
  year = {2004},
  month = apr,
  series = {{{CHI}} '04},
  pages = {151--158},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/985692.985712},
  urldate = {2024-02-25},
  abstract = {Debugging is still among the most common and costly of programming activities. One reason is that current debugging tools do not directly support the inquisitive nature of the activity. Interrogative Debugging is a new debugging paradigm in which programmers can ask why did and even why didn't questions directly about their program's runtime failures. The Whyline is a prototype Interrogative Debugging interface for the Alice programming environment that visualizes answers in terms of runtime events directly relevant to a programmer's question. Comparisons of identical debugging scenarios from user tests with and without the Whyline showed that the Whyline reduced debugging time by nearly a factor of 8, and helped programmers complete 40\% more tasks.},
  isbn = {978-1-58113-702-6},
  keywords = {Alice,debugging,program slicing},
  file = {/Users/harrison/Zotero/storage/FWY4WWDV/Ko and Myers - 2004 - Designing the whyline a debugging interface for a.pdf}
}

@article{koppelSearchingEntangledProgram2022,
  title = {Searching Entangled Program Spaces},
  author = {Koppel, James and Guo, Zheng and {de Vries}, Edsko and {Solar-Lezama}, Armando and Polikarpova, Nadia},
  year = {2022},
  month = aug,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {ICFP},
  pages = {91:23--91:51},
  doi = {10.1145/3547622},
  urldate = {2023-10-13},
  abstract = {Many problem domains, including program synthesis and rewrite-based optimization, require searching astronomically large spaces of programs. Existing approaches often rely on building specialized data structures---version-space algebras, finite tree automata, or e-graphs---to compactly represent such spaces. At their core, all these data structures exploit independence of subterms; as a result, they cannot efficiently represent more complex program spaces, where the choices of subterms are entangled. We introduce equality-constrained tree automata (ECTAs), a new data structure, designed to compactly represent large spaces of programs with entangled subterms. We present efficient algorithms for extracting programs from ECTAs, implemented in a performant Haskell library, ecta. Using the ecta library, we construct Hectare, a type-driven program synthesizer for Haskell. Hectare significantly outperforms a state-of-the-art synthesizer Hoogle+---providing an average speedup of 8{\texttimes}---despite its implementation being an order of magnitude smaller.},
  keywords = {e-graphs,Haskell,program synthesis,type systems},
  file = {/Users/harrison/Zotero/storage/3SMXW7EN/Koppel et al. - 2022 - Searching entangled program spaces.pdf}
}

@misc{krasnerCostPoorSoftware2022,
  title = {The Cost of Poor Software Quality in the {{US}}: {{A}} 2022 Report},
  author = {Krasner, Herb},
  year = {2022},
  month = dec
}

@misc{krekelPytestHelpsYou2023,
  title = {Pytest: Helps You Write Better Programs --- Pytest Documentation},
  shorttitle = {Pytest},
  author = {{krekel}, holger},
  year = {2023},
  urldate = {2023-07-06},
  howpublished = {https://docs.pytest.org/en/7.4.x/},
  file = {/Users/harrison/Zotero/storage/3Y3P5RFI/7.4.x.html}
}

@inproceedings{krishnamurthiHumanFormalMethods2019,
  title = {The {{Human}} in {{Formal Methods}}},
  booktitle = {Formal {{Methods}} -- {{The Next}} 30 {{Years}}},
  author = {Krishnamurthi, Shriram and Nelson, Tim},
  editor = {{ter Beek}, Maurice H. and McIver, Annabelle and Oliveira, Jos{\'e} N.},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--10},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-30942-8_1},
  abstract = {Formal methods are invaluable for reasoning about complex systems. As these techniques and tools have improved in expressiveness and scale, their adoption has grown rapidly. Sustaining this growth, however, requires attention to not only the technical but also the human side. In this paper (and accompanying talk), we discuss some of the challenges and opportunities for human factors in formal methods.},
  isbn = {978-3-030-30942-8},
  langid = {english},
  keywords = {Education,Formal methods,Human factors,User Interfaces},
  file = {/Users/harrison/Zotero/storage/BXP62ANL/Krishnamurthi and Nelson - 2019 - The Human in Formal Methods.pdf}
}

@inproceedings{kristensenJeopardyInvertibleFunctional2024,
  title = {Jeopardy: {{An Invertible Functional Programming Language}}},
  shorttitle = {Jeopardy},
  booktitle = {Reversible {{Computation}}},
  author = {Kristensen, Joachim Tilsted and Kaarsgaard, Robin and Thomsen, Michael Kirkedal},
  editor = {Mogensen, Torben {\AE}gidius and Mikulski, {\L}ukasz},
  year = {2024},
  pages = {124--141},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-62076-8_9},
  abstract = {Reversible programming languages guarantee that their programs are invertible at the cost of restricting the permissible operations to those which are locally invertible. However, writing programs in a reversible style can be cumbersome, and may produce significantly different implementations than the conventional -- even when the implemented algorithm is, in fact, invertible. We introduce Jeopardy, a functional programming language that guarantees global program invertibility without imposing local invertibility. In particular, Jeopardy allows the limited use of uninvertible -- and even nondeterministic -- operations, provided that they are used in a way that can be statically determined to be globally invertible. To this end, we outline an implicitly available arguments analysis and further approaches that can give a partial static guarantee to the (generally difficult) problem of guaranteeing invertibility.},
  isbn = {978-3-031-62076-8},
  langid = {english},
  keywords = {functional programming languages,invertible computing,program inversion,reversible computing},
  file = {/Users/harrison/Zotero/storage/6TTA9YGX/Kristensen et al. - 2024 - Jeopardy An Invertible Functional Programming Lan.pdf}
}

@inproceedings{krookQuickerCheckImplementingEvaluating2024,
  title = {{{QuickerCheck}}: {{Implementing}} and {{Evaluating}} a {{Parallel Run-Time}} for {{QuickCheck}}},
  shorttitle = {{{QuickerCheck}}},
  booktitle = {Proceedings of the 35th {{Symposium}} on {{Implementation}} and {{Application}} of {{Functional Languages}}},
  author = {Krook, Robert and Smallbone, Nicholas and Svensson, Bo Joel and Claessen, Koen},
  year = {2024},
  month = jun,
  series = {{{IFL}} '23},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3652561.3652570},
  urldate = {2025-03-12},
  abstract = {This paper introduces a new parallel run-time for QuickCheck, a Haskell library and EDSL for specifying and randomly testing properties of programs. The new run-time can run multiple tests for a single property in parallel, using the available cores. Moreover, if a counterexample is found, the run-time can also shrink the test case in parallel, implementing a parallel search for a locally minimal counterexample. Our experimental results show a 3--9 {\texttimes} speed-up for testing QuickCheck properties on a variety of heavy-weight benchmark problems. We also evaluate two different shrinking strategies; deterministic shrinking, which guarantees to produce the same minimal test case as standard sequential shrinking, and greedy shrinking, which does not have this guarantee but still produces a locally minimal test case, and is faster in practice.},
  isbn = {9798400716317},
  file = {/Users/harrison/Zotero/storage/5NUVICBP/Krook et al. - 2024 - QuickerCheck Implementing and Evaluating a Parall.pdf}
}

@inproceedings{kuhnCombinatorialMethodsEvent2012,
  title = {Combinatorial {{Methods}} for {{Event Sequence Testing}}},
  booktitle = {Fifth {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}, {{ICST}} 2012, {{Montreal}}, {{QC}}, {{Canada}}, {{April}} 17-21, 2012},
  author = {Kuhn, D. Richard and Higdon, James M. and Lawrence, James and Kacker, Raghu and Lei, Yu},
  editor = {Antoniol, Giuliano and Bertolino, Antonia and Labiche, Yvan},
  year = {2012},
  pages = {601--609},
  publisher = {IEEE Computer Society},
  doi = {10.1109/ICST.2012.147},
  file = {/Users/harrison/Zotero/storage/J5LFF8M9/Kuhn et al. - 2012 - Combinatorial Methods for Event Sequence Testing.pdf;/Users/harrison/Zotero/storage/XE3TMXN6/6200159.html}
}

@techreport{kuhnPracticalCombinatorialTesting2010,
  title = {Practical Combinatorial Testing},
  author = {Kuhn, D R and Kacker, R N and Lei, Y},
  year = {2010},
  edition = {0},
  number = {NIST SP 800-142},
  pages = {NIST SP 800-142},
  address = {Gaithersburg, MD},
  institution = {{National Institute of Standards and Technology}},
  doi = {10.6028/NIST.SP.800-142},
  urldate = {2022-11-22},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/8RQY5YS6/Kuhn et al. - 2010 - Practical combinatorial testing.pdf}
}

@article{kuhnSoftwareFaultInteractions2004,
  title = {Software Fault Interactions and Implications for Software Testing},
  author = {Kuhn, D.R. and Wallace, D.R. and Gallo, A.M.},
  year = {2004},
  month = jun,
  journal = {IEEE Transactions on Software Engineering},
  volume = {30},
  number = {6},
  pages = {418--421},
  issn = {1939-3520},
  doi = {10.1109/TSE.2004.24},
  abstract = {Exhaustive testing of computer software is intractable, but empirical studies of software failures suggest that testing can in some cases be effectively exhaustive. We show that software failures in a variety of domains were caused by combinations of relatively few conditions. These results have important implications for testing. If all faults in a system can be triggered by a combination of n or fewer parameters, then testing all n-tuples of parameters is effectively equivalent to exhaustive testing, if software behavior is not dependent on complex event sequences and variables have a small set of discrete values.},
  file = {/Users/harrison/Zotero/storage/BFRYASAI/Kuhn et al. - 2004 - Software fault interactions and implications for s.pdf;/Users/harrison/Zotero/storage/V6H3T6WN/1321063.html}
}

@article{kullbackInformationSufficiency1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  eprint = {2236703},
  eprinttype = {jstor},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2023-02-08},
  file = {/Users/harrison/Zotero/storage/BA5BK7QW/Kullback and Leibler - 1951 - On Information and Sufficiency.pdf}
}

@inproceedings{kurajProgrammingEnumerableSets2015,
  title = {Programming with Enumerable Sets of Structures},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object-Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}, {{OOPSLA}} 2015, Part of {{SPLASH}} 2015, {{Pittsburgh}}, {{PA}}, {{USA}}, {{October}} 25-30, 2015},
  author = {Kuraj, Ivan and Kuncak, Viktor and Jackson, Daniel},
  editor = {Aldrich, Jonathan and Eugster, Patrick},
  year = {2015},
  pages = {37--56},
  publisher = {ACM},
  doi = {10.1145/2814270.2814323},
  keywords = {algebra,data generation,Dependent enumeration,DSL,exhaustive testing,invariant,lazy evaluation,pairing function,program inversion,random testing,SciFe},
  file = {/Users/harrison/Zotero/storage/JC7QV4AV/Kuraj et al. - 2015 - Programming with enumerable sets of structures.pdf}
}

@inproceedings{lammelControllableCombinatorialCoverage2006,
  title = {Controllable {{Combinatorial Coverage}} in {{Grammar-Based Testing}}},
  booktitle = {Testing of {{Communicating Systems}}, 18th {{IFIP TC6}}/{{WG6}}.1 {{International Conference}}, {{TestCom}} 2006, {{New York}}, {{NY}}, {{USA}}, {{May}} 16-18, 2006, {{Proceedings}}},
  author = {L{\"a}mmel, Ralf and Schulte, Wolfram},
  editor = {Uyar, M. {\"U}mit and Duale, Ali Y. and Fecko, Mariusz A.},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3964},
  pages = {19--38},
  publisher = {Springer},
  doi = {10.1007/11754008_2},
  file = {/Users/harrison/Zotero/storage/XGSSP826/Lämmel and Schulte - 2006 - Controllable Combinatorial Coverage in Grammar-Bas.pdf}
}

@article{lamPropertybasedRandomizedTest2017,
  title = {Property-Based {{Randomized Test Generation}} for {{Interactive Apps}}},
  author = {Lam, Edmund S L and Zhang, Peilun and Chang, Bor-Yuh Evan},
  year = {2017},
  abstract = {We consider the problem of generating relevant execution traces to test rich interactive applications. Rich interactive applications, such as apps on mobile platforms, are complex stateful and often distributed systems where su ciently exercising the app with user-interaction (UI) event sequences to expose defects is both hard and time-consuming. In particular, there is a fundamental tension between brute-force random UI exercising tools, which are fully-automated but o er low relevance, and UI test scripts, which are manual but o er high relevance. In this paper, we consider a middle way---enabling a seamless fusion of scripted and randomized UI testing. This fusion is prototyped in a testing tool called ChimpCheck for programming, generating, and executing property-based randomized test cases for Android apps. Our approach realizes this fusion by o ering a high-level, embedded domain-speci c language for de ning custom generators of simulated user-interaction event sequences. What follows is a combinator library built on industrial strength frameworks for property-based testing (ScalaCheck) and Android testing (Android JUnit and Espresso) to implement propertybased randomized testing for Android development. Driven by real, reported issues in open source Android apps, we show, through case studies, how ChimpCheck enables expressing e ective testing patterns in a compact manner.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/IH3J9N94/Lam et al. - 2017 - Property-based Randomized Test Generation for Inte.pdf}
}

@article{lampropoulosBeginnerLuckLanguage2017,
  title = {Beginner's {{Luck}}: A Language for Property-Based Generators},
  author = {Lampropoulos, Leonidas and {Gallois-Wong}, Diane and Hritcu, Catalin and Hughes, John and Pierce, Benjamin C. and Xia, Li-yao},
  year = {2017},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL 2017, Paris, France, January 18-20, 2017},
  pages = {114--129},
  file = {/Users/harrison/Zotero/storage/LM7MTUJW/Lampropoulos et al. - 2017 - Beginner's luck a language for property-based gen.pdf}
}

@article{lampropoulosCoverageGuidedProperty2019,
  title = {Coverage Guided, Property Based Testing},
  author = {Lampropoulos, Leonidas and Hicks, Michael and Pierce, Benjamin C.},
  year = {2019},
  journal = {PACMPL},
  volume = {3},
  number = {OOPSLA},
  pages = {181:1--181:29},
  doi = {10.1145/3360607},
  keywords = {AFL,coverage,fuzz testing,FuzzChick,property-based testing,QuickChick,random testing},
  file = {/Users/harrison/Zotero/storage/DZM9WCC7/Lampropoulos et al. - 2019 - Coverage guided, property based testing.pdf}
}

@article{lampropoulosGeneratingGoodGenerators2017,
  title = {Generating Good Generators for Inductive Relations},
  author = {Lampropoulos, Leonidas and Paraskevopoulou, Zoe and Pierce, Benjamin C.},
  year = {2017},
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {POPL},
  pages = {1--30},
  publisher = {ACM New York, NY, USA},
  file = {/Users/harrison/Zotero/storage/I3E8C2JH/Lampropoulos et al. - 2017 - Generating good generators for inductive relations.pdf}
}

@inproceedings{lawranceHowWellProfessional2005,
  title = {How Well Do Professional Developers Test with Code Coverage Visualizations? An Empirical Study},
  booktitle = {2005 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}}'05)},
  author = {Lawrance, J and Clarke, Steven and Burnett, Margaret and Rothermel, Gregg},
  year = {2005},
  pages = {53--60},
  publisher = {IEEE}
}

@misc{leeLuxAlwaysonVisualization2021,
  title = {Lux: {{Always-on Visualization Recommendations}} for {{Exploratory Dataframe Workflows}}},
  shorttitle = {Lux},
  author = {Lee, Doris Jung-Lin and Tang, Dixin and Agarwal, Kunal and Boonmark, Thyne and Chen, Caitlyn and Kang, Jake and Mukhopadhyay, Ujjaini and Song, Jerry and Yong, Micah and Hearst, Marti A. and Parameswaran, Aditya G.},
  year = {2021},
  month = dec,
  number = {arXiv:2105.00121},
  eprint = {2105.00121},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.00121},
  urldate = {2022-12-21},
  abstract = {Exploratory data science largely happens in computational notebooks with dataframe APIs, such as pandas, that support flexible means to transform, clean, and analyze data. Yet, visually exploring data in dataframes remains tedious, requiring substantial programming effort for visualization and mental effort to determine what analysis to perform next. We propose Lux, an always-on framework for accelerating visual insight discovery in dataframe workflows. When users print a dataframe in their notebooks, Lux recommends visualizations to provide a quick overview of the patterns and trends and suggests promising analysis directions. Lux features a high level language for generating visualizations on demand to encourage rapid visual experimentation with data. We demonstrate that through the use of a careful design and three system optimizations, Lux adds no more than two seconds of overhead on top of pandas for over 98\% of datasets in the UCI repository. We evaluate Lux in terms of usability via a controlled first-use study and interviews with early adopters, finding that Lux helps fulfill the needs of data scientists for visualization support within their dataframe workflows. Lux has already been embraced by data science practitioners, with over 3.1k stars on Github.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Human-Computer Interaction},
  file = {/Users/harrison/Zotero/storage/TMK6XE2K/Lee et al. - 2021 - Lux Always-on Visualization Recommendations for E.pdf;/Users/harrison/Zotero/storage/BNDAZCKZ/2105.html}
}

@article{leijenParsecPracticalParser2001,
  title = {Parsec: {{A}} Practical Parser Library},
  author = {Leijen, Daan and Meijer, Erik},
  year = {2001},
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {41},
  number = {1},
  pages = {1--20}
}

@inproceedings{leinoDafnyAutomaticProgram2010,
  title = {Dafny: {{An Automatic Program Verifier}} for {{Functional Correctness}}},
  shorttitle = {Dafny},
  booktitle = {Logic for {{Programming}}, {{Artificial Intelligence}}, and {{Reasoning}}},
  author = {Leino, K. Rustan M.},
  editor = {Clarke, Edmund M. and Voronkov, Andrei},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {348--370},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-17511-4_20},
  abstract = {Traditionally, the full verification of a program's functional correctness has been obtained with pen and paper or with interactive proof assistants, whereas only reduced verification tasks, such as extended static checking, have enjoyed the automation offered by satisfiability-modulo-theories (SMT) solvers. More recently, powerful SMT solvers and well-designed program verifiers are starting to break that tradition, thus reducing the effort involved in doing full verification.},
  isbn = {978-3-642-17511-4},
  langid = {english},
  keywords = {Automatic Program,Call Graph,Java Modeling Language,Proof Obligation,Separation Logic},
  file = {/Users/harrison/Zotero/storage/89VWXCPX/Leino - 2010 - Dafny An Automatic Program Verifier for Functiona.pdf}
}

@inproceedings{lernerProjectionBoxesOnthefly2020,
  title = {Projection Boxes: {{On-the-fly}} Reconfigurable Visualization for Live Programming},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lerner, Sorin},
  year = {2020},
  pages = {1--7}
}

@inproceedings{leroyCompCertFormallyVerified2016,
  title = {{{CompCert}} - {{A Formally Verified Optimizing Compiler}}},
  booktitle = {{{ERTS}} 2016: {{Embedded Real Time Software}} and {{Systems}}, 8th {{European Congress}}},
  author = {Leroy, Xavier and Blazy, Sandrine and K{\"a}stner, Daniel and Schommer, Bernhard and Pister, Markus and Ferdinand, Christian},
  year = {2016},
  month = jan,
  urldate = {2024-07-08},
  abstract = {CompCert is the first commercially available optimizing compiler that is formally verified, using machine-assisted mathematical proofs, to be exempt from mis-compilation. The executable code it produces is proved to behave exactly as specified by the semantics of the source C program. This article gives an overview of the design of CompCert and its proof concept and then focuses on aspects relevant for industrial application. We briefly summarize practical experience and give an overview of recent CompCert development aiming at industrial usage. CompCert's intended use is the compilation of life-critical and mission-critical software meeting high levels of assurance. In this context tool qualification is of paramount importance. We summarize the confidence argument of CompCert and give an overview of relevant qualification strategies.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/M6Q5FWWT/Leroy et al. - 2016 - CompCert - A Formally Verified Optimizing Compiler.pdf}
}

@inproceedings{levenshteinBinaryCodesCapable1966,
  title = {Binary Codes Capable of Correcting Deletions, Insertions, and Reversals},
  booktitle = {Soviet Physics Doklady},
  author = {Levenshtein, Vladimir I and others},
  year = {1966},
  volume = {10},
  pages = {707--710},
  publisher = {Soviet Union},
  file = {/Users/harrison/Zotero/storage/MVZ5RQPA/Levenshtein1966a.pdf}
}

@article{levesonInvestigationTherac25Accidents1993,
  title = {An Investigation of the {{Therac-25}} Accidents},
  author = {Leveson, N.G. and Turner, C.S.},
  year = {1993},
  month = jul,
  journal = {Computer},
  volume = {26},
  number = {7},
  pages = {18--41},
  issn = {1558-0814},
  doi = {10.1109/MC.1993.274940},
  urldate = {2024-04-23},
  abstract = {Between June 1985 and January 1987, the Therac-25 medical electron accelerator was involved in six massive radiation overdoses. As a result, several people died and others were seriously injured. A detailed investigation of the factors involved in the software-related overdoses and attempts by users, manufacturers, and government agencies to deal with the accidents is presented. The authors demonstrate the complex nature of accidents and the need to investigate all aspects of system development and operation in order to prevent future accidents. The authors also present some lessons learned in terms of system engineering, software engineering, and government regulation of safety-critical systems containing software components.{$<>$}},
  keywords = {Accidents,Biomedical applications of radiation,Computer industry,Drugs,Electron accelerators,Food industry,Food manufacturing,History,Injuries,Software safety},
  file = {/Users/harrison/Zotero/storage/U58JU2YH/Leveson and Turner - 1993 - An investigation of the Therac-25 accidents.pdf;/Users/harrison/Zotero/storage/EJ6TY3C9/274940.html}
}

@article{liAcceleratingFuzzingPrefixGuided2023,
  title = {Accelerating {{Fuzzing}} through {{Prefix-Guided Execution}}},
  author = {Li, Shaohua and Su, Zhendong},
  year = {2023},
  month = apr,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {7},
  number = {OOPSLA1},
  pages = {75:1--75:27},
  doi = {10.1145/3586027},
  urldate = {2023-04-25},
  abstract = {Coverage-guided fuzzing is one of the most effective approaches for discovering software defects and vulnerabilities. It executes all mutated tests from seed inputs to expose coverage-increasing tests. However, executing all mutated tests incurs significant performance penalties---most of the mutated tests are discarded because they do not increase code coverage. Thus, determining if a test increases code coverage without actually executing it is beneficial, but a paradoxical challenge. In this paper, we introduce the notion of prefix-guided execution (PGE) to tackle this challenge. PGE leverages two key observations: (1) Only a tiny fraction of the mutated tests increase coverage, thus requiring full execution; and (2) whether a test increases coverage may be accurately inferred from its partial execution. PGE monitors the execution of a test and applies early termination when the execution prefix indicates that the test is unlikely to increase coverage. To demonstrate the potential of PGE, we implement a prototype on top of AFL++, which we call AFL++-PGE. We evaluate AFL++-PGE on MAGMA, a ground-truth benchmark set that consists of 21 programs from nine popular real-world projects. Our results show that, after 48 hours of fuzzing, AFL++-PGE finds more bugs, discovers bugs faster, and achieves higher coverage. Prefix-guided execution is general and can benefit the AFL-based family of fuzzers.},
  keywords = {code coverage,fuzzing,software testing},
  file = {/Users/harrison/Zotero/storage/9WA97L3H/Li and Su - 2023 - Accelerating Fuzzing through Prefix-Guided Executi.pdf}
}

@inproceedings{liangMonadTransformersModular1995,
  title = {Monad {{Transformers}} and {{Modular Interpreters}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  author = {Liang, Sheng and Hudak, Paul and Jones, Mark},
  year = {1995},
  series = {{{POPL}} '95},
  pages = {333--343},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/199448.199528},
  abstract = {We show how a set of building blocks can be used to construct programming language interpreters, and present implementations of such building blocks capable of supporting many commonly known features, including simple expressions, three different function call mechanisms (call-by-name, call-by-value and lazy evaluation), references and assignment, nondeterminism, first-class continuations, and program tracing.The underlying mechanism of our system is monad transformers, a simple form of abstraction for introducing a wide range of computational behaviors, such as state, I/O, continuations, and exceptions.Our work is significant in the following respects. First, we have succeeded in designing a fully modular interpreter based on monad transformers that incudes features missing from Steele's, Espinosa's, and Wadler's earlier efforts. Second, we have found new ways to lift monad operations through monad transformers, in particular difficult cases not achieved in Moggi's original work. Third, we have demonstrated that interactions between features are reflected in liftings and that semantics can be changed by reordering monad transformers. Finally, we have implemented our interpreter in Gofer, whose constructor classes provide just the added power over Haskell's type classes to allow precise and convenient expression of our ideas. This implementation includes a method for constructing extensible unions and a form of subtyping that is interesting in its own right.},
  isbn = {0-89791-692-1},
  file = {/Users/harrison/Zotero/storage/LW5D5DPA/Liang et al. - 1995 - Monad Transformers and Modular Interpreters.pdf}
}

@misc{lighttableLightTable,
  title = {Light {{Table}}},
  author = {{lighttable}},
  urldate = {2022-12-21},
  howpublished = {http://lighttable.com/},
  file = {/Users/harrison/Zotero/storage/5JC79PZB/lighttable.com.html}
}

@inproceedings{liModelbasedTestingNetworked2021,
  title = {Model-Based Testing of Networked Applications},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Li, Yishuai and Pierce, Benjamin C. and Zdancewic, Steve},
  year = {2021},
  month = jul,
  series = {{{ISSTA}} 2021},
  pages = {529--539},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3460319.3464798},
  urldate = {2022-12-05},
  abstract = {We present a principled automatic testing framework for application-layer protocols. The key innovation is a domain-specific embedded language for writing nondeterministic models of the behavior of networked servers. These models are defined within the Coq interactive theorem prover, supporting a smooth transition from testing to formal verification. Given a server model, we show how to automatically derive a tester that probes the server for unexpected behaviors. We address the uncertainties caused by both the server's internal choices and the network delaying messages nondeterministically. The derived tester accepts server implementations whose possible behaviors are a subset of those allowed by the nondeterministic model. We demonstrate the effectiveness of this framework by using it to specify and test a fragment of the HTTP/1.1 protocol, showing that the automatically derived tester can capture RFC violations in buggy server implementations, including the latest versions of Apache and Nginx.},
  isbn = {978-1-4503-8459-9},
  keywords = {Coq,HTTP,interaction trees,Model-based testing,network refinement,nondeterminism},
  file = {/Users/harrison/Zotero/storage/RETUZJ44/Li et al. - 2021 - Model-based testing of networked applications.pdf}
}

@inproceedings{limpergAesopWhiteBoxBestFirst2023,
  title = {Aesop: {{White-Box Best-First Proof Search}} for {{Lean}}},
  shorttitle = {Aesop},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Conference}} on {{Certified Programs}} and {{Proofs}}},
  author = {Limperg, Jannis and From, Asta Halkj{\ae}r},
  year = {2023},
  month = jan,
  series = {{{CPP}} 2023},
  pages = {253--266},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3573105.3575671},
  urldate = {2024-12-28},
  abstract = {We present Aesop, a proof search tactic for the Lean 4 interactive   theorem prover. Aesop performs a tree-based search over a user-specified   set of proof rules. It supports safe and unsafe rules and uses a best-first   search strategy with customisable prioritisation. Aesop also allows users   to register custom normalisation rules and integrates Lean's simplifier to   support equational reasoning. Many details of Aesop's search procedure are   designed to make it a white-box proof automation tactic, meaning that users   should be able to easily predict how their rules will be applied, and thus how   powerful and fast their Aesop invocations will be.   Since we use a best-first search strategy, it is not obvious how to handle   metavariables which appear in multiple goals. The most common strategy for   dealing with metavariables relies on backtracking and is therefore not   suitable for best-first search. We give an algorithm which addresses this   issue. The algorithm works with any search strategy, is independent of the   underlying logic and makes few assumptions about how rules interact with   metavariables. We conjecture that with a fair search strategy, the algorithm   is as complete as the given set of rules allows.},
  isbn = {9798400700262},
  file = {/Users/harrison/Zotero/storage/HKTTKJYE/Limperg and From - 2023 - Aesop White-Box Best-First Proof Search for Lean.pdf}
}

@inproceedings{lincroftThirtyThreeYearsMathematicians2024,
  title = {Thirty-{{Three Years}} of {{Mathematicians}} and {{Software Engineers}}: {{A Case Study}} of {{Domain Expertise}} and {{Participation}} in {{Proof Assistant Ecosystems}}},
  shorttitle = {Thirty-{{Three Years}} of {{Mathematicians}} and {{Software Engineers}}},
  booktitle = {2024 {{IEEE}}/{{ACM}} 21st {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Lincroft, Gwenyth and Cho, Minsung and Hough, Katherine and Bazzaz, Mahsa and Bell, Jonathan},
  year = {2024},
  month = apr,
  pages = {1--13},
  issn = {2574-3864},
  urldate = {2024-07-08},
  abstract = {As technical computing software, such as MATLAB and SciPy, has gained popularity, ecosystems of interdependent software solutions and communities have formed around these technologies. The development and maintenance of these technical computing ecosystems requires expertise in both software engineering and the underlying technical domain. The inherently interdisciplinary nature of these ecosystems presents unique challenges and opportunities that shape software development practices.Proof assistants, a type of technical computing software, aid users in the creation of formal proofs. In order to examine the influence of the underlying technical domain --- mathematics --- on the development of proof assistant ecosystems, we mined participant activity data from the code repositories and social channels of three popular proof assistants: Lean, Coq, Isabelle. Despite having a shared technical domain, we found little cross-pollination between contributors to the proof assistants. Additionally, we found that most long-term developers focused solely on technical work and did not participate in official social channels. We also found that proof assistant developers specialized into technical subfields. However, the proportion of specialists varied between ecosystems. We did not find evidence that these specialties contributed to fractures within the ecosystems. We discuss the implications of these results on the long-term health and sustainability of proof assistant ecosystems.},
  keywords = {Data mining,Ecosystems,Mathematics,Shape,Software,Surveys,Sustainable development},
  file = {/Users/harrison/Zotero/storage/A3GEUJIW/Lincroft et al. - 2024 - Thirty-Three Years of Mathematicians and Software .pdf;/Users/harrison/Zotero/storage/69PNNCQE/10555745.html}
}

@article{linDivergenceMeasuresBased1991,
  title = {Divergence Measures Based on the {{Shannon}} Entropy},
  author = {Lin, J.},
  year = {1991},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {37},
  number = {1},
  pages = {145--151},
  issn = {1557-9654},
  doi = {10.1109/18.61115},
  abstract = {A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness.{$<>$}},
  keywords = {Computer science,Entropy,Genetics,Pattern analysis,Pattern recognition,Probability distribution,Signal analysis,Signal processing,Taxonomy,Upper bound},
  file = {/Users/harrison/Zotero/storage/G8CVUIJ4/Lin - 1991 - Divergence measures based on the Shannon entropy.pdf;/Users/harrison/Zotero/storage/8EUJ8Y5B/61115.html}
}

@article{liuBobaAuthoringVisualizing2021,
  title = {Boba: {{Authoring}} and {{Visualizing Multiverse Analyses}}},
  shorttitle = {Boba},
  author = {Liu, Yang and Kale, Alex and Althoff, Tim and Heer, Jeffrey},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {27},
  number = {2},
  pages = {1753--1763},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2020.3028985},
  urldate = {2024-11-27},
  abstract = {Multiverse analysis is an approach to data analysis in which all ``reasonable'' analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency. However, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation. We contribute Baba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses. With the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths. The Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit. We demonstrate Boba's utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.},
  keywords = {Analytic Decisions,Analytical models,Computational modeling,Data models,DSL,Load modeling,Multiverse Analysis,Reproducibility,Robustness,Statistical Analysis,Tools},
  file = {/Users/harrison/Zotero/storage/WB7XBVSU/Liu et al. - 2021 - Boba Authoring and Visualizing Multiverse Analyse.pdf;/Users/harrison/Zotero/storage/S5WRVG56/9216579.html}
}

@article{livinskiiRandomTestingCompilers2020,
  title = {Random Testing for {{C}} and {{C}}++ Compilers with {{YARPGen}}},
  author = {Livinskii, Vsevolod and Babokin, Dmitry and Regehr, John},
  year = {2020},
  month = nov,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {4},
  number = {OOPSLA},
  pages = {196:1--196:25},
  doi = {10.1145/3428264},
  urldate = {2022-11-22},
  abstract = {Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel{\textregistered} C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20\% for LLVM and 40\% for GCC. We also created tools for automating most of the common tasks related to compiler fuzzing; these tools are also useful for fuzzers other than ours.},
  file = {/Users/harrison/Zotero/storage/54ABBV5F/Livinskii et al. - 2020 - Random testing for C and C++ compilers with YARPGe.pdf}
}

@inproceedings{loscherTargetedPropertybasedTesting2017,
  title = {Targeted Property-Based Testing},
  booktitle = {Proceedings of the 26th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {L{\"o}scher, Andreas and Sagonas, Konstantinos},
  year = {2017},
  month = jul,
  series = {{{ISSTA}} 2017},
  pages = {46--56},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3092703.3092711},
  urldate = {2022-11-21},
  abstract = {We introduce targeted property-based testing, an enhanced form of property-based testing that aims to make the input generation component of a property-based testing tool guided by a search strategy rather than being completely random. Thus, this testing technique combines the advantages of both search-based and property-based testing. We demonstrate the technique with the framework we have built, called Target, and show its effectiveness on three case studies. The first of them demonstrates how Target can employ simulated annealing to generate sensor network topologies that form configurations with high energy consumption. The second case study shows how the generation of routing trees for a wireless network equipped with directional antennas can be guided to fulfill different energy metrics. The third case study employs Target to test the noninterference property of information-flow control abstract machine designs, and compares it with a sophisticated hand-written generator for programs of these abstract machines.},
  isbn = {978-1-4503-5076-1},
  keywords = {PropEr,Property-based testing,QuickCheck,Search-based testing},
  file = {/Users/harrison/Zotero/storage/8CDQDGEZ/Löscher and Sagonas - 2017 - Targeted property-based testing.pdf}
}

@inproceedings{loShrinkingCounterexamplesPropertyBased2020,
  title = {Shrinking {{Counterexamples}} in {{Property-Based Testing}} with {{Genetic Algorithms}}},
  booktitle = {2020 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Lo, Fang-Yi and Chen, Chao-Hong and Chen, Ying-ping},
  year = {2020},
  month = jul,
  pages = {1--8},
  doi = {10.1109/CEC48606.2020.9185807},
  urldate = {2024-03-11},
  abstract = {In this paper, genetic algorithms are proposed to shrink counterexamples found by QuickChick, a property-based testing framework for Coq. In order to make the outcome of property-based testing humanly understandable and inspectable, genetic algorithms are brought into the realm of rigorous software development as shrinkers capable of handling a broad range of data structures. In the present study, two showcases, merge sort and insertion of red-black trees, are investigated for illustrative purposes. Due to the lack of relevant results existing in the literature, two baseline methods, random sample and random walk are included in the experiments for comparison with the proposed genetic algorithm. The obtained results indicate that the proposal is effective since the program mistake can be identified with ease by examining the shrunk counterexamples and also that the adopted genetic algorithm statistically significantly outperforms random sample and random walk in both counterexample sizes and running time.},
  keywords = {Binary search trees,Genetic algorithms,Software,Software testing,Urban areas,Vegetation},
  file = {/Users/harrison/Zotero/storage/WJXUBTTJ/Lo et al. - 2020 - Shrinking Counterexamples in Property-Based Testin.pdf;/Users/harrison/Zotero/storage/SX5EUT8M/9185807.html}
}

@article{lubinEquivalenceCanonicalizationSynthesisBacked2024,
  title = {Equivalence by {{Canonicalization}} for {{Synthesis-Backed Refactoring}}},
  author = {Lubin, Justin and Ferguson, Jeremy and Ye, Kevin and Yim, Jacob and Chasins, Sarah E.},
  year = {2024},
  month = jun,
  journal = {Reproduction Package for "Equivalence by Canonicalization for Synthesis-Backed Refactoring"},
  volume = {8},
  number = {PLDI},
  pages = {223:1879--223:1904},
  doi = {10.1145/3656453},
  urldate = {2024-12-17},
  abstract = {We present an enumerative program synthesis framework called component-based refactoring that can refactor "direct" style code that does not use library components into equivalent "combinator" style code that does use library components. This framework introduces a sound but incomplete technique to check the equivalence of direct code and combinator code called equivalence by canonicalization that does not rely on input-output examples or logical specifications. Moreover, our approach can repurpose existing compiler optimizations, leveraging decades of research from the programming languages community. We instantiated our new synthesis framework in two contexts: (i) higher-order functional combinators such as map and filter in the statically-typed functional programming language Elm and (ii) high-performance numerical computing combinators provided by the NumPy library for Python. We implemented both instantiations in a tool called Cobbler and evaluated it on thousands of real programs to test the performance of the component-based refactoring framework in terms of execution time and output quality. Our work offers evidence that synthesis-backed refactoring can apply across a range of domains without specification beyond the input program.},
  file = {/Users/harrison/Zotero/storage/27AJ52HU/Lubin et al. - 2024 - Equivalence by Canonicalization for Synthesis-Back.pdf}
}

@article{lubinHowStaticallytypedFunctional2021,
  title = {How Statically-Typed Functional Programmers Write Code},
  author = {Lubin, Justin and Chasins, Sarah E.},
  year = {2021},
  month = oct,
  journal = {Proc. ACM Program. Lang.},
  volume = {5},
  number = {OOPSLA},
  pages = {155:1--155:30},
  doi = {10.1145/3485532},
  urldate = {2024-12-17},
  abstract = {How working statically-typed functional programmers write code is largely understudied. And yet, a better understanding of developer practices could pave the way for the design of more useful and usable tooling, more ergonomic languages, and more effective on-ramps into programming communities. The goal of this work is to address this knowledge gap: to better understand the high-level authoring patterns that statically-typed functional programmers employ. We conducted a grounded theory analysis of 30 programming sessions of practicing statically-typed functional programmers, 15 of which also included a semi-structured interview. The theory we developed gives insight into how the specific affordances of statically-typed functional programming affect domain modeling, type construction, focusing techniques, exploratory and reasoning strategies, and expressions of intent. We conducted a set of quantitative lab experiments to validate our findings, including that statically-typed functional programmers often iterate between editing types and expressions, that they often run their compiler on code even when they know it will not successfully compile, and that they make textual program edits that reliably signal future edits that they intend to make. Lastly, we outline the implications of our findings for language and tool design. The success of this approach in revealing program authorship patterns suggests that the same methodology could be used to study other understudied programmer populations.},
  file = {/Users/harrison/Zotero/storage/X7M6KB5P/Lubin and Chasins - 2021 - How statically-typed functional programmers write .pdf}
}

@inproceedings{luoEmpiricalAnalysisFlaky2014,
  title = {An Empirical Analysis of Flaky Tests},
  booktitle = {Proceedings of the 22nd {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
  year = {2014},
  pages = {643--653},
  file = {/Users/harrison/Zotero/storage/88VZEPCX/Luo et al. - 2014 - An empirical analysis of flaky tests.pdf}
}

@inproceedings{maayanUsingReactiveSynthesis2023,
  title = {Using {{Reactive Synthesis}}: {{An End-to-End Exploratory Case Study}}},
  shorttitle = {Using {{Reactive Synthesis}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Ma'ayan, Dor and Maoz, Shahar},
  year = {2023},
  month = may,
  pages = {742--754},
  issn = {1558-1225},
  doi = {10.1109/ICSE48619.2023.00071},
  urldate = {2023-11-27},
  abstract = {Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Despite its attractiveness and major research progress in the past decades, reactive synthesis is still in early-stage and has not gained popularity outside academia. We conducted an exploratory case study in which we followed students in a semester-long university workshop class on their end-to-end use of a reactive synthesizer, from writing the specifications to executing the synthesized controllers. The data we collected includes more than 500 versions of more than 80 specifications, as well as more than 2500 Slack messages, all written by the class participants. Our grounded theory analysis reveals that the use of reactive synthesis has clear benefits for certain tasks and that adequate specification language constructs assist in the specification writing process. However, inherent issues such as unrealizabilty, non-well-separation, the gap of knowledge between the users and the synthesizer, and considerable running times prevent reactive synthesis from fulfilling its promise. Based on our analysis, we propose action items in the directions of language and specification quality, tools for analysis and execution, and process and methodology, all towards making reactive synthesis more applicable for software engineers.},
  file = {/Users/harrison/Zotero/storage/EUAX5MDG/Ma'ayan and Maoz - 2023 - Using Reactive Synthesis An End-to-End Explorator.pdf}
}

@article{maciverHypothesisNewApproach2019,
  title = {Hypothesis: {{A}} New Approach to Property-Based Testing},
  author = {MacIver, David R and {Hatfield-Dodds}, Zac and others},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1891},
  file = {/Users/harrison/Zotero/storage/T8C7A6WS/MacIver et al. - 2019 - Hypothesis A new approach to property-based testi.pdf}
}

@inproceedings{maciverTestCaseReductionTestCase2020,
  title = {Test-{{Case Reduction}} via {{Test-Case Generation}}: {{Insights}} from the {{Hypothesis Reducer}} ({{Tool Insights Paper}})},
  shorttitle = {Test-{{Case Reduction}} via {{Test-Case Generation}}},
  booktitle = {34th {{European Conference}} on {{Object-Oriented Programming}} ({{ECOOP}} 2020)},
  author = {MacIver, David R. and Donaldson, Alastair F.},
  editor = {Hirschfeld, Robert and Pape, Tobias},
  year = {2020},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {166},
  pages = {13:1--13:27},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum f{\"u}r Informatik},
  address = {Dagstuhl, Germany},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.ECOOP.2020.13},
  urldate = {2023-01-02},
  isbn = {978-3-95977-154-2},
  keywords = {Software testing,test-case reduction},
  file = {/Users/harrison/Zotero/storage/V4323DN7/MacIver and Donaldson - 2020 - Test-Case Reduction via Test-Case Generation Insi.pdf;/Users/harrison/Zotero/storage/QU25T8W9/13170.html}
}

@article{mackinnonEndotestingUnitTesting2000,
  title = {Endo-Testing: Unit Testing with Mock Objects},
  author = {Mackinnon, Tim and Freeman, Steve and Craig, Philip},
  year = {2000},
  journal = {Extreme programming examined},
  pages = {287--301},
  publisher = {Addison-Wesley Boston, MA, USA}
}

@inproceedings{macqueenModulesStandardML1984,
  title = {Modules for Standard {{ML}}},
  booktitle = {Proceedings of the 1984 {{ACM Symposium}} on {{LISP}} and Functional Programming},
  author = {MacQueen, David},
  year = {1984},
  month = aug,
  series = {{{LFP}} '84},
  pages = {198--207},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/800055.802036},
  urldate = {2022-12-09},
  abstract = {The functional programming language ML has been undergoing a thorough redesign during the past year, and the module facility described here has been proposed as part of the revised language, now called Standard ML. The design has three main goals: (1) to facilitate the structuring of large ML programs; (2) to support separate compilation and generic library units; and (3) to employ new ideas in the semantics of data types to extend the power of ML's polymorphic type system. It is based on concepts inherent in the structure of ML, primarily the notions of a declaration, its type signature, and the environment that it denotes.},
  isbn = {978-0-89791-142-9},
  file = {/Users/harrison/Zotero/storage/DE899WMA/MacQueen - 1984 - Modules for standard ML.pdf}
}

@article{magalhaesGenericDerivingMechanism2010,
  title = {A Generic Deriving Mechanism for {{Haskell}}},
  author = {Magalh{\~a}es, Jos{\'e} Pedro and Dijkstra, Atze and Jeuring, Johan and L{\"o}h, Andres},
  year = {2010},
  month = sep,
  journal = {ACM SIGPLAN Notices},
  volume = {45},
  number = {11},
  pages = {37--48},
  issn = {0362-1340},
  doi = {10.1145/2088456.1863529},
  urldate = {2023-02-09},
  abstract = {Haskell's deriving mechanism supports the automatic generation of instances for a number of functions. The Haskell 98 Report only specifies how to generate instances for the Eq, Ord, Enum, Bounded, Show, and Read classes. The description of how to generate instances is largely informal. The generation of instances imposes restrictions on the shape of datatypes, depending on the particular class to derive. As a consequence, the portability of instances across different compilers is not guaranteed. We propose a new approach to Haskell's deriving mechanism, which allows users to specify how to derive arbitrary class instances using standard datatype-generic programming techniques. Generic functions, including the methods from six standard Haskell 98 derivable classes, can be specified entirely within Haskell 98 plus multi-parameter type classes, making them lightweight and portable. We can also express Functor, Typeable, and many other derivable classes with our technique. We implemented our deriving mechanism together with many new derivable classes in the Utrecht Haskell Compiler.},
  keywords = {datatype-generic programming,haskell,type classes},
  file = {/Users/harrison/Zotero/storage/DMGZP38G/Magalhães et al. - 2010 - A generic deriving mechanism for Haskell.pdf}
}

@article{majumdarWhyRandomTesting2018,
  title = {Why Is Random Testing Effective for Partition Tolerance Bugs?},
  author = {Majumdar, Rupak and Niksic, Filip},
  year = {2018},
  journal = {PACMPL},
  volume = {2},
  number = {POPL},
  pages = {46:1--46:24},
  doi = {10.1145/3158134},
  file = {/Users/harrison/Zotero/storage/XYG43DAJ/Majumdar and Niksic - 2018 - Why is random testing effective for partition tole.pdf}
}

@inproceedings{marianiGenerationIntegrationTests2004,
  title = {Generation of {{Integration Tests}} for {{Self-Testing Components}}},
  booktitle = {Applying {{Formal Methods}}: {{Testing}}, {{Performance}}, and {{M}}/{{E-Commerce}}},
  author = {Mariani, Leonardo and Pezz{\`e}, Mauro and Willmor, David},
  editor = {N{\'u}{\~n}ez, Manuel and Maamar, Zakaria and Pelayo, Fernando L. and Pousttchi, Key and Rubio, Fernando},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {337--350},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30233-9_25},
  abstract = {Internet software tightly integrates classic computation with communication software. Heterogeneity and complexity can be tackled with a~component-based approach, where components are developed by application experts and integrated by domain experts. Component-based systems cannot be tested with classic approaches but present new problems. Current techniques for integration testing are based upon the component developer providing test specifications or suites with their components. However, components are often being used in ways not envisioned by their developers, thus the packaged test specifications and suites cannot be relied upon. Often this results in conditions being placed upon a~components use, however, what is required is a method for allowing test suites to be adapted for new situations. In this paper, we propose an approach for implementing self-testing components, which allow integration test specifications and suites to be developed by observing both the behavior of the component and of the entire system.},
  isbn = {978-3-540-30233-9},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/N8XLVG2Y/Mariani et al. - 2004 - Generation of Integration Tests for Self-Testing C.pdf}
}

@incollection{marlowGlasgowHaskellCompiler2012,
  title = {The {{Glasgow Haskell Compiler}}},
  booktitle = {The {{Architecture}} of {{Open Source Applications}}},
  author = {Marlow, Simon and {Peyton-Jones}, Simon},
  editor = {Brown, Amy and Wilson, Greg},
  year = {2012},
  month = mar,
  volume = {II},
  publisher = {Available online under the Creative Commons Attribution 3.0 Unported license}
}

@inproceedings{martinGoodOrganisationalReasons2007,
  title = {'{{Good}}' {{Organisational Reasons}} for '{{Bad}}' {{Software Testing}}: {{An Ethnographic Study}} of {{Testing}} in a {{Small Software Company}}},
  shorttitle = {'{{Good}}' {{Organisational Reasons}} for '{{Bad}}' {{Software Testing}}},
  booktitle = {29th {{International Conference}} on {{Software Engineering}} ({{ICSE}}'07)},
  author = {Martin, David and Rooksby, John and Rouncefield, Mark and Sommerville, Ian},
  year = {2007},
  month = may,
  pages = {602--611},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2007.1},
  abstract = {In this paper we report on an ethnographic study of a small software house to discuss the practical work of software testing. Through use of two rich descriptions, we discuss that 'rigour' in systems integration testing necessarily has to be organisationally defined. Getting requirements 'right', defining 'good' test scenarios and ensuring 'proper' test coverage are activities that need to be pragmatically achieved taking account of organisational realities and constraints such as: the dynamics of customer relationships; using limited effort in an effective way; timing software releases; and creating a market. We discuss how these organisational realities shape (1) requirements testing; (2) test coverage; (3) test automation; and (4) test scenario design.},
  keywords = {Automatic testing,Best practices,Computer industry,Computer science,Design automation,Shape,Software engineering,Software testing,System testing,Timing},
  file = {/Users/harrison/Zotero/storage/C43NKMFU/Martin et al. - 2007 - 'Good' Organisational Reasons for 'Bad' Software T.pdf}
}

@inproceedings{mathisLearningInputTokens2020,
  title = {Learning Input Tokens for Effective Fuzzing},
  booktitle = {{{ISSTA}} '20: 29th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}, {{Virtual Event}}, {{USA}}, {{July}} 18-22, 2020},
  author = {Mathis, Bj{\"o}rn and Gopinath, Rahul and Zeller, Andreas},
  editor = {Khurshid, Sarfraz and Pasareanu, Corina S.},
  year = {2020},
  pages = {27--37},
  publisher = {ACM},
  doi = {10.1145/3395363.3397348},
  file = {/Users/harrison/Zotero/storage/RFEF5TC8/Mathis et al. - 2020 - Learning input tokens for effective fuzzing.pdf}
}

@article{matsudaEmbeddingUnembedding2023,
  title = {Embedding by {{Unembedding}}},
  author = {Matsuda, Kazutaka and Frohlich, Samantha and Wang, Meng and Wu, Nicolas},
  year = {2023},
  month = aug,
  journal = {Embedding by Unembedding Code Artifact},
  volume = {7},
  number = {ICFP},
  pages = {189:1--189:47},
  doi = {10.1145/3607830},
  urldate = {2024-11-15},
  abstract = {Embedding is a language development technique that implements the object language as a library in a host language.   There are many advantages of the approach, including being lightweight and the ability to inherit features of the host language. A notable example is the technique of HOAS, which makes crucial use of higher-order functions to represent abstract syntax trees with binders.   Despite its popularity, HOAS has its limitations.   We observe that HOAS struggles with semantic domains that cannot be naturally expressed as functions, particularly when open expressions are involved.   Prominent examples of this include incremental computation and reversible/bidirectional languages.      In this paper, we pin-point the challenge faced by HOAS as a mismatch between the semantic domain of host and object language functions, and propose a solution.   The solution is based on the technique of unembedding, which converts   from the finally-tagless representation to de Bruijn-indexed terms with   strong correctness guarantees. We show that this approach is able to extend the   applicability of HOAS while preserving its elegance. We provide a generic   strategy for Embedding by Unembedding, and then demonstrate its effectiveness   with two substantial case studies in the domains of incremental computation   and bidirectional transformations. The resulting embedded implementations are   comparable in features to the state-of-the-art language implementations in   the respective areas.},
  file = {/Users/harrison/Zotero/storage/GQ8SN9LA/Matsuda et al. - 2023 - Embedding by Unembedding.pdf}
}

@article{matsudaFliPprSystemDeriving2018,
  title = {{{FliPpr}}: {{A System}} for {{Deriving Parsers}} from {{Pretty-Printers}}},
  author = {Matsuda, Kazutaka and Wang, Meng},
  year = {2018},
  journal = {New Generation Computing},
  volume = {36},
  number = {3},
  pages = {173--202},
  publisher = {Springer Verlag},
  issn = {0288-3635},
  doi = {10.1007/s00354-018-0033-7},
  abstract = {When implementing a programming language, we often write a parser and a pretty-printer. However, manually writing both programs is not only tedious but also error-prone; it may happen that a pretty-printed result is not correctly parsed. In this paper, we propose FliPpr, which is a program transformation system that uses program inversion to produce a CFG parser from a pretty-printer. This novel approach has the advantages of fine-grained control over pretty-printing, and easy reuse of existing efficient pretty-printer and parser implementations.},
  langid = {english},
  keywords = {Domain specific language,Language design,Parsing,Pretty-printing,Program inversion,Program transformation},
  file = {/Users/harrison/Zotero/storage/N484ICTL/Matsuda and Wang - 2018 - FliPpr A System for Deriving Parsers from Pretty-.pdf}
}

@inproceedings{mayerUserInteractionModels2015,
  title = {User {{Interaction Models}} for {{Disambiguation}} in {{Programming}} by {{Example}}},
  booktitle = {{\textbackslash}confuist},
  author = {Mayer, Mika{\"e}l and Soares, Gustavo and Grechkin, Maxim and Le, Vu and Marron, Mark and Polozov, Oleksandr and Singh, Rishabh and Zorn, Benjamin and Gulwani, Sumit},
  year = {2015},
  pages = {291--301},
  publisher = {{\textbackslash}pubacm}
}

@article{mcbrideApplicativeProgrammingEffects2008,
  title = {Applicative Programming with Effects},
  author = {McBride, Conor and Paterson, Ross},
  year = {2008},
  journal = {Journal of functional programming},
  volume = {18},
  number = {1},
  pages = {1--13},
  publisher = {Cambridge University Press},
  doi = {10.1017/S0956796807006326},
  file = {/Users/harrison/Zotero/storage/6BVN228E/Mcbride and Paterson - 2008 - Applicative programming with effects.pdf;/Users/harrison/Zotero/storage/BT6A5KZF/C80616ACD5687ABDC86D2B341E83D298.html}
}

@inproceedings{mcbrideTuringCompletenessTotallyFree2015,
  title = {Turing-{{Completeness Totally Free}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {McBride, Conor},
  editor = {Hinze, Ralf and Voigtl{\"a}nder, Janis},
  year = {2015},
  pages = {257--275},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-19797-5_13},
  abstract = {In this paper, I show that general recursive definitions can be represented in the free monad which supports the `effect' of making a recursive call, without saying how these calls should be executed. Diverse semantics can be given within a total framework by suitable monad morphisms. The Bove-Capretta construction of the domain of a general recursive function can be presented datatype-generically as an instance of this technique. The paper is literate Agda, but its key ideas are more broadly transferable.},
  isbn = {978-3-319-19797-5},
  langid = {english},
  keywords = {False Confession,General Recursion,Recursive Call,Recursive Definition,Strong Bisimulation},
  file = {/Users/harrison/Zotero/storage/G8HTIYAG/McBride - 2015 - Turing-Completeness Totally Free.pdf}
}

@inproceedings{mcgrathBifrostVisualizingChecking2017,
  title = {Bifr{\"o}st: {{Visualizing}} and Checking Behavior of Embedded Systems across Hardware and Software},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {McGrath, Will and Drew, Daniel and Warner, Jeremy and Kazemitabaar, Majeed and Karchemsky, Mitchell and Mellis, David and Hartmann, Bj{\"o}rn},
  year = {2017},
  pages = {299--310}
}

@inproceedings{mcgrathWifrostBridgingInformation2018,
  title = {Wifr{\"o}st: {{Bridging}} the Information Gap for Debugging of Networked Embedded Systems},
  booktitle = {Proceedings of the 31st {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {McGrath, William and Warner, Jeremy and Karchemsky, Mitchell and Head, Andrew and Drew, Daniel and Hartmann, Bjoern},
  year = {2018},
  pages = {447--455}
}

@article{mcknightMannWhitneyTest2010,
  title = {Mann-{{Whitney U Test}}},
  author = {McKnight, Patrick E and Najab, Julius},
  year = {2010},
  journal = {The Corsini encyclopedia of psychology},
  pages = {1--1},
  publisher = {Wiley Online Library}
}

@inproceedings{mcnuttIntegratedVisualizationEditing2021,
  title = {Integrated {{Visualization Editing}} via {{Parameterized Declarative Templates}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {McNutt, Andrew M and Chugh, Ravi},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--14},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3411764.3445356},
  urldate = {2024-03-16},
  abstract = {Interfaces for creating visualizations typically embrace one of several common forms. Textual specification enables fine-grained control, shelf building facilitates rapid exploration, while chart choosing promotes immediacy and simplicity. Ideally these approaches could be unified to integrate the user- and usage-dependent benefits found in each modality, yet these forms remain distinct. We propose parameterized declarative templates, a simple abstraction mechanism over JSON-based visualization grammars, as a foundation for multimodal visualization editors. We demonstrate how templates can facilitate organization and reuse by factoring the more than 160 charts that constitute Vega-Lite's example gallery into approximately 40 templates. We exemplify the pliability of abstracting over charting grammars by implementing---as a template---the functionality of the shelf builder Polestar (a simulacra of Tableau) and a set of templates that emulate the Google Sheets chart chooser. We show how templates support multimodal visualization editing by implementing a prototype and evaluating it through an approachability study.},
  isbn = {978-1-4503-8096-6},
  keywords = {Declarative Grammars,Information Visualization,Ivy,Systems,Templates,User Interfaces},
  file = {/Users/harrison/Zotero/storage/DSXJNM6B/McNutt and Chugh - 2021 - Integrated Visualization Editing via Parameterized.pdf}
}

@article{meyerTodayWasGood2019,
  title = {Today Was a Good Day: {{The}} Daily Life of Software Developers},
  author = {Meyer, Andr{\'e} N and Barr, Earl T and Bird, Christian and Zimmermann, Thomas},
  year = {2019},
  journal = {IEEE Transactions on Software Engineering},
  volume = {47},
  number = {5},
  pages = {863--880},
  publisher = {IEEE}
}

@misc{microsoftVisualStudioCode2024,
  title = {Visual {{Studio Code}}},
  author = {Microsoft},
  year = {2024}
}

@article{millerEmpiricalStudyReliability1990,
  title = {An Empirical Study of the Reliability of {{UNIX}} Utilities},
  author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  year = {1990},
  month = dec,
  journal = {Communications of the ACM},
  volume = {33},
  number = {12},
  pages = {32--44},
  issn = {0001-0782},
  doi = {10.1145/96267.96279},
  urldate = {2023-10-26},
  abstract = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
  file = {/Users/harrison/Zotero/storage/IB3LXB2G/Miller et al. - 1990 - An empirical study of the reliability of UNIX util.pdf}
}

@inproceedings{millerOutlierFindingFocusing2001,
  title = {Outlier Finding: {{Focusing}} User Attention on Possible Errors},
  booktitle = {Proceedings of the 14th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Miller, Robert C and Myers, Brad A},
  year = {2001},
  pages = {81--90}
}

@misc{minskySignalsThreads2023,
  title = {Signals and {{Threads}}},
  shorttitle = {Signals and {{Threads}}},
  author = {Minsky, Yaron},
  year = {2023},
  urldate = {2023-06-20},
  abstract = {A technology podcast, from Jane Street. Get a peek at how Jane Street approaches technology, and how those ideas relate to the tech landscape more broadly.},
  howpublished = {https://signalsandthreads.com/},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/HEZGXVII/signalsandthreads.com.html}
}

@misc{minskyTestingExpectations2015,
  title = {Testing with Expectations},
  author = {Minsky},
  year = {2015},
  month = dec,
  journal = {Jane Street Tech Blog},
  urldate = {2023-06-27},
  abstract = {Testing is important, and it's hard to get people to do as much of it as theyshould. Testing tools matter because the smoother the process is, the more tests...},
  howpublished = {https://blog.janestreet.com/testing-with-expectations/},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/AWIDK4PM/testing-with-expectations.html}
}

@inproceedings{mistaBranchingProcessesQuickCheck2018,
  title = {Branching Processes for {{QuickCheck}} Generators},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN International Symposium}} on {{Haskell}}, {{Haskell}}@{{ICFP}} 2018, {{St}}. {{Louis}}, {{MO}}, {{USA}}, {{September}} 27-17, 2018},
  author = {Mista, Agust{\'i}n and Russo, Alejandro and Hughes, John},
  editor = {Wu, Nicolas},
  year = {2018},
  pages = {1--13},
  publisher = {ACM},
  doi = {10.1145/3242744.3242747},
  file = {/Users/harrison/Zotero/storage/2SKS636Y/Mista et al. - 2018 - Branching processes for QuickCheck generators.pdf}
}

@inproceedings{mistaDerivingCompositionalRandom2021,
  title = {Deriving Compositional Random Generators},
  booktitle = {Proceedings of the 31st {{Symposium}} on {{Implementation}} and {{Application}} of {{Functional Languages}}},
  author = {Mista, Agust{\'i}n and Russo, Alejandro},
  year = {2021},
  month = jul,
  series = {{{IFL}} '19},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3412932.3412943},
  urldate = {2022-11-21},
  abstract = {Generating good random values described by algebraic data types is often quite intricate. State-of-the-art tools for synthesizing random generators serve the valuable purpose of helping with this task, while providing different levels of invariants imposed over the generated values. However, they are often not built for composability nor extensibility, a useful feature when the shape of our random data needs to be adapted while testing different properties or sub-systems. In this work, we develop an extensible framework for deriving compositional generators, which can be easily combined in different ways in order to fit developers' demands using a simple type-level description language. Our framework relies on familiar ideas from the {\`a} la Carte technique for writing composable interpreters in Haskell. In particular, we adapt this technique with the machinery required in the scope of random generation, showing how concepts like generation frequency or terminal constructions can also be expressed in the same type-level fashion. We provide an implementation of our ideas, and evaluate its performance using real-world examples.},
  isbn = {978-1-4503-7562-7},
  file = {/Users/harrison/Zotero/storage/IBLKSPIL/Mista and Russo - 2021 - Deriving compositional random generators.pdf}
}

@inproceedings{mistaMUTAGENFasterMutationBased2021,
  title = {{{MUTAGEN}}: {{Faster Mutation-Based Random Testing}}},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd {{International Conference}} on {{Software Engineering}}: {{Companion Proceedings}} ({{ICSE-Companion}})},
  author = {Mista, Agust{\'i}n},
  year = {2021},
  pages = {120--122},
  publisher = {IEEE},
  file = {/Users/harrison/Zotero/storage/THN24VZV/Mista - 2021 - MUTAGEN Faster Mutation-Based Random Testing.pdf;/Users/harrison/Zotero/storage/52SHNHS2/9402642.html}
}

@inproceedings{mistaMUTAGENReliableCoverageGuided2023,
  title = {{{MUTAGEN}}: {{Reliable Coverage-Guided}}, {{Property-Based Testing}} Using {{Exhaustive Mutations}}},
  shorttitle = {{{MUTAGEN}}},
  booktitle = {2023 {{IEEE Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}} ({{ICST}})},
  author = {Mista, Agust{\'i}n and Russo, Alejandro},
  year = {2023},
  month = apr,
  pages = {176--187},
  issn = {2159-4848},
  doi = {10.1109/ICST57152.2023.00025},
  abstract = {Automatically-synthesized random data generators are an appealing option when using property-based testing. There exists a variety of techniques that extract static information from the codebase to produce random test cases. Unfortunately, such techniques cannot enforce the complex invariants often needed to test properties with sparse preconditions.Coverage-guided, property-based testing (CGPT) tackles this limitation by enhancing synthesized generators with structure-preserving mutations guided by execution traces. Albeit effective, CGPT relies largely on randomness and exhibits poor scheduling, which can prevent bugs from being found.We present MUTAGEN, a CGPT framework that tackles such limitations by generating mutants exhaustively. Our tool incorporates heuristics that help to minimize scalability issues as well as cover the search space in a principled manner. Our evaluation shows that MUTAGEN not only outperforms existing CGPT tools but also finds previously unknown bugs in real-world software.},
  keywords = {Computer bugs,Data mining,Generators,heuristics,mutations,random testing,Scalability,Software,Software reliability,Software testing},
  file = {/Users/harrison/Zotero/storage/6ANINXVF/Mista and Russo - 2023 - MUTAGEN Reliable Coverage-Guided, Property-Based .pdf;/Users/harrison/Zotero/storage/IN55T3F5/10132245.html}
}

@article{moggiNotionsComputationMonads1991,
  title = {Notions of Computation and Monads},
  author = {Moggi, Eugenio},
  year = {1991},
  month = jul,
  journal = {Information and Computation},
  series = {Selections from 1989 {{IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  volume = {93},
  number = {1},
  pages = {55--92},
  issn = {0890-5401},
  doi = {10.1016/0890-5401(91)90052-4},
  urldate = {2022-11-22},
  abstract = {The {$\lambda$}-calculus is considered a useful mathematical tool in the study of programming languages, since programs can be identified with {$\lambda$}-terms. However, if one goes further and uses {$\beta\eta$}-conversion to prove equivalence of programs, then a gross simplification is introduced (programs are identified with total functions from values to values) that may jeopardise the applicability of theoretical results. In this paper we introduce calculi, based on a categorical semantics for computations, that provide a correct basis for proving equivalence of programs for a wide range of notions of computation.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/2V5XRUTM/Moggi - 1991 - Notions of computation and monads.pdf;/Users/harrison/Zotero/storage/SLYC2AJX/0890540191900524.html}
}

@article{montesiChoreographicProgramming,
  title = {"{{Choreographic Programming}}"},
  author = {Montesi, Fabrizio},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/NI7EXK7P/Montesi - Choreographic Programming.pdf}
}

@inproceedings{mordvinovSynchronizingConstrainedHorn,
  title = {Synchronizing {{Constrained Horn Clauses}}},
  booktitle = {{{LPAR-21}}. 21st {{International Conference}} on {{Logic}} for {{Programming}}, {{Artificial Intelligence}} and {{Reasoning}}},
  author = {Mordvinov, Dmitry and Fedyukovich, Grigory},
  pages = {338--319},
  doi = {10.29007/gr5c},
  urldate = {2024-07-03},
  abstract = {Simultaneous occurrences of multiple recurrence relations in a system of non-linear constrained Horn clauses are crucial for proving its satisfiability. A solution of such system is often inexpressible in the constraint language. We propose to synchronize recurrent computations, thus increasing the chances for a solution to be found. We introduce a notion of CHC product allowing to formulate a lightweight iterative algorithm of merging recurrent computations into groups and prove its soundness. The evaluation over a set of systems handling lists and linear integer arithmetic confirms that the transformed systems are drastically more simple to solve than the original ones.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/64MZ6X3Y/Mordvinov and Fedyukovich - Synchronizing Constrained Horn Clauses.pdf}
}

@inproceedings{mouraLean4Theorem2021,
  title = {The {{Lean}} 4 {{Theorem Prover}} and {{Programming Language}}},
  booktitle = {Automated {{Deduction}} -- {{CADE}} 28: 28th {{International Conference}} on {{Automated Deduction}}, {{Virtual Event}}, {{July}} 12--15, 2021, {{Proceedings}}},
  author = {de Moura, Leonardo and Ullrich, Sebastian},
  year = {2021},
  month = jul,
  pages = {625--635},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-79876-5_37},
  urldate = {2024-12-28},
  abstract = {Lean 4 is a reimplementation of the Lean interactive theorem prover (ITP) in Lean itself. It addresses many shortcomings of the previous versions and contains many new features. Lean 4 is fully extensible: users can modify and extend the parser, elaborator, tactics, decision procedures, pretty printer, and code generator. The new system has a hygienic macro system custom-built for ITPs. It contains a new typeclass resolution procedure based on tabled resolution, addressing significant performance problems reported by the growing user base. Lean 4 is also an efficient functional programming language based on a novel programming paradigm called functional but in-place. Efficient code generation is crucial for Lean users because many write custom proof automation procedures in Lean itself.},
  isbn = {978-3-030-79875-8},
  file = {/Users/harrison/Zotero/storage/AUMR8AJF/Moura and Ullrich - 2021 - The Lean 4 Theorem Prover and Programming Language.pdf}
}

@article{moyTraceContracts2023,
  title = {Trace Contracts},
  author = {Moy, Cameron and Felleisen, Matthias},
  year = {2023},
  month = jan,
  journal = {Journal of Functional Programming},
  volume = {33},
  pages = {e14},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796823000096},
  urldate = {2024-10-03},
  abstract = {Behavioral software contracts allow programmers to strengthen the obligations and promises that they express with conventional types. They lack expressive power, though, when it comes to invariants that hold across several function calls. Trace contracts narrow this expressiveness gap. A trace contract is a predicate over the sequence of values that flow through function calls and returns. This paper presents a principled design, an implementation, and an evaluation of trace contracts.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/QVPFDH6P/Moy and Felleisen - 2023 - Trace contracts.pdf}
}

@article{muduliSatisfiabilityModuloFuzzing2022,
  title = {Satisfiability modulo Fuzzing: A Synergistic Combination of {{SMT}} Solving and Fuzzing},
  shorttitle = {Satisfiability modulo Fuzzing},
  author = {Muduli, Sujit Kumar and Roy, Subhajit},
  year = {2022},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {OOPSLA2},
  pages = {169:1236--169:1263},
  doi = {10.1145/3563332},
  urldate = {2023-05-28},
  abstract = {Programming languages and software engineering tools routinely encounter components that are difficult to reason on via formal techniques or whose formal semantics are not even available---third-party libraries, inline assembly code, SIMD instructions, system calls, calls to machine learning models, etc. However, often access to these components is available as input-output oracles---interfaces are available to query these components on certain inputs to receive the respective outputs. We refer to such functions as closed-box functions. Regular SMT solvers are unable to handle such closed-box functions. We propose S{\=a}dhak, a solver for SMT theories modulo closed-box functions. Our core idea is to use a synergistic combination of a fuzzer to reason on closed-box functions and an SMT engine to solve the constraints pertaining to the SMT theories. The fuzz and the SMT engines attempt to converge to a model by exchanging a rich set of interface constraints that are relevant and interpretable by them. Our implementation, S{\=a}dhak, demonstrates a significant advantage over the only other solver that is capable of handling such closed-box constraints: S{\=a}dhak solves 36.45\% more benchmarks than the best-performing mode of this state-of-the-art solver and has 5.72x better PAR-2 score; on the benchmarks that are solved by both tools, S{\=a}dhak is (on an average) 14.62x faster.},
  keywords = {Closed-Box Function,Conflict-Driven Fuzz Loop,Fuzzing,SMT},
  file = {/Users/harrison/Zotero/storage/97JS7WC2/Muduli and Roy - 2022 - Satisfiability modulo fuzzing a synergistic combi.pdf}
}

@article{myersProgrammersAreUsers2016,
  title = {Programmers {{Are Users Too}}: {{Human-Centered Methods}} for {{Improving Programming Tools}}},
  shorttitle = {Programmers {{Are Users Too}}},
  author = {Myers, Brad A. and Ko, Amy J. and LaToza, Thomas D. and Yoon, YoungSeok},
  year = {2016},
  month = jul,
  journal = {Computer},
  volume = {49},
  number = {7},
  pages = {44--52},
  issn = {1558-0814},
  doi = {10.1109/MC.2016.200},
  urldate = {2024-01-23},
  abstract = {Human-centered methods can help researchers better understand and meet programmers' needs. Because programming is a human activity, many of these methods can be used without change. However, some programmer needs require new methods, which can also be applied to domains other than software engineering. This article features five Web extras. The video at https://youtu.be/4PH9-qi-yTQ demonstrates Azurite, an Eclipse plug-in with a selective undo feature that lets programmers more easily backtrack their code. The video at https://youtu.be/gOSlR62-rd8 describes Graphite, an Eclipse plug-in offering active code completion, a simple but powerful technique that integrates useful code-generation tools directly into the editor. The video at https://youtu.be/zyrqcYxqDtI describes HANDS, a new programming system that emphasizes usability by building on children's and beginning programmers' natural problem-solving tendencies. The video extra at https://youtu.be/80EctbI7PFc describes Whyline, a debugging tool that lets developers ask questions about their program's output and behavior. The video at https://youtu.be/3L4MK2dG\_6k demonstrates the prototype for Whyline, a debugging tool that lets developers pose questions about their program's output.},
  file = {/Users/harrison/Zotero/storage/IKIAQZMY/Myers et al. - 2016 - Programmers Are Users Too Human-Centered Methods .pdf;/Users/harrison/Zotero/storage/MVRGFLRD/7503516.html}
}

@article{nagyResearchPracticeFun2023,
  title = {Research for {{Practice}}: {{The Fun}} in {{Fuzzing}}},
  shorttitle = {Research for {{Practice}}},
  author = {Nagy, Stefan},
  year = {2023},
  month = may,
  journal = {Communications of the ACM},
  volume = {66},
  number = {5},
  pages = {48--50},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3588045},
  urldate = {2023-04-25},
  abstract = {The debugging technique comes into its own.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/BP9QM9VA/Nagy - 2023 - Research for Practice The Fun in Fuzzing.pdf}
}

@inproceedings{naikSporqInteractiveEnvironment2021,
  title = {Sporq: {{An}} Interactive Environment for Exploring Code Using Query-by-Example},
  booktitle = {The 34th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Naik, Aaditya and Mendelson, Jonathan and Sands, Nathaniel and Wang, Yuepeng and Naik, Mayur and Raghothaman, Mukund},
  year = {2021},
  pages = {84--99}
}

@article{nelsonAutomatedTargetedTesting2021,
  title = {Automated, {{Targeted Testing}} of {{Property-Based Testing Predicates}}},
  author = {Nelson, Tim and Rivera, Elijah and Soucie, Sam and Del Vecchio, Thomas and Wrenn, John and Krishnamurthi, Shriram},
  year = {2021},
  month = nov,
  journal = {The Art, Science, and Engineering of Programming},
  volume = {6},
  number = {2},
  eprint = {2111.10414},
  primaryclass = {cs},
  pages = {10},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2022/6/10},
  urldate = {2022-12-17},
  abstract = {Context: This work is based on property-based testing (PBT). PBT is an increasingly important form of software testing. Furthermore, it serves as a concrete gateway into the abstract area of formal methods. Specifically, we focus on students learning PBT methods. Inquiry: How well do students do at PBT? Our goal is to assess the quality of the predicates they write as part of PBT. Prior work introduced the idea of decomposing the predicate's property into a conjunction of independent subproperties. Testing the predicate against each subproperty gives a "semantic" understanding of their performance. Approach: The notion of independence of subproperties both seems intuitive and was an important condition in prior work. First, we show that this condition is overly restrictive and might hide valuable information: it both undercounts errors and makes it hard to capture misconceptions. Second, we introduce two forms of automation, one based on PBT tools and the other on SAT-solving, to enable testing of student predicates. Third, we compare the output of these automated tools against manually-constructed tests. Fourth, we also measure the performance of those tools. Finally, we re-assess student performance reported in prior work. Knowledge: We show the difficulty caused by the independent subproperty requirement. We provide insight into how to use automation effectively to assess PBT predicates. In particular, we discuss the steps we had to take to beat human performance. We also provide insight into how to make the automation work efficiently. Finally, we present a much richer account than prior work of how students did. Grounding: Our methods are grounded in mathematical logic. We also make use of well-understood principles of test generation from more formal specifications. This combination ensures the soundness of our work. We use standard methods to measure performance. Importance: As both educators and programmers, we believe PBT is a valuable tool for students to learn, and its importance will only grow as more developers appreciate its value. Effective teaching requires a clear understanding of student knowledge and progress. Our methods enable a rich and automated analysis of student performance on PBT that yields insight into their understanding and can capture misconceptions. We therefore expect these results to be valuable to educators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/harrison/Zotero/storage/PSAUZJJ5/Nelson et al. - 2021 - Automated, Targeted Testing of Property-Based Test.pdf;/Users/harrison/Zotero/storage/DNPK6XXT/2111.html}
}

@phdthesis{nemethCatamorphismbasedProgramTransformations2000,
  title = {Catamorphism-Based Program Transformations for Non-Strict Functional Languages},
  author = {N{\'e}meth, L{\'a}szl{\'o}},
  year = {2000},
  urldate = {2024-09-22},
  abstract = {In functional languages intermediate data structures are often used as glue to connect separate parts of a program together. These intermediate data structures are useful because they allow modularity, but they are also a cause of inefficiency: each element need to be allocated, to be examined, and to be deallocated. Warm fusion is a program transformation technique which aims to eliminate intermediate data structures. Functions in a program are first transformed into the so called build-cata form, then fused via a one-step rewrite rule, the cata-build rule. In the process of the transformation to build-cata form we attempt to replace explicit recursion with a fixed pattern of recursion (catamorphism). We analyse in detail the problem of removing - possibly mutually recursive sets of - polynomial datatypes. Wehave implemented the warm fusion method in the Glasgow Haskell Compiler, which has allowed practical feedback. One important conclusion is that catamorphisms and fusion in general deserve a more prominent role in the compilation process. We give a detailed measurement of our implementation on a suite of real application programs.},
  langid = {english},
  school = {University of Glasgow},
  file = {/Users/harrison/Zotero/storage/Z5HKT4E4/Németh - 2000 - Catamorphism-based program transformations for non.pdf;/Users/harrison/Zotero/storage/NC6T4E6F/4612.html}
}

@inproceedings{ngMicaAutomatedDifferential2024,
  title = {Mica: {{Automated Differential Testing}} for {{OCaml Modules}}},
  shorttitle = {Mica},
  booktitle = {{{OCaml}}'24},
  author = {Ng, Ernest and Goldstein, Harrison and Pierce, Benjamin C.},
  year = {2024},
  month = aug,
  eprint = {2408.14561},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2408.14561},
  urldate = {2024-09-18},
  abstract = {Suppose we are given two OCaml modules implementing the same signature. How do we check that they are observationally equivalent -- that is, that they behave the same on all inputs? One established technique is to use a property-based testing (PBT) tool such as QuickCheck. Currently, however, this can require significant amounts of boilerplate code and ad-hoc test harnesses. To address this issue, we present Mica, an automated tool for testing observational equivalence of OCaml modules. Mica is implemented as a PPX compiler extension, allowing users to supply minimal annotations to a module signature. These annotations guide Mica to automatically derive specialized PBT code that checks observational equivalence. We discuss the design of Mica and demonstrate its efficacy as a testing tool on various modules taken from real-world OCaml libraries.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/Users/harrison/Zotero/storage/M553NTBM/Ng et al. - 2024 - Mica Automated Differential Testing for OCaml Mod.pdf;/Users/harrison/Zotero/storage/XYCWKZV3/2408.html}
}

@article{nieSurveyCombinatorialTesting2011,
  title = {A {{Survey}} of {{Combinatorial Testing}}},
  author = {Nie, Changhai and Leung, Hareton},
  year = {2011},
  month = feb,
  journal = {ACM Comput. Surv.},
  volume = {43},
  number = {2},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/1883612.1883618},
  keywords = {combinatorial testing,covering array,Software testing,test case generation},
  file = {/Users/harrison/Zotero/storage/QES82AAM/Nie and Leung - 2011 - A Survey of Combinatorial Testing.pdf}
}

@misc{nilssonScalaCheck2024,
  title = {{{ScalaCheck}}},
  author = {Nilsson, Rickard},
  year = {2024},
  urldate = {2024-02-26},
  howpublished = {https://scalacheck.org/},
  file = {/Users/harrison/Zotero/storage/MNPPN6TS/scalacheck.org.html}
}

@inproceedings{niRecodeLightweightFindandreplace2021,
  title = {Recode: {{A}} Lightweight Find-and-Replace Interaction in the Ide for Transforming Code by Example},
  booktitle = {The 34th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Ni, Wode and Sunshine, Joshua and Le, Vu and Gulwani, Sumit and Barik, Titus},
  year = {2021},
  pages = {258--269}
}

@book{norman1986user,
  title = {User Centered System Design; New Perspectives on Human-Computer Interaction},
  author = {Norman, Donald A and Draper, Stephen W},
  year = {1986},
  publisher = {L. Erlbaum Associates Inc.}
}

@misc{ocamlsoftwarefoundationOCamlUserSurvey2022,
  title = {{{OCaml User Survey}}},
  author = {OCaml Software Foundation},
  year = {2022},
  urldate = {2023-06-07},
  howpublished = {https://ocaml-sf.org/docs/2022/ocaml-user-survey-2022.pdf},
  file = {/Users/harrison/Zotero/storage/LW2TENPA/ocaml-user-survey-2022.pdf}
}

@inproceedings{oconnorQuickstromPropertybasedAcceptance2022,
  title = {Quickstrom: Property-Based Acceptance Testing with {{LTL}} Specifications},
  shorttitle = {Quickstrom},
  booktitle = {Proceedings of the 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {O'Connor, Liam and Wickstr{\"o}m, Oskar},
  year = {2022},
  month = jun,
  series = {{{PLDI}} 2022},
  pages = {1025--1038},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3519939.3523728},
  urldate = {2022-12-04},
  abstract = {We present Quickstrom, a property-based testing system for acceptance testing of interactive applications. Using Quickstrom, programmers can specify the behaviour of web applications as properties in our testing-oriented dialect of Linear Temporal Logic (LTL) called QuickLTL, and then automatically test their application against the given specification with hundreds of automatically generated interactions. QuickLTL extends existing finite variants of LTL for the testing use-case, determining likely outcomes from partial traces whose minimum length is itself determined by the LTL formula. This temporal logic is embedded in our specification language, Specstrom, which is designed to be approachable to web programmers, expressive for writing specifications, and easy to analyse. Because Quickstrom tests only user-facing behaviour, it is agnostic to the implementation language of the system under test. We therefore formally specify and test many implementations of the popular TodoMVC benchmark, used for evaluation and comparison across various web frontend frameworks and languages. Our tests uncovered bugs in almost half of the available implementations.},
  isbn = {978-1-4503-9265-5},
  keywords = {linear temporal logic,property-based testing,web applications},
  file = {/Users/harrison/Zotero/storage/CMXTFMF6/O'Connor and Wickström - 2022 - Quickstrom property-based acceptance testing with.pdf}
}

@inproceedings{olsenEvaluatingUserInterface2007,
  title = {Evaluating User Interface Systems Research},
  booktitle = {Proceedings of the 20th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Olsen, Dan R.},
  year = {2007},
  month = oct,
  series = {{{UIST}} '07},
  pages = {251--258},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1294211.1294256},
  urldate = {2023-06-16},
  abstract = {The development of user interface systems has languished with the stability of desktop computing. Future systems, however, that are off-the-desktop, nomadic or physical in nature will involve new devices and new software systems for creating interactive applications. Simple usability testing is not adequate for evaluating complex systems. The problems with evaluating systems work are explored and a set of criteria for evaluating new UI systems work is presented.},
  isbn = {978-1-59593-679-0},
  keywords = {user interface systems evaluation},
  file = {/Users/harrison/Zotero/storage/WGGH6ZPZ/Olsen - 2007 - Evaluating user interface systems research.pdf}
}

@inproceedings{omarActiveCodeCompletion2012,
  title = {Active Code Completion},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Omar, Cyrus and Yoon, Young Seok and LaToza, Thomas D. and Myers, Brad A.},
  year = {2012},
  month = jun,
  pages = {859--869},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2012.6227133},
  abstract = {Code completion menus have replaced standalone API browsers for most developers because they are more tightly integrated into the development workflow. Refinements to the code completion menu that incorporate additional sources of information have similarly been shown to be valuable, even relative to standalone counterparts offering similar functionality. In this paper, we describe active code completion, an architecture that allows library developers to introduce interactive and highly-specialized code generation interfaces, called palettes, directly into the editor. Using several empirical methods, we examine the contexts in which such a system could be useful, describe the design constraints governing the system architecture as well as particular code completion interfaces, and design one such system, named Graphite, for the Eclipse Java development environment. Using Graphite, we implement a palette for writing regular expressions as our primary example and conduct a small pilot study. In addition to showing the feasibility of this approach, it provides further evidence in support of the claim that integrating specialized code completion interfaces directly into the editor is valuable to professional developers.},
  keywords = {code completion,Computer architecture,Databases,development environments,Image color analysis,Java,Standards,Syntactics},
  file = {/Users/harrison/Zotero/storage/8Q2CVMGQ/Omar et al. - 2012 - Active code completion.pdf}
}

@inproceedings{omarFillingTypedHoles2021,
  title = {Filling Typed Holes with Live {{GUIs}}},
  booktitle = {Proceedings of the 42nd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Omar, Cyrus and Moon, David and Blinn, Andrew and Voysey, Ian and Collins, Nick and Chugh, Ravi},
  year = {2021},
  month = jun,
  series = {{{PLDI}} 2021},
  pages = {511--525},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3453483.3454059},
  urldate = {2022-12-21},
  abstract = {Text editing is powerful, but some types of expressions are more naturally represented and manipulated graphically. Examples include expressions that compute colors, music, animations, tabular data, plots, diagrams, and other domain-specific data structures. This paper introduces live literals, or livelits, which allow clients to fill holes of types like these by directly manipulating a user-defined GUI embedded persistently into code. Uniquely, livelits are compositional: a livelit GUI can itself embed spliced expressions, which are typed, lexically scoped, and can in turn embed other livelits. Livelits are also uniquely live: a livelit can provide continuous feedback about the run-time implications of the client's choices even when splices mention bound variables, because the system continuously gathers closures associated with the hole that the livelit is filling. We integrate livelits into Hazel, a live hole-driven programming environment, and describe case studies that exercise these novel capabilities. We then define a simply typed livelit calculus, which specifies how livelits operate as live graphical macros. The metatheory of macro expansion has been mechanized in Agda.},
  isbn = {978-1-4503-8391-2},
  keywords = {GUIs,live programming,macros,typed holes},
  file = {/Users/harrison/Zotero/storage/3ZGZHFAH/Omar et al. - 2021 - Filling typed holes with live GUIs.pdf}
}

@article{omarLiveFunctionalProgramming2019,
  title = {Live Functional Programming with Typed Holes},
  author = {Omar, Cyrus and Voysey, Ian and Chugh, Ravi and Hammer, Matthew A.},
  year = {2019},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {POPL},
  pages = {14:1--14:32},
  doi = {10.1145/3290327},
  urldate = {2022-12-21},
  abstract = {Live programming environments aim to provide programmers (and sometimes audiences) with continuous feedback about a program's dynamic behavior as it is being edited. The problem is that programming languages typically assign dynamic meaning only to programs that are complete, i.e. syntactically well-formed and free of type errors. Consequently, live feedback presented to the programmer exhibits temporal or perceptive gaps. This paper confronts this "gap problem" from type-theoretic first principles by developing a dynamic semantics for incomplete functional programs, starting from the static semantics for incomplete functional programs developed in recent work on Hazelnut. We model incomplete functional programs as expressions with holes, with empty holes standing for missing expressions or types, and non-empty holes operating as membranes around static and dynamic type inconsistencies. Rather than aborting when evaluation encounters any of these holes as in some existing systems, evaluation proceeds around holes, tracking the closure around each hole instance as it flows through the remainder of the program. Editor services can use the information in these hole closures to help the programmer develop and confirm their mental model of the behavior of the complete portions of the program as they decide how to fill the remaining holes. Hole closures also enable a fill-and-resume operation that avoids the need to restart evaluation after edits that amount to hole filling. Formally, the semantics borrows machinery from both gradual type theory (which supplies the basis for handling unfilled type holes) and contextual modal type theory (which supplies a logical basis for hole closures), combining these and developing additional machinery necessary to continue evaluation past holes while maintaining type safety. We have mechanized the metatheory of the core calculus, called Hazelnut Live, using the Agda proof assistant. We have also implemented these ideas into the Hazel programming environment. The implementation inserts holes automatically, following the Hazelnut edit action calculus, to guarantee that every editor state has some (possibly incomplete) type. Taken together with this paper's type safety property, the result is a proof-of-concept live programming environment where rich dynamic feedback is truly available without gaps, i.e. for every reachable editor state.},
  keywords = {contextual modal type theory,gradual typing,live programming,structured editing,typed holes},
  file = {/Users/harrison/Zotero/storage/IAJQQ82I/Omar et al. - 2019 - Live functional programming with typed holes.pdf}
}

@inproceedings{oneyFireCrystalUnderstandingInteractive2009,
  title = {{{FireCrystal}}: {{Understanding}} Interactive Behaviors in Dynamic Web Pages},
  shorttitle = {{{FireCrystal}}},
  booktitle = {2009 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  author = {Oney, Stephen and Myers, Brad},
  year = {2009},
  month = sep,
  pages = {105--108},
  issn = {1943-6106},
  doi = {10.1109/VLHCC.2009.5295287},
  abstract = {For developers debugging their own code, augmenting the code of others, or trying to learn the implementation details of interactive behaviors, understanding how web pages work is a fundamental problem. FireCrystal is a new Firefox extension that allows developers to indicate interactive behaviors of interest, and shows the specific code (Javascript, CSS, and HTML) that is responsible for those behaviors. FireCrystal provides an execution timeline that users can scrub back and forth, and the ability to select items of interest in the actual web page UI to see the associated code. FireCrystal may be especially useful for developers who are trying to learn the implementation details of interactive behaviors, so they can reuse these behaviors in their own web site.},
  keywords = {Cascading style sheets,Debugging,Formal languages,HTML,Internet,Java,Mice,Programming profession,Web page design,Web pages},
  file = {/Users/harrison/Zotero/storage/96UW4UJE/Oney and Myers - 2009 - FireCrystal Understanding interactive behaviors i.pdf;/Users/harrison/Zotero/storage/J44P4PQV/5295287.html}
}

@misc{openaiChatGPT2024,
  title = {{{ChatGPT}}},
  author = {OpenAI},
  year = {2024},
  urldate = {2024-04-23},
  abstract = {A conversational AI system that listens, learns, and challenges},
  howpublished = {https://chat.openai.com},
  langid = {american},
  file = {/Users/harrison/Zotero/storage/PYITUQZ5/chat.openai.com.html}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {{OpenAI}},
  year = {2023}
}

@inproceedings{oseraConstraintbasedTypedirectedProgram2019,
  title = {Constraint-Based Type-Directed Program Synthesis},
  booktitle = {Proceedings of the 4th {{ACM SIGPLAN International Workshop}} on {{Type-Driven Development}}},
  author = {Osera, Peter-Michael},
  year = {2019},
  month = aug,
  series = {{{TyDe}} 2019},
  pages = {64--76},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3331554.3342608},
  urldate = {2022-12-02},
  abstract = {We explore an approach to type-directed program synthesis rooted in constraint-based type inference techniques. By doing this, we aim to more efficiently synthesize polymorphic code while also tackling advanced typing features such as GADTs that build upon polymorphism. Along the way, we also present an implementation of these techniques in Scythe, a prototype live, type-directed programming tool for the Haskell programming language and reflect on our initial experience with the tool.},
  isbn = {978-1-4503-6815-5},
  keywords = {Functional Programming,Program Synthesis,Type Inference,Type Theory},
  file = {/Users/harrison/Zotero/storage/37SWHKVA/Osera - 2019 - Constraint-based type-directed program synthesis.pdf}
}

@article{oseraTypeandexampledirectedProgramSynthesis2015,
  title = {Type-and-Example-Directed Program Synthesis},
  author = {Osera, Peter-Michael and Zdancewic, Steve},
  year = {2015},
  month = jun,
  journal = {ACM SIGPLAN Notices},
  volume = {50},
  number = {6},
  pages = {619--630},
  issn = {0362-1340},
  doi = {10.1145/2813885.2738007},
  urldate = {2024-01-23},
  abstract = {This paper presents an algorithm for synthesizing recursive functions that process algebraic datatypes. It is founded on proof-theoretic techniques that exploit both type information and input--output examples to prune the search space. The algorithm uses refinement trees, a data structure that succinctly represents constraints on the shape of generated code. We evaluate the algorithm by using a prototype implementation to synthesize more than 40 benchmarks and several non-trivial larger examples. Our results demonstrate that the approach meets or outperforms the state-of-the-art for this domain, in terms of synthesis time or attainable size of the generated programs.},
  keywords = {Functional Programming,Program Syn- thesis,Proof Search,Type Theory},
  file = {/Users/harrison/Zotero/storage/4G5HGFVX/Osera and Zdancewic - 2015 - Type-and-example-directed program synthesis.pdf}
}

@misc{otter.aiOtterAiVoice2023,
  title = {Otter.Ai - {{Voice Meeting Notes}} \& {{Real-time Transcription}}},
  author = {Otter.ai},
  year = {2023},
  journal = {Otter.ai},
  urldate = {2023-06-24},
  abstract = {Otter.ai uses AI to write automatic meeting notes with real-time transcription, recorded audio, automated slide capture, and automated meeting summaries.},
  howpublished = {https://otter.ai/},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/E7Q3MKAE/otter.ai.html}
}

@article{padhyeFuzzFactoryDomainspecificFuzzing2019,
  title = {{{FuzzFactory}}: Domain-Specific Fuzzing with Waypoints},
  author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Simon, Laurent and Vijayakumar, Hayawardh},
  year = {2019},
  journal = {Proc. ACM Program. Lang.},
  volume = {3},
  number = {OOPSLA},
  pages = {174:1--174:29},
  doi = {10.1145/3360600},
  keywords = {domain-specific fuzzing,frameworks,fuzz testing,waypoints},
  file = {/Users/harrison/Zotero/storage/8HNSVPT8/Padhye et al. - 2019 - FuzzFactory domain-specific fuzzing with waypoint.pdf}
}

@article{padhyeFuzzFactoryDomainspecificFuzzing2019a,
  title = {{{FuzzFactory}}: Domain-Specific Fuzzing with Waypoints},
  shorttitle = {{{FuzzFactory}}},
  author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Simon, Laurent and Vijayakumar, Hayawardh},
  year = {2019},
  month = oct,
  journal = {Replication Package for "FuzzFactory: Domain-Specific Fuzzing with Waypoints"},
  volume = {3},
  number = {OOPSLA},
  pages = {174:1--174:29},
  doi = {10.1145/3360600},
  urldate = {2024-11-13},
  abstract = {Coverage-guided fuzz testing has gained prominence as a highly effective method of finding security vulnerabilities such as buffer overflows in programs that parse binary data. Recently, researchers have introduced various specializations to the coverage-guided fuzzing algorithm for different domain-specific testing goals, such as finding performance bottlenecks, generating valid inputs, handling magic-byte comparisons, etc. Each such solution can require non-trivial implementation effort and produces a distinct variant of a fuzzing tool. We observe that many of these domain-specific solutions follow a common solution pattern.  In this paper, we present FuzzFactory, a framework for developing domain-specific fuzzing applications without requiring changes to mutation and search heuristics. FuzzFactory allows users to specify the collection of dynamic domain-specific feedback during test execution, as well as how such feedback should be aggregated. FuzzFactory uses this information to selectively save intermediate inputs, called waypoints, to augment coverage-guided fuzzing. Such waypoints always make progress towards domain-specific multi-dimensional objectives. We instantiate six domain-specific fuzzing applications using FuzzFactory: three re-implementations of prior work and three novel solutions, and evaluate their effectiveness on benchmarks from Google's fuzzer test suite. We also show how multiple domains can be composed to perform better than the sum of their parts. For example, we combine domain-specific feedback about strict equality comparisons and dynamic memory allocations, to enable the automatic generation of LZ4 bombs and PNG bombs.},
  file = {/Users/harrison/Zotero/storage/P2CLX2ZP/Padhye et al. - 2019 - FuzzFactory domain-specific fuzzing with waypoint.pdf}
}

@inproceedings{padhyeJQFCoverageguidedPropertybased2019,
  title = {{{JQF}}: Coverage-Guided Property-Based Testing in {{Java}}},
  booktitle = {Proceedings of the 28th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}, {{ISSTA}} 2019, {{Beijing}}, {{China}}, {{July}} 15-19, 2019},
  author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik},
  editor = {Zhang, Dongmei and M{\o}ller, Anders},
  year = {2019},
  pages = {398--401},
  publisher = {ACM},
  doi = {10.1145/3293882.3339002},
  file = {/Users/harrison/Zotero/storage/E2L6KAE3/Padhye et al. - 2019 - JQF coverage-guided property-based testing in Jav.pdf}
}

@inproceedings{padhyeSemanticFuzzingZest2019,
  title = {Semantic Fuzzing with Zest},
  booktitle = {Proceedings of the 28th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Papadakis, Mike and Le Traon, Yves},
  year = {2019},
  month = jul,
  series = {{{ISSTA}} 2019},
  pages = {329--340},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3293882.3330576},
  urldate = {2022-11-21},
  abstract = {Programs expecting structured inputs often consist of both a syntactic analysis stage, which parses raw input, and a semantic analysis stage, which conducts checks on the parsed input and executes the core logic of the program. Generator-based testing tools in the lineage of QuickCheck are a promising way to generate random syntactically valid test inputs for these programs. We present Zest, a technique which automatically guides QuickCheck-like random input generators to better explore the semantic analysis stage of test programs. Zest converts random-input generators into deterministic parametric input generators. We present the key insight that mutations in the untyped parameter domain map to structural mutations in the input domain. Zest leverages program feedback in the form of code coverage and input validity to perform feedback-directed parameter search. We evaluate Zest against AFL and QuickCheck on five Java programs: Maven, Ant, BCEL, Closure, and Rhino. Zest covers 1.03x-2.81x as many branches within the benchmarks' semantic analysis stages as baseline techniques. Further, we find 10 new bugs in the semantic analysis stages of these benchmarks. Zest is the most effective technique in finding these bugs reliably and quickly, requiring at most 10 minutes on average to find each bug.},
  isbn = {978-1-4503-6224-5},
  file = {/Users/harrison/Zotero/storage/8U5QM7MK/Padhye et al. - 2019 - Semantic fuzzing with zest.pdf}
}

@inproceedings{palkaTestingOptimisingCompiler2011,
  title = {Testing an {{Optimising Compiler}} by {{Generating Random Lambda Terms}}},
  booktitle = {Proceedings of the 6th {{International Workshop}} on {{Automation}} of {{Software Test}}},
  author = {Pa{\textbackslash}lka, Micha{\textbackslash}l H. and Claessen, Koen and Russo, Alejandro and Hughes, John},
  year = {2011},
  series = {{{AST}} '11},
  pages = {91--97},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1982595.1982615},
  isbn = {978-1-4503-0592-1},
  keywords = {random testing,software testing},
  file = {/Users/harrison/Zotero/storage/T97BK9HZ/Palka et al. - 2011 - Testing an Optimising Compiler by Generating Rando.pdf}
}

@incollection{papadakisChapterSixMutation2019,
  title = {Chapter {{Six}} - {{Mutation Testing Advances}}: {{An Analysis}} and {{Survey}}},
  shorttitle = {Chapter {{Six}} - {{Mutation Testing Advances}}},
  booktitle = {Advances in {{Computers}}},
  author = {Papadakis, Mike and Kintis, Marinos and Zhang, Jie and Jia, Yue and Traon, Yves Le and Harman, Mark},
  editor = {Memon, Atif M.},
  year = {2019},
  month = jan,
  volume = {112},
  pages = {275--378},
  publisher = {Elsevier},
  doi = {10.1016/bs.adcom.2018.03.015},
  urldate = {2024-02-27},
  abstract = {Mutation testing realizes the idea of using artificial defects to support testing activities. Mutation is typically used as a way to evaluate the adequacy of test suites, to guide the generation of test cases, and to support experimentation. Mutation has reached a maturity phase and gradually gains popularity both in academia and in industry. This chapter presents a survey of recent advances, over the past decade, related to the fundamental problems of mutation testing and sets out the challenges and open problems for the future development of the method. It also collects advices on best practices related to the use of mutation in empirical studies of software testing. Thus, giving the reader a ``mini-handbook''-style roadmap for the application of mutation testing as experimental methodology.},
  keywords = {Mutation testing,Seeded faults,Software testing,Survey},
  file = {/Users/harrison/Zotero/storage/A22DGBBJ/S0065245818300305.html}
}

@article{papadakisMutationTestingAdvances2018,
  title = {Mutation {{Testing Advances}}: {{An Analysis}} and {{Survey}}},
  shorttitle = {Mutation {{Testing Advances}}},
  author = {Papadakis, M. and Kintis, M. and Zhang, J. and Jia, Y. and Traon, Y. L. and Harman, M.},
  year = {2018},
  month = jan,
  journal = {Advances in Computers},
  issn = {0065-2458},
  urldate = {2022-12-07},
  abstract = {Mutation testing realizes the idea of using artificial defects to support testing activities. Mutation is typically used as a way to evaluate the adequacy of test suites, to guide the generation of test cases, and to support experimentation. Mutation has reached a maturity phase and gradually gains popularity both in academia and in industry. This chapter presents a survey of recent advances, over the past decade, related to the fundamental problems of mutation testing and sets out the challenges and open problems for the future development of the method. It also collects advices on best practices related to the use of mutation in empirical studies of software testing. Thus, giving the reader a ``mini-handbook''-style roadmap for the application of mutation testing as experimental methodology.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/BS5GFCDC/Papadakis et al. - 2019 - Mutation Testing Advances An Analysis and Survey.pdf;/Users/harrison/Zotero/storage/8L3MHP95/10056704.html}
}

@misc{paquattePytestmutagen2023,
  title = {Pytest-Mutagen},
  shorttitle = {Pytest-Mutagen},
  author = {Paquatte, Timothee and Goldstein, Harrison},
  year = {2023},
  urldate = {2023-10-05},
  copyright = {MIT License},
  keywords = {mutagen,mutant,mutation,python,Software Development - Testing,test,testing},
  file = {/Users/harrison/Zotero/storage/JVIMG669/pytest-mutagen.html}
}

@inproceedings{paraskevopoulouComputingCorrectlyInductive2022,
  title = {Computing Correctly with Inductive Relations},
  booktitle = {Proceedings of the 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Paraskevopoulou, Zoe and Eline, Aaron and Lampropoulos, Leonidas},
  year = {2022},
  month = jun,
  series = {{{PLDI}} 2022},
  pages = {966--980},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3519939.3523707},
  urldate = {2023-01-24},
  abstract = {Inductive relations are the predominant way of writing specifications in mechanized proof developments. Compared to purely functional specifications, they enjoy increased expressive power and facilitate more compositional reasoning. However, inductive relations also come with a significant drawback: they can't be used for computation. In this paper, we present a unifying framework for extracting three different kinds of computational content from inductively defined relations: semi-decision procedures, enumerators, and random generators. We show how three different instantiations of the same algorithm can be used to generate all three classes of computational definitions inside the logic of the Coq proof assistant. For each derived computation, we also derive mechanized proofs that it is sound and complete with respect to the original inductive relation, using Ltac2, Coq's new metaprogramming facility. We implement our framework on top of the QuickChick testing tool for Coq, and demonstrate that it covers most cases of interest by extracting computations for the inductive relations found in the Software Foundations series. Finally, we evaluate the practicality and the efficiency of our approach with small case studies in randomized property-based testing and proof by computational reflection.},
  isbn = {978-1-4503-9265-5},
  keywords = {Coq,enumeration,inductive relations,partial decidability,proof assistants,QuickChick,random generation},
  file = {/Users/harrison/Zotero/storage/Z2VUQ7MD/Paraskevopoulou et al. - 2022 - Computing correctly with inductive relations.pdf}
}

@inproceedings{paraskevopoulouFoundationalPropertyBasedTesting2015,
  title = {Foundational {{Property-Based Testing}}},
  booktitle = {Interactive {{Theorem Proving}}},
  author = {Paraskevopoulou, Zoe and Hri{\c t}cu, C{\u a}t{\u a}lin and D{\'e}n{\`e}s, Maxime and Lampropoulos, Leonidas and Pierce, Benjamin C.},
  editor = {Urban, Christian and Zhang, Xingyuan},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {325--343},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-22102-1_22},
  abstract = {Integrating property-based testing with a proof assistant creates an interesting opportunity: reusable or tricky testing code can be formally verified using the proof assistant itself. In this work we introduce a novel methodology for formally verified property-based testing and implement it as a foundational verification framework for QuickChick, a port of QuickCheck to Coq. Our framework enables one to verify that the executable testing code is testing the right Coq property. To make verification tractable, we provide a systematic way for reasoning about the set of outcomes a random data generator can produce with non-zero probability, while abstracting away from the actual probabilities. Our framework is firmly grounded in a fully verified implementation of QuickChick itself, using the same underlying verification methodology. We also apply this methodology to a complex case study on testing an information-flow control abstract machine, demonstrating that our verification methodology is modular and scalable and that it requires minimal changes to existing code.},
  isbn = {978-3-319-22102-1},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/JEPHXVET/Paraskevopoulou et al. - 2015 - Foundational Property-Based Testing.pdf}
}

@book{perez2019invisible,
  title = {Invisible Women: {{Data}} Bias in a World Designed for Men},
  author = {Perez, Caroline Criado},
  year = {2019},
  publisher = {Abrams}
}

@misc{petricekEncodingMonadicComputations2009,
  title = {Encoding Monadic Computations in {{C}}\# Using Iterators},
  author = {Pet{\v r}{\'i}{\v c}ek, Tom{\'a}{\v s}},
  year = {2009},
  abstract = {Many programming problems can be easily solved if we express them as computations with some non-standard aspect. This is a very important problem, because today we're struggling for example to efficiently program multi-core processors and to write asynchronous code. Unfortunately mainstream languages such as C\# don't support any direct way for encoding unrestricted non-standard computations.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/CBURRZC9/Petříček - Encoding monadic computations in C# using iterator.pdf}
}

@inproceedings{petrovicStateMutationTesting2018,
  title = {State of {{Mutation Testing}} at {{Google}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}} 2017 ({{SEIP}})},
  author = {Petrovic, Goran and Ivankovic, Marko},
  year = {2018}
}

@inproceedings{phamCreatingSharedUnderstanding2013,
  title = {Creating a Shared Understanding of Testing Culture on a Social Coding Site},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Pham, Raphael and Singer, Leif and Liskin, Olga and Filho, Fernando Figueira and Schneider, Kurt},
  year = {2013},
  month = may,
  pages = {112--121},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2013.6606557},
  abstract = {Many software development projects struggle with creating and communicating a testing culture that is appropriate for the project's needs. This may degrade software quality by leaving defects undiscovered. Previous research suggests that social coding sites such as GitHub provide a collaborative environment with a high degree of social transparency. This makes developers' actions and interactions more visible and traceable. We conducted interviews with 33 active users of GitHub to investigate how the increased transparency found on GitHub influences developers' testing behaviors. Subsequently, we validated our findings with an online questionnaire that was answered by 569 members of GitHub. We found several strategies that software developers and managers can use to positively influence the testing behavior in their projects. However, project owners on GitHub may not be aware of them. We report on the challenges and risks caused by this and suggest guidelines for promoting a sustainable testing culture in software development projects.},
  keywords = {Encoding,Guidelines,Interviews,Media,Sociology,Software,Testing},
  file = {/Users/harrison/Zotero/storage/ZUFBZ9XB/Pham et al. - 2013 - Creating a shared understanding of testing culture.pdf;/Users/harrison/Zotero/storage/QU4A2UGP/6606557.html}
}

@misc{phuongFormalAlgorithmsTransformers2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  year = {2022},
  month = jul,
  number = {arXiv:2207.09238},
  eprint = {2207.09238},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.09238},
  urldate = {2023-07-11},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/harrison/Zotero/storage/8S7H4LWQ/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf;/Users/harrison/Zotero/storage/L3ZK7CT5/2207.html}
}

@article{pickeringProfunctorOpticsModular2017,
  title = {Profunctor Optics: {{Modular}} Data Accessors},
  shorttitle = {Profunctor Optics},
  author = {Pickering, M. and Gibbons, J. and Wu, N.},
  year = {2017},
  journal = {Art, Science, and Engineering of Programming},
  volume = {1},
  number = {2},
  publisher = {Aspect-Oriented Software Association},
  issn = {2473-7321},
  urldate = {2023-01-25},
  abstract = {{$<$}p{$>$}Data accessors allow one to read and write components of a data structure, such as the fields of a record, the variants of a union, or the elements of a container. These data accessors are collectively known as optics; they are fundamental to programs that manipulate complex data. Individual data accessors for simple data structures are easy to write, for example as pairs of `getter' and `setter' methods. However, it is not obvious how to combine data accessors, in such a way that data accessors for a compound data structure are composed out of smaller data accessors for the parts of that structure. Generally, one has to write a sequence of statements or declarations that navigate step by step through the data structure, accessing one level at a time---which is to say, data accessors are traditionally not first-class citizens, combinable in their own right.{$<$}/p{$>$} {$<$}br/{$>$} {$<$}p{$>$}We present a framework for modular data access, in which individual data accessors for simple data structures may be freely combined to obtain more complex data accessors for compound data structures. Data accessors become first-class citizens. The framework is based around the notion of profunctors, a flexible generalization of functions. The language features required are higher-order functions (`lambdas' or `closures'), parametrized types (`generics' or `abstract types') of higher kind, and some mechanism for separating interfaces from implementations (`abstract classes' or `modules'). We use Haskell as a vehicle in which to present our constructions, but other languages such as Scala that provide the necessary features should work just as well. We provide implementations of all our constructions, in the form of a literate program: the manuscript file for the paper is also the source code for the program, and the extracted code is available separately for evaluation. We also prove the essential properties, demonstrating that our profunctor-based representations are precisely equivalent to the more familiar concrete representations. Our results should pave the way to simpler ways of writing programs that access the components of compound data structures.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/WAKSVACP/Pickering et al. - 2017 - Profunctor optics Modular data accessors.pdf}
}

@book{pierceTypesProgrammingLanguages2002,
  title = {Types and Programming Languages},
  author = {Pierce, Benjamin C.},
  year = {2002},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-16209-8},
  lccn = {QA76.7 .P54 2002}
}

@misc{pierceWhenWillPropertyBased,
  title = {({{When}}) {{Will Property-Based Testing Rule}} the {{World}}? {\textbar} {{SkillsCast}}},
  shorttitle = {({{When}}) {{Will Property-Based Testing Rule}} the {{World}}?},
  author = {Pierce, Benjamin C.},
  journal = {(When) Will Property-Based Testing Rule the World? {\textbar} SkillsCast},
  urldate = {2022-12-13},
  abstract = {YOW! Lambda Jam 2022 conference cast. Benjamin  Pierce:},
  howpublished = {https://skillsmatter.com/skillscasts/17525-when-will-property-based-testing-rule-the-world},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/LANZAVB5/17525-when-will-property-based-testing-rule-the-world.html}
}

@inproceedings{pikeSmartCheckAutomaticEfficient2014,
  title = {{{SmartCheck}}: Automatic and Efficient Counterexample Reduction and Generalization},
  shorttitle = {{{SmartCheck}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGPLAN}} Symposium on {{Haskell}}},
  author = {Pike, Lee},
  year = {2014},
  month = sep,
  series = {Haskell '14},
  pages = {53--64},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2633357.2633365},
  urldate = {2023-01-16},
  abstract = {QuickCheck is a powerful library for automatic test-case generation. Because QuickCheck performs random testing, some of the counterexamples discovered are very large. QuickCheck provides an interface for the user to write shrink functions to attempt to reduce the size of counter examples. Hand-written implementations of shrink can be complex, inefficient, and consist of significant boilerplate code. Furthermore, shrinking is only one aspect in debugging: counterexample generalization is the process of extrapolating from individual counterexamples to a class of counterexamples, often requiring a flash of insight from the programmer. To improve counterexample reduction and generalization, we introduce SmartCheck. SmartCheck is a debugging tool that reduces algebraic data using generic search heuristics to efficiently find smaller counterexamples. In addition to shrinking, SmartCheck also automatically generalizes counterexamples to formulas representing classes of counterexamples. SmartCheck has been implemented for Haskell and is freely available.},
  isbn = {978-1-4503-3041-1},
  keywords = {delta-debugging,property-based testing,test-case generalization},
  file = {/Users/harrison/Zotero/storage/MNIC6GBW/Pike - 2014 - SmartCheck automatic and efficient counterexample.pdf}
}

@article{plotkinAlgebraicOperationsGeneric2003,
  title = {Algebraic {{Operations}} and {{Generic Effects}}},
  author = {Plotkin, Gordon and Power, John},
  year = {2003},
  month = feb,
  journal = {Applied Categorical Structures},
  volume = {11},
  pages = {69--94},
  doi = {10.1023/A:1023064908962},
  file = {/Users/harrison/Zotero/storage/FK8NYGFF/Plotkin and Power - 2003 - Algebraic Operations and Generic Effects.pdf}
}

@inproceedings{plotkinHandlersAlgebraicEffects2009,
  title = {Handlers of {{Algebraic Effects}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Plotkin, Gordon and Pretnar, Matija},
  editor = {Castagna, Giuseppe},
  year = {2009},
  pages = {80--94},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  abstract = {We present an algebraic treatment of exception handlers and, more generally, introduce handlers for other computational effects representable by an algebraic theory. These include nondeterminism, interactive input/output, concurrency, state, time, and their combinations; in all cases the computation monad is the free-model monad of the theory. Each such handler corresponds to a model of the theory for the effects at hand. The handling construct, which applies a handler to a computation, is based on the one introduced by Benton and Kennedy, and is interpreted using the homomorphism induced by the universal property of the free model. This general construct can be used to describe previously unrelated concepts from both theory and practice.},
  isbn = {978-3-642-00590-9},
  file = {/Users/harrison/Zotero/storage/TA2TXKD5/Plotkin and Pretnar - 2009 - Handlers of Algebraic Effects.pdf}
}

@misc{polikarpovaProgramSynthesisPolymorphic2016,
  title = {Program {{Synthesis}} from {{Polymorphic Refinement Types}}},
  author = {Polikarpova, Nadia and Kuraj, Ivan and {Solar-Lezama}, Armando},
  year = {2016},
  month = apr,
  number = {arXiv:1510.08419},
  eprint = {1510.08419},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1510.08419},
  urldate = {2024-01-21},
  abstract = {We present a method for synthesizing recursive functions that provably satisfy a given specification in the form of a polymorphic refinement type. We observe that such specifications are particularly suitable for program synthesis for two reasons. First, they offer a unique combination of expressive power and decidability, which enables automatic verification---and hence synthesis---of nontrivial programs. Second, a type-based specification for a program can often be effectively decomposed into independent specifications for its components, causing the synthesizer to consider fewer component combinations and leading to a combinatorial reduction in the size of the search space. At the core of our synthesis procedure is a new algorithm for refinement type checking, which supports specification decomposition. We have evaluated our prototype implementation on a large set of synthesis problems and found that it exceeds the state of the art in terms of both scalability and usability. The tool was able to synthesize more complex programs than those reported in prior work (several sorting algorithms and operations on balanced search trees), as well as most of the benchmarks tackled by existing synthesizers, often starting from a more concise and intuitive user input.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages,F.3.1,I.2.2},
  file = {/Users/harrison/Zotero/storage/BAC9AHP5/Polikarpova et al. - 2016 - Program Synthesis from Polymorphic Refinement Type.pdf;/Users/harrison/Zotero/storage/VM5C684L/1510.html}
}

@article{polikarpovaStructuringSynthesisHeapmanipulating2019,
  title = {Structuring the Synthesis of Heap-Manipulating Programs},
  author = {Polikarpova, Nadia and Sergey, Ilya},
  year = {2019},
  month = jan,
  journal = {SuSLik, Tool Implementation for Article: Structuring the Synthesis of Heap-Manipulating Programs},
  volume = {3},
  number = {POPL},
  pages = {72:1--72:30},
  doi = {10.1145/3290385},
  urldate = {2024-09-22},
  abstract = {This paper describes a deductive approach to synthesizing imperative programs with pointers from declarative specifications expressed in Separation Logic. Our synthesis algorithm takes as input a pair of assertions---a pre- and a postcondition---which describe two states of the symbolic heap, and derives a program that transforms one state into the other, guided by the shape of the heap. Our approach to program synthesis is grounded in proof theory: we introduce the novel framework of Synthetic Separation Logic (SSL), which generalises the classical notion of heap entailment P {$\vdash$} Q to incorporate a possibility of transforming a heap satisfying an assertion P into a heap satisfying an assertion Q. A synthesized program represents a proof term for a transforming entailment statement P {$\arrowwaveright$} Q, and the synthesis procedure corresponds to a proof search. The derived programs are, thus, correct by construction, in the sense that they satisfy the ascribed pre/postconditions, and are accompanied by complete proof derivations, which can be checked independently. We have implemented a proof search engine for SSL in a form of the program synthesizer called SuSLik. For efficiency, the engine exploits properties of SSL rules, such as invertibility and commutativity of rule applications on separate heaps, to prune the space of derivations it has to consider. We explain and showcase the use of SSL on characteristic examples, describe the design of SuSLik, and report on our experience of using it to synthesize a series of benchmark programs manipulating heap-based linked data structures.},
  file = {/Users/harrison/Zotero/storage/G5VIAIX3/Polikarpova and Sergey - 2019 - Structuring the synthesis of heap-manipulating pro.pdf}
}

@inproceedings{porncharoenwaseCompoSATSpecificationGuidedCoverage2018,
  title = {{{CompoSAT}}: {{Specification-Guided Coverage}} for {{Model Finding}}},
  shorttitle = {{{CompoSAT}}},
  booktitle = {Formal {{Methods}}},
  author = {Porncharoenwase, Sorawee and Nelson, Tim and Krishnamurthi, Shriram},
  editor = {Havelund, Klaus and Peleska, Jan and Roscoe, Bill and {de Vink}, Erik},
  year = {2018},
  pages = {568--587},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-95582-7_34},
  abstract = {Model-finding tools like the Alloy Analyzer produce concrete examples of how a declarative specification can be satisfied. These formal tools are useful in a wide range of domains: software design, security, networking, and more. By producing concrete examples, they assist in exploring system behavior and can help find surprising faults.},
  isbn = {978-3-319-95582-7},
  langid = {english},
  keywords = {Enumerative Model,Local Necessities,Model Finding,Overconstraint,Rightmost Model},
  file = {/Users/harrison/Zotero/storage/DFNZHRKT/Porncharoenwase et al. - 2018 - CompoSAT Specification-Guided Coverage for Model .pdf}
}

@article{prinzMergingInductiveRelations2023,
  title = {Merging {{Inductive Relations}}},
  author = {Prinz, Jacob and Lampropoulos, Leonidas},
  year = {2023},
  month = jun,
  journal = {Reproduction Package for "Merging Inductive Relations"},
  volume = {7},
  number = {PLDI},
  pages = {178:1759--178:1778},
  doi = {10.1145/3591292},
  urldate = {2024-07-29},
  abstract = {Inductive relations offer a powerful and expressive way of writing    program specifications while facilitating compositional reasoning.    Their widespread use by proof assistant users has made them a    particularly attractive target for proof engineering tools such as    QuickChick, a property-based testing tool for Coq which can    automatically derive generators for values satisfying an inductive    relation.    However, while such generators are generally efficient, there is    an infrequent yet seemingly inevitable situation where their    performance greatly degrades: when multiple inductive relations    constrain the same piece of data.    In this paper, we introduce an algorithm for merging two such    inductively defined properties that share an index. The algorithm    finds shared structure between the two relations, and creates a    single merged relation that is provably equivalent to the    conjunction of the two.    We demonstrate, through a series of case studies,    that the merged relations can improve the performance of automatic    generation by orders of magnitude, as well as simplify mechanized    proofs by getting rid of the need for nested induction and tedious    low-level book-keeping.},
  file = {/Users/harrison/Zotero/storage/8LQZ8R29/Prinz and Lampropoulos - 2023 - Merging Inductive Relations.pdf}
}

@inproceedings{puSemanticOnSpecifyingContentbased2022,
  title = {{{SemanticOn}}: {{Specifying}} Content-Based Semantic Conditions for Web Automation Programs},
  booktitle = {Proceedings of the 35th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Pu, Kevin and Fu, Rainey and Dong, Rui and Wang, Xinyu and Chen, Yan and Grossman, Tovi},
  year = {2022},
  pages = {1--16}
}

@inproceedings{reddyQuicklyGeneratingDiverse2020,
  title = {Quickly Generating Diverse Valid Test Inputs with Reinforcement Learning},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}},
  author = {Reddy, Sameer and Lemieux, Caroline and Padhye, Rohan and Sen, Koushik},
  year = {2020},
  month = oct,
  series = {{{ICSE}} '20},
  pages = {1410--1421},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3377811.3380399},
  urldate = {2022-11-21},
  abstract = {Property-based testing is a popular approach for validating the logic of a program. An effective property-based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver. However, when the test driver requires strict validity constraints on the inputs, completely random input generation fails to generate enough valid inputs. Existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and/or test driver. However, collecting such information reduces the speed at which tests can be executed. In this paper, we propose and study a black-box approach for generating valid test inputs. We first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs. This formalization highlights the role of a guide which governs the space of choices within a random input generator. We then propose a solution based on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator. We evaluate this approach, RLCheck, against pure random input generation as well as a state-of-the-art greybox evolutionary algorithm, on four real-world benchmarks. We find that in the same time budget, RLCheck generates an order of magnitude more diverse valid inputs than the baselines.},
  isbn = {978-1-4503-7121-6},
  file = {/Users/harrison/Zotero/storage/6L5IJYXN/Reddy et al. - 2020 - Quickly generating diverse valid test inputs with .pdf}
}

@inproceedings{regehrTestcaseReductionCompiler2012,
  title = {Test-Case Reduction for {{C}} Compiler Bugs},
  booktitle = {{{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}, {{PLDI}} '12, {{Beijing}}, {{China}} - {{June}} 11 - 16, 2012},
  author = {Regehr, John and Chen, Yang and Cuoq, Pascal and Eide, Eric and Ellison, Chucky and Yang, Xuejun},
  year = {2012},
  pages = {335--346},
  doi = {10.1145/2254064.2254104},
  file = {/Users/harrison/Zotero/storage/9U937GZF/Regehr et al. - 2012 - Test-case reduction for C compiler bugs.pdf}
}

@misc{reidMakingFormalMethods2020,
  title = {Towards Making Formal Methods Normal: Meeting Developers Where They Are},
  shorttitle = {Towards Making Formal Methods Normal},
  author = {Reid, Alastair and Church, Luke and Flur, Shaked and {de Haas}, Sarah and Johnson, Maritza and Laurie, Ben},
  year = {2020},
  month = oct,
  number = {arXiv:2010.16345},
  eprint = {2010.16345},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-23},
  abstract = {Formal verification of software is a bit of a niche activity: it is only applied to the most safety-critical or security-critical software and it is typically only performed by specialized verification engineers. This paper considers whether it would be possible to increase adoption of formal methods by integrating formal methods with developers' existing practices and workflows. We do not believe that widespread adoption will follow from making the prevailing formal methods argument that correctness is more important than engineering teams realize. Instead, our focus is on what we would need to do to enable programmers to make effective use of formal verification tools and techniques. We do this by considering how we might make verification tooling that both serves developers' needs and fits into their existing development lifecycle. We propose a target of two orders of magnitude increase in adoption within a decade driven by ensuring a positive `weekly cost-benefit' ratio for developer time invested.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Software Engineering,D.2.4,D.2.5,F.3.1,H.1.2},
  file = {/Users/harrison/Zotero/storage/J7IS9UV9/Reid et al. - 2020 - Towards making formal methods normal meeting deve.pdf;/Users/harrison/Zotero/storage/PFHFFX65/2010.html}
}

@article{reinExploratoryLiveProgramming2018,
  title = {Exploratory and {{Live}}, {{Programming}} and {{Coding}}},
  author = {Rein, Patrick and Ramson, Stefan and Lincke, Jens and Hirschfeld, Robert and Pape, Tobias},
  year = {2018},
  journal = {The Art, Science, and Engineering of Programming},
  volume = {3},
  number = {1},
  publisher = {Aspect-Oriented Software Association (AOSA)}
}

@inproceedings{rendelInvertibleSyntaxDescriptions2010,
  title = {Invertible {{Syntax Descriptions}}: {{Unifying Parsing}} and {{Pretty Printing}}},
  booktitle = {Proceedings of the {{Third ACM Haskell Symposium}} on {{Haskell}}},
  author = {Rendel, Tillmann and Ostermann, Klaus},
  year = {2010},
  series = {Haskell '10},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1863523.1863525},
  abstract = {Parsers and pretty-printers for a language are often quite similar, yet both are typically implemented separately, leading to redundancy and potential inconsistency. We propose a new interface of syntactic descriptions, with which both parser and pretty-printer can be described as a single program. Whether a syntactic description is used as a parser or as a pretty-printer is determined by the implementation of the interface. Syntactic descriptions enable programmers to describe the connection between concrete and abstract syntax once and for all, and use these descriptions for parsing or pretty-printing as needed. We also discuss the generalization of our programming technique towards an algebra of partial isomorphisms.},
  isbn = {978-1-4503-0252-4},
  keywords = {embedded domain specific languages,invertible computation,parser combinators,pretty printing},
  file = {/Users/harrison/Zotero/storage/T8IV7FBH/Rendel and Ostermann - 2010 - Invertible Syntax Descriptions Unifying Parsing a.pdf}
}

@article{resnickScratchProgrammingAll2009,
  title = {Scratch: Programming for All},
  shorttitle = {Scratch},
  author = {Resnick, Mitchel and Maloney, John and {Monroy-Hern{\'a}ndez}, Andr{\'e}s and Rusk, Natalie and Eastmond, Evelyn and Brennan, Karen and Millner, Amon and Rosenbaum, Eric and Silver, Jay and Silverman, Brian and Kafai, Yasmin},
  year = {2009},
  month = nov,
  journal = {Communications of the ACM},
  volume = {52},
  number = {11},
  pages = {60--67},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1592761.1592779},
  urldate = {2024-09-09},
  abstract = {"Digital fluency" should mean designing, creating, and remixing, not just browsing, chatting, and interacting.},
  langid = {english}
}

@inproceedings{reynoldsTheoryTypeStructure1974,
  title = {Towards a Theory of Type Structure},
  booktitle = {Programming {{Symposium}}, {{Proceedings Colloque}} Sur La {{Programmation}}, {{Paris}}, {{France}}, {{April}} 9-11, 1974},
  author = {Reynolds, John C.},
  editor = {Robinet, Bernard},
  year = {1974},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {19},
  pages = {408--423},
  publisher = {Springer},
  doi = {10.1007/3-540-06859-7_148},
  file = {/Users/harrison/Zotero/storage/B22ZTWK5/Reynolds - 1974 - Towards a theory of type structure.pdf}
}

@article{ringerQEDLargeSurvey2019,
  title = {{{QED}} at {{Large}}: {{A Survey}} of {{Engineering}} of {{Formally Verified Software}}},
  shorttitle = {{{QED}} at {{Large}}},
  author = {Ringer, Talia and Palmskog, Karl and Sergey, Ilya and Gligoric, Milos and Tatlock, Zachary},
  year = {2019},
  month = sep,
  journal = {Foundations and Trends{\textregistered} in Programming Languages},
  volume = {5},
  number = {2-3},
  pages = {102--281},
  publisher = {Now Publishers, Inc.},
  issn = {2325-1107, 2325-1131},
  doi = {10.1561/2500000045},
  urldate = {2024-07-08},
  abstract = {QED at Large: A Survey of Engineering of Formally Verified Software},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/BC9EDAYR/Ringer et al. - 2019 - QED at Large A Survey of Engineering of Formally .pdf}
}

@inproceedings{ringerREPLicaREPLInstrumentation2020,
  title = {{{REPLica}}: {{REPL}} Instrumentation for {{Coq}} Analysis},
  shorttitle = {{{REPLica}}},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN International Conference}} on {{Certified Programs}} and {{Proofs}}},
  author = {Ringer, Talia and {Sanchez-Stern}, Alex and Grossman, Dan and Lerner, Sorin},
  year = {2020},
  month = jan,
  series = {{{CPP}} 2020},
  pages = {99--113},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3372885.3373823},
  urldate = {2024-07-08},
  abstract = {Proof engineering tools make it easier to develop and maintain large systems verified using interactive theorem provers. Developing useful proof engineering tools hinges on understanding the development processes of proof engineers. This paper breaks down one barrier to achieving that understanding: remotely collecting granular data on proof developments as they happen. We have built a tool called REPLica that instruments Coq's interaction model in order to collect fine-grained data on proof developments. It is decoupled from the user interface, and designed in a way that generalizes to other interactive theorem provers with similar interaction models. We have used REPLica to collect data over the span of a month from a group of intermediate through expert proof engineers---enough data to reconstruct hundreds of interactive sessions. The data reveals patterns in fixing proofs and in changing programs and specifications useful for the improvement of proof engineering tools. Our experiences conducting this study suggest design considerations both at the level of the study and at the level of the interactive theorem prover that can facilitate future studies of this kind.},
  isbn = {978-1-4503-7097-4},
  file = {/Users/harrison/Zotero/storage/47YIWC8F/Ringer et al. - 2020 - REPLica REPL instrumentation for Coq analysis.pdf}
}

@inproceedings{robillardCatamorphismGenerationFusion2014,
  title = {Catamorphism {{Generation}} and {{Fusion Using Coq}}},
  booktitle = {2014 16th {{International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}}},
  author = {Robillard, Simon},
  year = {2014},
  month = sep,
  pages = {180--185},
  doi = {10.1109/SYNASC.2014.32},
  urldate = {2024-09-25},
  abstract = {Catamorphisms are a class of higher-order functions that recursively traverse an inductive data structure to produce a value. An important result related to catamorphisms is the fusion theorem, which gives sufficient conditions to rewrite compositions of catamorphisms. We use the Coq proof assistant to automatically define a catamorphism and a fusion theorem according to an arbitrary inductive type definition. Catamorphisms are then used to define functional specifications and the fusion theorem is applied to derive efficient programs that match those specifications.},
  keywords = {Automation,Calculus,Catamorphism,Category theory,Cognition,Context,Coq,Data structures,Distance measurement,Fusion theorem,Interactive theorem prover,Program derivation,Vegetation},
  file = {/Users/harrison/Zotero/storage/RGWDEKIG/Robillard - 2014 - Catamorphism Generation and Fusion Using Coq.pdf;/Users/harrison/Zotero/storage/K3IYQK8X/7034682.html}
}

@misc{rocqteamWelcomeWorldRocq2025,
  title = {Welcome to a {{World}} of {{Rocq}}},
  author = {Rocq Team},
  year = {2025},
  month = jan,
  journal = {Rocq},
  urldate = {2025-02-11},
  abstract = {Rocq is a general-purpose, industrial-strength interactive theorem prover.},
  howpublished = {https://rocq-prover.org},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/JH5ULX65/rocq-prover.org.html}
}

@book{rogersDiffusionInnovations4th2010,
  title = {Diffusion of {{Innovations}}, 4th {{Edition}}},
  author = {Rogers, Everett M.},
  year = {2010},
  month = jul,
  publisher = {{Simon and Schuster}},
  abstract = {Since the first edition of this landmark book was published in 1962, Everett Rogers's name has become "virtually synonymous with the study of diffusion of innovations," according to Choice. The second and third editions of Diffusion of Innovations became the standard textbook and reference on diffusion studies. Now, in the fourth edition, Rogers presents the culmination of more than thirty years of research that will set a new standard for analysis and inquiry.The fourth edition is (1) a revision of the theoretical framework and the research evidence supporting this model of diffusion, and (2) a new intellectual venture, in that new concepts and new theoretical viewpoints are introduced. This edition differs from its predecessors in that it takes a much more critical stance in its review and synthesis of 5,000 diffusion publications. During the past thirty years or so, diffusion research has grown to be widely recognized, applied and admired, but it has also been subjected to both constructive and destructive criticism. This criticism is due in large part to the stereotyped and limited ways in which many diffusion scholars have defined the scope and method of their field of study. Rogers analyzes the limitations of previous diffusion studies, showing, for example, that the convergence model, by which participants create and share information to reach a mutual understanding, more accurately describes diffusion in most cases than the linear model. Rogers provides an entirely new set of case examples, from the Balinese Water Temple to Nintendo videogames, that beautifully illustrate his expansive research, as well as a completely revised bibliography covering all relevant diffusion scholarship in the past decade. Most important, he discusses recent research and current topics, including social marketing, forecasting the rate of adoption, technology transfer, and more. This all-inclusive work will be essential reading for scholars and students in the fields of communications, marketing, geography, economic development, political science, sociology, and other related fields for generations to come.},
  googlebooks = {v1ii4QsB7jIC},
  isbn = {978-1-4516-0247-0},
  langid = {english},
  keywords = {Business & Economics / Business Communication / General,Business & Economics / General,Business & Economics / Marketing / General}
}

@inproceedings{rondonLiquidTypes2008,
  title = {Liquid Types},
  booktitle = {Proceedings of the 29th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Rondon, Patrick M. and Kawaguci, Ming and Jhala, Ranjit},
  year = {2008},
  month = jun,
  series = {{{PLDI}} '08},
  pages = {159--169},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1375581.1375602},
  urldate = {2024-01-22},
  abstract = {We present Logically Qualified Data Types, abbreviated to Liquid Types, a system that combines Hindley-Milner type inference with Predicate Abstraction to automatically infer dependent types precise enough to prove a variety of safety properties. Liquid types allow programmers to reap many of the benefits of dependent types, namely static verification of critical properties and the elimination of expensive run-time checks, without the heavy price of manual annotation. We have implemented liquid type inference in DSOLVE, which takes as input an OCAML program and a set of logical qualifiers and infers dependent types for the expressions in the OCAML program. To demonstrate the utility of our approach, we describe experiments using DSOLVE to statically verify the safety of array accesses on a set of OCAML benchmarks that were previously annotated with dependent types as part of the DML project. We show that when used in conjunction with a fixed set of array bounds checking qualifiers, DSOLVE reduces the amount of manual annotation required for proving safety from 31\% of program text to under 1\%.},
  isbn = {978-1-59593-860-2},
  keywords = {dependent types,hindley-milner,predicate abstraction,type inference},
  file = {/Users/harrison/Zotero/storage/VA3AYFUD/Rondon et al. - 2008 - Liquid types.pdf}
}

@inproceedings{rothermelWhatYouSee1998,
  title = {What You See Is What You Test: {{A}} Methodology for Testing Form-Based Visual Programs},
  booktitle = {Proceedings of the 20th International Conference on {{Software}} Engineering},
  author = {Rothermel, Gregg and Li, Lixin and DuPuis, Christopher and Burnett, Margaret},
  year = {1998},
  pages = {198--207},
  publisher = {IEEE}
}

@article{rtiEconomicImpactsInadequate2002,
  title = {The Economic Impacts of Inadequate Infrastructure for Software Testing},
  author = {{RTI}},
  year = {2002},
  journal = {National Institute of Standards and Technology},
  volume = {1}
}

@inproceedings{ruleAidingCollaborativeReuse2018,
  title = {Aiding {{Collaborative Reuse}} of {{Computational Notebooks}} with {{Annotated Cell Folding}}},
  booktitle = {{\textbackslash}confcscw},
  author = {Rule, Adam and Drosos, Ian and Tabard, Aur{\'e}lien and Hollan, James D.},
  year = {2018},
  publisher = {{\textbackslash}pubacm}
}

@article{runcimanSmallcheckLazySmallcheck2008,
  title = {Smallcheck and Lazy Smallcheck: Automatic Exhaustive Testing for Small Values},
  shorttitle = {Smallcheck and Lazy Smallcheck},
  author = {Runciman, Colin and Naylor, Matthew and Lindblad, Fredrik},
  year = {2008},
  month = sep,
  journal = {ACM SIGPLAN Notices},
  volume = {44},
  number = {2},
  pages = {37--48},
  issn = {0362-1340},
  doi = {10.1145/1543134.1411292},
  urldate = {2022-11-22},
  abstract = {This paper describes two Haskell libraries for property-based testing. Following the lead of QuickCheck, these testing libraries SmallCheck and Lazy SmallCheck also use type-based generators to obtain test-sets of finite values for which properties are checked, and report any counter-examples found. But instead of using a sample of randomly generated values they test properties for all values up to some limiting depth, progressively increasing this limit. The paper explains the design and implementation of both libraries and evaluates them in comparison with each other and with QuickCheck.},
  file = {/Users/harrison/Zotero/storage/5XV4CE3G/Runciman et al. - 2008 - Smallcheck and lazy smallcheck automatic exhausti.pdf}
}

@article{rushbySubtypesSpecificationsPredicate1998,
  title = {Subtypes for Specifications: Predicate Subtyping in {{PVS}}},
  shorttitle = {Subtypes for Specifications},
  author = {Rushby, J. and Owre, S. and Shankar, N.},
  year = {1998},
  month = sep,
  journal = {IEEE Transactions on Software Engineering},
  volume = {24},
  number = {9},
  pages = {709--720},
  issn = {1939-3520},
  doi = {10.1109/32.713327},
  urldate = {2023-10-26},
  abstract = {A specification language used in the context of an effective theorem prover can provide novel features that enhance precision and expressiveness. In particular, type checking for the language can exploit the services of the theorem prover. We describe a feature called "predicate subtyping" that uses this capability and illustrate its utility as mechanized in PVS.},
  file = {/Users/harrison/Zotero/storage/U5F7QJ3E/Rushby et al. - 1998 - Subtypes for specifications predicate subtyping i.pdf;/Users/harrison/Zotero/storage/D2AW9JHM/713327.html}
}

@inproceedings{ruvimovaExploratoryStudyProductivity2022,
  title = {An Exploratory Study of Productivity Perceptions in Software Teams},
  booktitle = {Proceedings of the 44th {{International Conference}} on {{Software Engineering}}},
  author = {Ruvimova, Anastasia and Lill, Alexander and Gugler, Jan and Howe, Lauren and Huang, Elaine and Murphy, Gail and Fritz, Thomas},
  year = {2022},
  month = jul,
  series = {{{ICSE}} '22},
  pages = {99--111},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3510003.3510081},
  urldate = {2023-05-02},
  abstract = {Software development is a collaborative process requiring a careful balance of focused individual effort and team coordination. Though questions of individual productivity have been widely examined in past literature, less is known about the interplay between developers' perceptions of their own productivity as opposed to their team's. In this paper, we present an analysis of 624 daily surveys and 2899 self-reports from 25 individuals across five software teams in North America and Europe, collected over the course of three months. We found that developers tend to operate in fluid team constructs, which impacts team awareness and complicates gauging team productivity. We also found that perceived individual productivity most strongly predicted perceived team productivity, even more than the amount of team interactions, unplanned work, and time spent in meetings. Future research should explore how fluid team structures impact individual and organizational productivity.},
  isbn = {978-1-4503-9221-1},
  keywords = {productivity,software developer,team,user study},
  file = {/Users/harrison/Zotero/storage/GULUGN8L/Ruvimova et al. - 2022 - An exploratory study of productivity perceptions i.pdf}
}

@misc{sablotnyReinforcementLearningGuided2023,
  title = {Reinforcement Learning Guided Fuzz Testing for a Browser's {{HTML}} Rendering Engine},
  author = {Sablotny, Martin and Jensen, Bj{\o}rn Sand and Singer, Jeremy},
  year = {2023},
  month = jul,
  number = {arXiv:2307.14556},
  eprint = {2307.14556},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-01},
  abstract = {Generation-based fuzz testing can uncover various bugs and security vulnerabilities. However, compared to mutation-based fuzz testing, it takes much longer to develop a well-balanced generator that produces good test cases and decides where to break the underlying structure to exercise new code paths. We propose a novel approach to combine a trained test case generator deep learning model with a double deep Q-network (DDQN) for the first time. The DDQN guides test case creation based on a code coverage signal. Our approach improves the code coverage performance of the underlying generator model by up to 18.5{\textbackslash}\% for the Firefox HTML rendering engine compared to the baseline grammar based fuzzer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/harrison/Zotero/storage/682EF5C6/Sablotny et al. - 2023 - Reinforcement learning guided fuzz testing for a b.pdf;/Users/harrison/Zotero/storage/XWHFZX79/2307.html}
}

@inproceedings{saleckerCombinatorialInteractionTesting2012,
  title = {Combinatorial {{Interaction Testing}} for {{Test Selection}} in {{Grammar-Based Testing}}},
  booktitle = {Fifth {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}, {{ICST}} 2012, {{Montreal}}, {{QC}}, {{Canada}}, {{April}} 17-21, 2012},
  author = {Salecker, Elke and Glesner, Sabine},
  editor = {Antoniol, Giuliano and Bertolino, Antonia and Labiche, Yvan},
  year = {2012},
  pages = {610--619},
  publisher = {IEEE Computer Society},
  doi = {10.1109/ICST.2012.148},
  keywords = {combinatorial testing},
  file = {/Users/harrison/Zotero/storage/PELHR3G9/Salecker and Glesner - 2012 - Combinatorial Interaction Testing for Test Selecti.pdf;/Users/harrison/Zotero/storage/MVYG6F9V/6200160.html;/Users/harrison/Zotero/storage/NRHKIWW2/6200160.html}
}

@article{salmanWhatLeadsConfirmatory2020,
  title = {What Leads to a Confirmatory or Disconfirmatory Behavior of Software Testers?},
  author = {Salman, Iflaah and Rodriguez, Pilar and Turhan, Burak and Tosun, Ay{\c s}e and G{\"u}reller, Arda},
  year = {2020},
  journal = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {4},
  pages = {1351--1368},
  publisher = {IEEE}
}

@article{sarkarUpperBoundsSize2017,
  title = {Upper {{Bounds}} on the {{Size}} of {{Covering Arrays}}},
  author = {Sarkar, Kaushik and Colbourn, Charles J.},
  year = {2017},
  journal = {SIAM J. Discrete Math.},
  volume = {31},
  number = {2},
  pages = {1277--1293},
  doi = {10.1137/16M1067767},
  file = {/Users/harrison/Zotero/storage/8KTLPUK4/Sarkar and Colbourn - 2017 - Upper Bounds on the Size of Covering Arrays.pdf}
}

@inproceedings{sarmaMultiverseMultiplexingAlternative2023,
  title = {Multiverse: {{Multiplexing Alternative Data Analyses}} in {{R Notebooks}}},
  shorttitle = {Multiverse},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sarma, Abhraneel and Kale, Alex and Moon, Michael Jongho and Taback, Nathan and Chevalier, Fanny and Hullman, Jessica and Kay, Matthew},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3580726},
  urldate = {2024-11-27},
  abstract = {There are myriad ways to analyse a dataset. But which one to trust? In the face of such uncertainty, analysts may adopt multiverse analysis: running all reasonable analyses on the dataset. Yet this is cognitively and technically difficult with existing tools---how does one specify and execute all combinations of reasonable analyses of a dataset?---and often requires discarding existing workflows. We present multiverse, a tool for implementing multiverse analyses in R with expressive syntax supporting existing computational notebook workflows. multiverse supports building up a multiverse through local changes to a single analysis and optimises execution by pruning redundant computations. We evaluate how multiverse supports programming multiverse analyses using (a) principles of cognitive ergonomics to compare with two existing multiverse tools; and (b) case studies based on semi-structured interviews with researchers who have successfully implemented an end-to-end analysis using multiverse. We identify design tradeoffs (e.g. increased flexibility versus learnability), and suggest future directions for multiverse tool design.},
  isbn = {978-1-4503-9421-5},
  file = {/Users/harrison/Zotero/storage/YA8AGESX/Sarma et al. - 2023 - multiverse Multiplexing Alternative Data Analyses.pdf}
}

@article{satyanarayanReactiveVegaStreaming2016,
  title = {Reactive {{Vega}}: {{A Streaming Dataflow Architecture}} for {{Declarative Interactive Visualization}}},
  shorttitle = {Reactive {{Vega}}},
  author = {Satyanarayan, Arvind and Russell, Ryan and Hoffswell, Jane and Heer, Jeffrey},
  year = {2016},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {22},
  number = {1},
  pages = {659--668},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2015.2467091},
  urldate = {2024-03-17},
  abstract = {We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system.},
  keywords = {Computer architecture,Data models,Data visualization,declarative specification,Encoding,Indexes,Information visualization,interaction,optimization,Runtime,streaming data,systems,toolkits,Visualization},
  file = {/Users/harrison/Zotero/storage/6AQWCSCK/Satyanarayan et al. - 2016 - Reactive Vega A Streaming Dataflow Architecture f.pdf;/Users/harrison/Zotero/storage/N7T8YPJF/7192704.html}
}

@article{satyanarayanVegaLiteGrammarInteractive2017,
  title = {Vega-{{Lite}}: {{A Grammar}} of {{Interactive Graphics}}},
  shorttitle = {Vega-{{Lite}}},
  author = {Satyanarayan, Arvind and Moritz, Dominik and Wongsuphasawat, Kanit and Heer, Jeffrey},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {23},
  number = {1},
  pages = {341--350},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2016.2599030},
  abstract = {We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.},
  keywords = {Brushes,Data visualization,declarative specification,Encoding,Grammar,Information visualization,interaction,systems,toolkits,Transforms,Visualization},
  file = {/Users/harrison/Zotero/storage/6ZJTKHMX/Satyanarayan et al. - 2017 - Vega-Lite A Grammar of Interactive Graphics.pdf;/Users/harrison/Zotero/storage/35RKGRYM/7539624.html}
}

@misc{savageDemonstratingFEDTSupporting2024,
  title = {Demonstrating {{FEDT}}: {{Supporting Characterization Experiments}} in {{Fabrication Research}}},
  author = {Savage, Valkyrie and P{\"u}s{\"o}k, N{\'o}ra and Goldstein, Harrison and Nandi, Chandrakana and Ren, Jia Yi and Oehlberg, Lora},
  year = {2024}
}

@inproceedings{schrijversMonadTransformersModular2019,
  title = {Monad {{Transformers}} and {{Modular Algebraic Effects}}: {{What Binds Them Together}}},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Symposium}} on {{Haskell}}},
  author = {Schrijvers, Tom and Pir{\'o}g, Maciej and Wu, Nicolas and Jaskelioff, Mauro},
  year = {2019},
  series = {Haskell 2019},
  pages = {98--113},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3331545.3342595},
  abstract = {For over two decades, monad transformers have been the main modular approach for expressing purely functional side-effects in Haskell. Yet, in recent years algebraic effects have emerged as an alternative whose popularity is growing. While the two approaches have been well-studied, there is still confusion about their relative merits and expressiveness, especially when it comes to their comparative modularity. This paper clarifies the connection between the two approaches---some of which is folklore---and spells out consequences that we believe should be better known. We characterise a class of algebraic effects that is modular, and show how these correspond to a specific class of monad transformers. In particular, we show that our modular algebraic effects gives rise to monad transformers. Moreover, every monad transformer for algebraic operations gives rise to a modular effect handler.},
  isbn = {978-1-4503-6813-1},
  file = {/Users/harrison/Zotero/storage/M6KLVTMQ/Schrijvers et al. - 2019 - Monad Transformers and Modular Algebraic Effects .pdf}
}

@article{scibiorFunctionalProgrammingModular2018,
  title = {Functional Programming for Modular {{Bayesian}} Inference},
  author = {{\'S}cibior, Adam and Kammar, Ohad and Ghahramani, Zoubin},
  year = {2018},
  month = jul,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {ICFP},
  pages = {83:1--83:29},
  doi = {10.1145/3236778},
  urldate = {2023-02-09},
  abstract = {We present an architectural design of a library for Bayesian modelling and inference in modern functional programming languages. The novel aspect of our approach are modular implementations of existing state-of-the-art inference algorithms. Our design relies on three inherently functional features: higher-order functions, inductive data-types, and support for either type-classes or an expressive module system. We provide a performant Haskell implementation of this architecture, demonstrating that high-level and modular probabilistic programming can be added as a library in sufficiently expressive languages. We review the core abstractions in this architecture: inference representations, inference transformations, and inference representation transformers. We then implement concrete instances of these abstractions, counterparts to particle filters and Metropolis-Hastings samplers, which form the basic building blocks of our library. By composing these building blocks we obtain state-of-the-art inference algorithms: Resample-Move Sequential Monte Carlo, Particle Marginal Metropolis-Hastings, and Sequential Monte Carlo Squared. We evaluate our implementation against existing probabilistic programming systems and find it is already competitively performant, although we conjecture that existing functional programming optimisation techniques could reduce the overhead associated with the abstractions we use. We show that our modular design enables deterministic testing of inherently stochastic Monte Carlo algorithms. Finally, we demonstrate using OCaml that an expressive module system can also implement our design.},
  keywords = {Anglican,Bayesian inference,functional programming,higher-order functions,inductive types,machine learning,Markov Chain Monte Carlo,module systems,monad transformers,monads,Monte Carlo samplers,probabilistic programming,Sequential Monte Carlo,type-classes,WebPPL},
  file = {/Users/harrison/Zotero/storage/RZQSEQQW/Ścibior et al. - 2018 - Functional programming for modular Bayesian infere.pdf}
}

@article{sedlmairVisualParameterSpace2014,
  title = {Visual {{Parameter Space Analysis}}: {{A Conceptual Framework}}},
  shorttitle = {Visual {{Parameter Space Analysis}}},
  author = {Sedlmair, Michael and Heinzl, Christoph and Bruckner, Stefan and Piringer, Harald and M{\"o}ller, Torsten},
  year = {2014},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  pages = {2161--2170},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2014.2346321},
  urldate = {2024-11-27},
  abstract = {Various case studies in different application domains have shown the great potential of visual parameter space analysis to support validating and using simulation models. In order to guide and systematize research endeavors in this area, we provide a conceptual framework for visual parameter space analysis problems. The framework is based on our own experience and a structured analysis of the visualization literature. It contains three major components: (1) a data flow model that helps to abstractly describe visual parameter space analysis problems independent of their application domain; (2) a set of four navigation strategies of how parameter space analysis can be supported by visualization tools; and (3) a characterization of six analysis tasks. Based on our framework, we analyze and classify the current body of literature, and identify three open research gaps in visual parameter space analysis. The framework and its discussion are meant to support visualization designers and researchers in characterizing parameter space analysis problems and to guide their design and evaluation processes.},
  keywords = {Analytical models,Biological system modeling,Computational modeling,Data models,Image segmentation,input-output model,literature analysis,Parameter space analysis,Predictive models,simulation,task characterization},
  file = {/Users/harrison/Zotero/storage/6JZWWMWT/Sedlmair et al. - 2014 - Visual Parameter Space Analysis A Conceptual Fram.pdf;/Users/harrison/Zotero/storage/CMNDV9QT/6876043.html}
}

@inproceedings{seidelTypeTargetedTesting2015,
  title = {Type {{Targeted Testing}}},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  author = {Seidel, Eric L. and Vazou, Niki and Jhala, Ranjit},
  editor = {Vitek, Jan},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {812--836},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-46669-8_33},
  abstract = {We present a new technique called type targeted testing, which translates precise refinement types into comprehensive test-suites. The key insight behind our approach is that through the lens of SMT solvers, refinement types can also be viewed as a high-level, declarative, test generation technique, wherein types are converted to SMT queries whose models can be decoded into concrete program inputs. Our approach enables the systematic and exhaustive testing of implementations from high-level declarative specifications, and furthermore, provides a gradual path from testing to full verification. We have implemented our approach as a Haskell testing tool called TARGET, and present an evaluation that shows how TARGET can be used to test a wide variety of properties and how it compares against state-of-the-art testing approaches.},
  isbn = {978-3-662-46669-8},
  langid = {english},
  keywords = {Choice Variable,Code Coverage,List Type,Symbolic Execution,Valid Input},
  file = {/Users/harrison/Zotero/storage/B9AR7CB3/Seidel et al. - 2015 - Type Targeted Testing.pdf}
}

@inproceedings{serranoGenericMatchingTree2016,
  title = {Generic {{Matching}} of {{Tree Regular Expressions}} over {{Haskell Data Types}}},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}} - 18th {{International Symposium}}, {{PADL}} 2016, {{St}}. {{Petersburg}}, {{FL}}, {{USA}}, {{January}} 18-19, 2016. {{Proceedings}}},
  author = {Serrano, Alejandro and Hage, Jurriaan},
  editor = {Gavanelli, Marco and Reppy, John H.},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9585},
  pages = {83--98},
  publisher = {Springer},
  doi = {10.1007/978-3-319-28228-2_6},
  file = {/Users/harrison/Zotero/storage/87SJ2U32/Serrano and Hage - 2016 - Generic Matching of Tree Regular Expressions over .pdf}
}

@inproceedings{seshiaUCLID5IntegratingModeling2018,
  title = {{{UCLID5}}: {{Integrating Modeling}}, {{Verification}}, {{Synthesis}} and {{Learning}}},
  shorttitle = {{{UCLID5}}},
  booktitle = {2018 16th {{ACM}}/{{IEEE International Conference}} on {{Formal Methods}} and {{Models}} for {{System Design}} ({{MEMOCODE}})},
  author = {Seshia, Sanjit A. and Subramanyan, Pramod},
  year = {2018},
  month = oct,
  pages = {1--10},
  doi = {10.1109/MEMCOD.2018.8556946},
  urldate = {2025-03-14},
  abstract = {Formal methods for system design are facing a confluence of transformative trends. First, systems are increasingly heterogeneous, comprising some combination of hardware, software, networking, and physical processes. Second, these systems are increasingly being designed with data-driven methods, in addition to traditional model-based design techniques. Third, traditional automated reasoning techniques based on deduction are being combined with new techniques for inductive inference and machine learning. In this paper, we present UCLID5, a new system for formal modeling, verification, and synthesis that addresses the challenges and opportunities arising from this confluence. UCLID5 can model heterogeneous computational systems, provides term-level abstraction supported by satisfiability modulo theories (SMT) solvers, enables compositional reasoning, and implements the paradigm of verification by reduction to synthesis, leveraging the advances in algorithmic synthesis and machine learning. We describe the key features of UCLID5 using illustrative examples.},
  keywords = {Cognition,Computational modeling,cyber-physical systems,Diversity reception,Formal methods,hardware,machine learning,Machine learning,Machine learning algorithms,Market research,security,software,Software,specification,synthesis,verification},
  file = {/Users/harrison/Zotero/storage/ZL9EAEC3/8556946.html}
}

@article{shiEtnaEvaluationPlatform2023,
  title = {Etna: {{An Evaluation Platform}} for {{Property-Based Testing}} ({{Experience Report}})},
  author = {Shi, Jessica and Keles, Alperen and Goldstein, Harrison and Pierce, Benjamin C and Lampropoulos, Leonidas},
  year = {2023},
  journal = {Proc. ACM Program. Lang.},
  volume = {7},
  doi = {10.1145/3607860},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/B2WHJM9H/Shi et al. - Etna An Evaluation Platform for Property-Based Te.pdf}
}

@article{shiScienceInteractiveProof2023,
  title = {Towards a {{Science}} of {{Interactive Proof Reading}}},
  author = {Shi, Jessica and Pierce, Benjamin and Head, Andrew},
  year = {2023},
  abstract = {Proof assistants such as Coq are powerful tools for formally verifying the correctness of software. We are interested in the process of reading the proofs produced in this mechanized context, with a goal of building tools to reduce sources of friction and misunderstanding. In this paper, we summarize the early steps we have taken to understand the design space of comprehension aids and conduct a pilot study, as well as our future plans for this ongoing research.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/KS8CTLLH/Shi et al. - 2023 - Towards a Science of Interactive Proof Reading.pdf;/Users/harrison/Zotero/storage/LCMVJ6M9/Shi et al. - 2023 - Towards a Science of Interactive Proof Reading.pdf}
}

@inproceedings{shresthaUnravelFluentCode2021,
  title = {Unravel: {{A}} Fluent Code Explorer for Data Wrangling},
  booktitle = {The 34th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Shrestha, Nischal and Barik, Titus and Parnin, Chris},
  year = {2021},
  pages = {198--207}
}

@misc{simmonsFiniteChoiceLogicProgramming2024,
  title = {Finite-{{Choice Logic Programming}}},
  author = {Simmons, Robert J. and Arntzenius, Michael and Martens, Chris},
  year = {2024},
  month = may,
  number = {arXiv:2405.19040},
  eprint = {2405.19040},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.19040},
  urldate = {2024-10-04},
  abstract = {Logic programming, as exemplified by datalog, defines the meaning of a program as the canonical smallest model derived from deductive closure over its inference rules. However, many problems call for an enumeration of models that vary along some set of choices while maintaining structural and logical constraints -- there is no single canonical model. The notion of stable models has successfully captured programmer intuition about the set of valid solutions for such problems, giving rise to a family of programming languages and associated solvers collectively known as answer set programming. Unfortunately, the definition of a stable model is frustratingly indirect, especially in the presence of rules containing free variables. We propose a new formalism, called finite-choice logic programing, for which the set of stable models can be characterized as the least fixed point of an immediate consequence operator. Our formalism allows straightforward expression of common idioms in both datalog and answer set programming, gives meaning to a new and useful class of programs, enjoys a constructive and direct operational semantics, and admits a predictive cost semantics, which we demonstrate through our implementation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages},
  file = {/Users/harrison/Zotero/storage/3BNCJB9T/Simmons et al. - 2024 - Finite-Choice Logic Programming.pdf;/Users/harrison/Zotero/storage/HMV2MY3M/2405.html}
}

@inproceedings{singhalCreatingThoroughTests2023,
  title = {Creating {{Thorough Tests}} for {{AI-Generated Code}} Is {{Hard}}},
  booktitle = {Proceedings of the 16th {{Annual ACM India Compute Conference}}},
  author = {Singhal, Shreya and Kumar, Viraj},
  year = {2023},
  month = dec,
  series = {{{COMPUTE}} '23},
  pages = {108--111},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3627217.3627238},
  urldate = {2023-12-29},
  abstract = {Before implementing a function, programmers are encouraged to write a suite of test cases that specify its intended behaviour on several inputs. A suite of tests is thorough if any buggy implementation fails at least one of these tests. We posit that as the proportion of code generated by Large Language Models (LLMs) grows, so must the ability of students to create test suites that are thorough enough to detect subtle bugs in such code. Our paper makes two contributions. First, we demonstrate how difficult it can be to create thorough tests for LLM-generated code by evaluating 27 test suites from a public dataset (EvalPlus). Second, by identifying deficiencies in these test suites, we propose strategies for improving the ability of students to develop thorough test suites for LLM-generated code.},
  isbn = {9798400708404},
  file = {/Users/harrison/Zotero/storage/Q6UBVHEL/Singhal and Kumar - 2023 - Creating Thorough Tests for AI-Generated Code is H.pdf}
}

@inproceedings{smithDiscoveringRelationalSpecifications2017,
  title = {Discovering Relational Specifications},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  author = {Smith, Calvin and Ferns, Gabriel and Albarghouthi, Aws},
  year = {2017},
  month = aug,
  pages = {616--626},
  publisher = {ACM},
  address = {Paderborn Germany},
  doi = {10.1145/3106237.3106279},
  urldate = {2022-12-13},
  abstract = {Formal specifications of library functions play a critical role in a number of program analysis and development tasks. We present Bach, a technique for discovering likely relational specifications from data describing input--output behavior of a set of functions comprising a library or a program. Relational specifications correlate different executions of different functions; for instance, commutativity, transitivity, equivalence of two functions, etc. Bach combines novel insights from program synthesis and databases to discover a rich array of specifications. We apply Bach to learn specifications from data generated for a number of standard libraries. Our experimental evaluation demonstrates Bach's ability to learn useful and deep specifications in a small amount of time.},
  isbn = {978-1-4503-5105-8},
  langid = {english},
  keywords = {Datalog,Hyperproperties,Specification Mining},
  file = {/Users/harrison/Zotero/storage/NJZMFAGI/Smith et al. - 2017 - Discovering relational specifications.pdf}
}

@phdthesis{solar-lezamaProgramSynthesisSketching,
  title = {Program Synthesis by Sketching},
  author = {{Solar-Lezama}, Armando},
  address = {United States -- California},
  urldate = {2023-10-22},
  abstract = {The goal of software synthesis is to generate programs automatically from high-level specifications. However, efficient implementations for challenging programs require a combination of high-level algorithmic insights and low-level implementation details. Deriving the low-level details is a natural job for a computer, but the synthesizer can not replace the human insight. Therefore, one of the central challenges for software synthesis is to establish a synergy between the programmer and the synthesizer, exploiting the programmer's expertise to reduce the burden on the synthesizer. This thesis introduces sketching, a new style of synthesis that offers a fresh approach to the synergy problem. Previous approaches have relied on meta-programming, or variations of interactive theorem proving to help the synthesizer deduce an efficient implementation. The resulting systems are very powerful, but they require the programmer to master new formalisms far removed from traditional programming models. To make synthesis accessible, programmers must be able to provide their insight effortlessly, using formalisms they already understand. In Sketching, insight is communicated through a partial program, a sketch that expresses the high-level structure of an implementation but leaves holes in place of the low-level details. This form of synthesis is made possible by a new SAT-based inductive synthesis procedure that can efficiently synthesize an implementation from a small number of test cases. This algorithm forms the core of a new counterexample guided inductive synthesis procedure (CEGIS) which combines the inductive synthesizer with a validation procedure to automatically generate test inputs and ensure that the generated program satisfies its specification. With a few extensions, CEGIS can even use its sequential inductive synthesizer to generate concurrent programs; all the concurrency related reasoning is delegated to an off-the-shelf validation procedure. The resulting synthesis system scales to real programming problems from a variety of domains ranging from bit-level ciphers to manipulations of linked datastructures. The system was even used to produce a complete optimized implementation of the AES cipher. The concurrency aware synthesizer was also used to synthesize, in a matter of minutes, the details of a fine-locking scheme for a concurrent set, a sense reversing barrier, and even a solution to the dining philosophers problem. The system was also extended with domain specific knowledge to better handle the problem of implementing stencil computations, an important domain in scientific computing. For this domain, we were able to encode domain specific insight as a problem reduction that converted stencil sketches into simplified sketch problems which CEGIS resolved in a matter of minutes. This specialized synthesizer was used to quickly implement a MultiGrid solver for partial differential equations containing many difficult implementation strategies from the literature. In short, this thesis shows that sketching is a viable approach to making synthesis practical in a general programming context.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781109097450},
  langid = {english},
  school = {University of California, Berkeley},
  keywords = {Applied sciences,Program synthesis,Sketching,Software tools},
  file = {/Users/harrison/Zotero/storage/9UE46ZYB/Solar-Lezama - Program synthesis by sketching.pdf}
}

@techreport{sonnexFixedPointPromotion2017,
  title = {Fixed Point Promotion: Taking the Induction out of Automated Induction},
  shorttitle = {Fixed Point Promotion},
  author = {Sonnex, William},
  year = {2017},
  number = {UCAM-CL-TR-905},
  institution = {University of Cambridge, Computer Laboratory},
  doi = {10.48456/tr-905},
  urldate = {2024-12-04},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/LZTWTHRE/Sonnex - 2017 - Fixed point promotion taking the induction out of.pdf}
}

@article{soremekunInputsHellLearning2020,
  title = {Inputs from {{Hell}}: {{Learning Input Distributions}} for {{Grammar-Based Test Generation}}},
  author = {Soremekun, Ezekiel and Pavese, Esteban and Havrikov, Nikolas and Grunske, Lars and Zeller, Andreas},
  year = {2020},
  journal = {IEEE Transactions on Software Engineering},
  publisher = {IEEE},
  doi = {10.1109/TSE.2020.3013716},
  file = {/Users/harrison/Zotero/storage/BWWB88RQ/Soremekun et al. - 2022 - Inputs From Hell.pdf;/Users/harrison/Zotero/storage/TD3YCFMB/9154602.html}
}

@article{spearmanProofMeasurementAssociation1904,
  title = {The {{Proof}} and {{Measurement}} of {{Association}} between {{Two Things}}.},
  author = {Spearman, C},
  year = {1904},
  journal = {American Journal of Psychology},
  volume = {15},
  pages = {72--101},
  publisher = {University of Illinois Press, etc.}
}

@inproceedings{srivastavaGramatronEffectiveGrammaraware2021,
  title = {Gramatron: Effective Grammar-Aware Fuzzing},
  shorttitle = {Gramatron},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Srivastava, Prashast and Payer, Mathias},
  year = {2021},
  month = jul,
  series = {{{ISSTA}} 2021},
  pages = {244--256},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3460319.3464814},
  urldate = {2023-02-08},
  abstract = {Fuzzers aware of the input grammar can explore deeper program states using grammar-aware mutations. Existing grammar-aware fuzzers are ineffective at synthesizing complex bug triggers due to: (i) grammars introducing a sampling bias during input generation due to their structure, and (ii) the current mutation operators for parse trees performing localized small-scale changes. Gramatron uses grammar automatons in conjunction with aggressive mutation operators to synthesize complex bug triggers faster. We build grammar automatons to address the sampling bias. It restructures the grammar to allow for unbiased sampling from the input state space. We redesign grammar-aware mutation operators to be more aggressive, i.e., perform large-scale changes. Gramatron can consistently generate complex bug triggers in an efficient manner as compared to using conventional grammars with parse trees. Inputs generated from scratch by Gramatron have higher diversity as they achieve up to 24.2\% more coverage relative to existing fuzzers. Gramatron makes input generation 98\% faster and the input representations are 24\% smaller. Our redesigned mutation operators are 6.4{\texttimes} more aggressive while still being 68\% faster at performing these mutations. We evaluate Gramatron across three interpreters with 10 known bugs consisting of three complex bug triggers and seven simple bug triggers against two Nautilus variants. Gramatron finds all the complex bug triggers reliably and faster. For the simple bug triggers, Gramatron outperforms Nautilus four out of seven times. To demonstrate Gramatron's effectiveness in the wild, we deployed Gramatron on three popular interpreters for a 10-day fuzzing campaign where it discovered 10 new vulnerabilities.},
  isbn = {978-1-4503-8459-9},
  keywords = {dynamic software analysis,Fuzzing,grammar-aware},
  file = {/Users/harrison/Zotero/storage/QCWZE6FV/Srivastava and Payer - 2021 - Gramatron effective grammar-aware fuzzing.pdf}
}

@misc{staffOctoverseAILeads2024,
  title = {Octoverse: {{AI}} Leads {{Python}} to Top Language as the Number of Global Developers Surges},
  shorttitle = {Octoverse},
  author = {Staff, GitHub},
  year = {2024},
  month = oct,
  journal = {The GitHub Blog},
  urldate = {2024-12-12},
  abstract = {In this year's Octoverse report, we study how public and open source activity on GitHub shows how AI is expanding as the global developer community surges in size.},
  langid = {american},
  file = {/Users/harrison/Zotero/storage/SLHNJ787/octoverse-2024.html}
}

@misc{stanleyHedgehogWillEat2017,
  title = {Hedgehog Will Eat All Your Bugs},
  author = {Stanley, Jacob},
  year = {2017},
  urldate = {2023-01-25},
  abstract = {Generate hundreds of test cases automatically, exposing even the most insidious of corner cases. Failures are automatically simplified, giving developers coherent, intelligible error messages.},
  howpublished = {https://hedgehog.qa/},
  file = {/Users/harrison/Zotero/storage/92Z855RK/hedgehog.qa.html}
}

@inproceedings{steinhofelInputInvariants2022,
  title = {Input Invariants},
  booktitle = {Proceedings of the 30th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Steinh{\"o}fel, Dominic and Zeller, Andreas},
  year = {2022},
  month = nov,
  series = {{{ESEC}}/{{FSE}} 2022},
  pages = {583--594},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3540250.3549139},
  urldate = {2023-02-16},
  abstract = {How can we generate valid system inputs? Grammar-based fuzzers are highly efficient in producing syntactically valid system inputs. However, programs will often reject inputs that are semantically invalid. We introduce ISLa, a declarative specification language for context-sensitive properties of structured system inputs based on context-free grammars. With ISLa, it is possible to specify input constraints like "a variable has to be defined before it is used," "the 'file name' block must be 100 bytes long," or "the number of columns in all CSV rows must be identical." Such constraints go into the ISLa fuzzer, which leverages the power of solvers like Z3 to solve semantic constraints and, on top, handles quantifiers and predicates over grammar structure. We show that a few ISLa constraints suffice to produce 100\% semantically valid inputs while still maintaining input diversity. ISLa can also parse and precisely validate inputs against semantic constraints. ISLa constraints can be mined from existing input samples. For this, our ISLearn prototype uses a catalog of common patterns, instantiates these over input elements, and retains those candidates that hold for the inputs observed and whose instantiations are fully accepted by input-processing programs. The resulting constraints can then again be used for fuzzing and parsing.},
  isbn = {978-1-4503-9413-0},
  keywords = {constraint mining,fuzzing,grammars,specification language},
  file = {/Users/harrison/Zotero/storage/CQRFYSPF/Steinhöfel and Zeller - 2022 - Input invariants.pdf}
}

@inproceedings{stephensDrillerAugmentingFuzzing2016,
  title = {Driller: {{Augmenting Fuzzing Through Selective Symbolic Execution}}.},
  booktitle = {Network and {{Distributed System Security Symposium}} ({{NDSS}})},
  author = {Stephens, Nick and Grosen, John and Salls, Christopher and Dutcher, Andrew and Wang, Ruoyu and Corbetta, Jacopo and Shoshitaishvili, Yan and Kruegel, Christopher and Vigna, Giovanni},
  year = {2016},
  file = {/Users/harrison/Zotero/storage/H7ZYDLHF/Stephens et al. - 2016 - Driller Augmenting Fuzzing Through Selective Symb.pdf}
}

@misc{stewartTestQuickCheckHackage2024,
  title = {Test.{{QuickCheck}} --- Hackage.Haskell.Org},
  author = {Stewart, Donald and Claessen, Koen and Smallbone, Nick and Marlow, Simon},
  year = {2024}
}

@inproceedings{subramanianLiveAPIDocumentation2014,
  title = {Live {{API Documentation}}},
  booktitle = {{\textbackslash}conficse},
  author = {Subramanian, Siddharth and Inozemtseva, Laura and Holmes, Reid},
  year = {2014},
  pages = {643--652},
  publisher = {{\textbackslash}pubacm}
}

@article{sunshineResearchExperiencesUndergraduates2024,
  title = {Research {{Experiences}} for {{Undergraduates Are Necessary}} for an {{Equitable Research Community}}},
  author = {Sunshine, Joshua and {Velez-Ginorio}, Joey},
  year = {2024},
  month = aug,
  journal = {Communications of the ACM},
  volume = {67},
  number = {8},
  pages = {26--28},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3665517},
  urldate = {2024-09-09},
  abstract = {Encouraging the expansion of undergraduate research programs.},
  langid = {english}
}

@inproceedings{suzukiTraceDiffDebuggingUnexpected2017,
  title = {{{TraceDiff}}: {{Debugging}} Unexpected Code Behavior Using Trace Divergences},
  shorttitle = {{{TraceDiff}}},
  booktitle = {2017 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  author = {Suzuki, Ryo and Soares, Gustavo and Head, Andrew and Glassman, Elena and Reis, Ruan and Mongiovi, Melina and D'Antoni, Loris and Hartmann, Bj{\"o}rn},
  year = {2017},
  month = oct,
  pages = {107--115},
  issn = {1943-6106},
  doi = {10.1109/VLHCC.2017.8103457},
  urldate = {2024-02-25},
  abstract = {Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q\&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.},
  keywords = {Computer bugs,Concrete,Debugging,Programming,Tools,Visualization},
  file = {/Users/harrison/Zotero/storage/L9PL7X43/Suzuki et al. - 2017 - TraceDiff Debugging unexpected code behavior usin.pdf;/Users/harrison/Zotero/storage/NBIJ9LE5/8103457.html}
}

@article{tanimotoVIVAVisualLanguage1990,
  title = {{{VIVA}}: {{A}} Visual Language for Image Processing},
  shorttitle = {{{VIVA}}},
  author = {Tanimoto, Steven L.},
  year = {1990},
  month = jun,
  journal = {Journal of Visual Languages \& Computing},
  volume = {1},
  number = {2},
  pages = {127--139},
  issn = {1045-926X},
  doi = {10.1016/S1045-926X(05)80012-6},
  urldate = {2022-12-21},
  abstract = {Visual languages have been developed to help new programmers express algorithms easily. They also help to make experienced programmers more productive by simplifying the organization of a program through the use of visual representations. However, visual languages have not reached their full potential because of several problems including the following: difficulty of producing visual representations for the more abstract computing constructs; the lack of adequate computing power to update the visual representations in response to user actions; the immaturity of the subfield of visual programming and need for additional breakthroughs and standardization of existing mechanisms. Visualization of Vision Algorithms (VIVA) is a proposed visual language for image processing. Its main purpose is to serve as an effective teaching tool for students of image processing. Its design also takes account of several secondary goals, including the completion of a software platform for research in human/image interaction, the creation of a vehicle for studying algorithms and architectures for parallel image processing, and the establishment of a presentation medium for image-processing algorithms.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/C2QR36WT/Tanimoto - 1990 - VIVA A visual language for image processing.pdf;/Users/harrison/Zotero/storage/IKCC5I3Q/S1045926X05800126.html}
}

@inproceedings{teegenHaskellAutomaticFunction2021,
  title = {{{Haskell}}{$^{-1}$}: {{Automatic Function Inversion}} in {{Haskell}}},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN International Symposium}} on {{Haskell}}},
  author = {Teegen, Finn and Prott, Kai-Oliver and Bunkenburg, Niels},
  year = {2021},
  series = {Haskell 2021},
  pages = {41--55},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3471874.3472982},
  abstract = {We present an approach for automatic function inversion in Haskell. The inverse functions we generate are based on an extension of Haskell's computational model with non-determinism and free variables. We implement this functional logic extension of Haskell via a monadic lifting of functions and type declarations. Using inverse functions, we additionally show how Haskell's pattern matching can be augmented with support for functional patterns, which enable arbitrarily deep pattern matching in data structures. Finally, we provide a plugin for the Glasgow Haskell Compiler to seamlessly integrate inverses and functional patterns into the language, covering almost all of the Haskell2010 language standard.},
  isbn = {978-1-4503-8615-9},
  keywords = {GHC plugin,Haskell,inversion,monadic transformation,partial inversion,pattern matching},
  file = {/Users/harrison/Zotero/storage/CNDETDQN/Teegen et al. - 2021 - Haskell⁻¹ automatic function inversion in Haskell.pdf}
}

@article{thayerTheoryRobustAPI2021,
  title = {A {{Theory}} of {{Robust API Knowledge}}},
  author = {Thayer, Kyle and Chasins, Sarah E. and Ko, Amy J.},
  year = {2021},
  month = mar,
  journal = {ACM Transactions on Computing Education},
  volume = {21},
  number = {1},
  pages = {1--32},
  issn = {1946-6226},
  doi = {10.1145/3444945},
  urldate = {2023-09-12},
  abstract = {Creating modern software inevitably requires using application programming interfaces (APIs). While software developers can sometimes use APIs by simply copying and pasting code examples, a lack of robust knowledge of how an API works can lead to defects, complicate software maintenance, and limit what someone can express with an API. Prior work has uncovered the many ways that API documentation fails to be helpful, though rarely describes precisely why. We present a theory of robust API knowledge that attempts to explain why, arguing that effective understanding and use of APIs depends on three components of knowledge: (1) the domain concepts the API models along with terminology, (2) the usage patterns of APIs along with rationale, and (3) facts about an API's execution to support reasoning about its runtime behavior. We derive five hypotheses from this theory and present a study to test them. Our study investigated the effect of having access to these components of knowledge, finding that while learners requested these three components of knowledge when they were not available, whether the knowledge helped the learner use or understand the API depended on the tasks and likely the relevance and quality of the specific information provided. The theory and our evidence in support of its claims have implications for what content API documentation, tutorials, and instruction should contain and the importance of giving the right information at the right time, as well as what information API tools should compute, and even how APIs should be designed. Future work is necessary to both further test and refine the theory, as well as exploit its ideas for better instructional design.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/N5VWYPNG/Thayer et al. - 2021 - A Theory of Robust API Knowledge.pdf}
}

@inproceedings{tillmannParameterizedUnitTesting2010,
  title = {Parameterized Unit Testing: Theory and Practice},
  shorttitle = {Parameterized Unit Testing},
  booktitle = {Proceedings of the 32nd {{ACM}}/{{IEEE International Conference}} on {{Software Engineering}} - {{Volume}} 2},
  author = {Tillmann, Nikolai and {de Halleux}, Jonathan and Xie, Tao},
  year = {2010},
  month = may,
  series = {{{ICSE}} '10},
  pages = {483--484},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1810295.1810441},
  urldate = {2023-08-15},
  abstract = {Unit testing has been widely recognized as an important and valuable means of improving software reliability, as it exposes bugs early in the software development life cycle. However, manual unit testing is often tedious and insufficient. Testing tools can be used to enable economical use of resources by reducing manual effort. Recently parameterized unit testing has emerged as a very promising and effective methodology to allow the separation of two testing concerns or tasks: the specification of external, black-box behavior (i.e., assertions or specifications) by developers and the generation and selection of internal, white-box test inputs (i.e., high-code-covering test inputs) by tools. A parameterized unit test (PUT) is simply a test method that takes parameters, calls the code under test, and states assertions. PUTs have been supported by various testing frameworks. Various open source and industrial testing tools also exist to generate test inputs for PUTs. This tutorial presents latest research on principles and techniques, as well as practical considerations to apply parameterized unit testing on real-world programs, highlighting success stories, research and education achievements, and future research directions in developer testing. The tutorial will help improve developer skills and knowledge for writing PUTs and give overview of tool automation in supporting PUTs. Attendees will acquire the skills and knowledge needed to perform research or conduct practice in the field of developer testing and to integrate developer testing techniques in their own research, practice, and education.},
  isbn = {978-1-60558-719-6},
  keywords = {mock objects,parameterized unit testing,Pex,symbolic execution,testing,theories,unit testing},
  file = {/Users/harrison/Zotero/storage/W323W7ML/Tillmann et al. - 2010 - Parameterized unit testing theory and practice.pdf}
}

@inproceedings{torlakGrowingSolveraidedLanguages2013,
  title = {Growing Solver-Aided Languages with Rosette},
  booktitle = {Proceedings of the 2013 {{ACM}} International Symposium on {{New}} Ideas, New Paradigms, and Reflections on Programming \& Software},
  author = {Torlak, Emina and Bodik, Rastislav},
  year = {2013},
  month = oct,
  series = {Onward! 2013},
  pages = {135--152},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2509578.2509586},
  urldate = {2023-02-16},
  abstract = {SAT and SMT solvers have automated a spectrum of programming tasks, including program synthesis, code checking, bug localization, program repair, and programming with oracles. In principle, we obtain all these benefits by translating the program (once) to a constraint system understood by the solver. In practice, however, compiling a language to logical formulas is a tricky process, complicated by having to map the solution back to the program level and extend the language with new solver-aided constructs, such as symbolic holes used in synthesis. This paper introduces ROSETTE, a framework for designing solver-aided languages. ROSETTE is realized as a solver-aided language embedded in Racket, from which it inherits extensive support for meta-programming. Our framework frees designers from having to compile their languages to constraints: new languages, and their solver-aided constructs, are defined by shallow (library-based) or deep (interpreter-based) embedding in ROSETTE itself. We describe three case studies, by ourselves and others, of using ROSETTE to implement languages and synthesizers for web scraping, spatial programming, and superoptimization of bitvector programs.},
  isbn = {978-1-4503-2472-4},
  keywords = {solver-aided languages},
  file = {/Users/harrison/Zotero/storage/MSGK9USA/Torlak and Bodik - 2013 - Growing solver-aided languages with rosette.pdf}
}

@article{torres-jimenezNewBoundsBinary2012,
  title = {New Bounds for Binary Covering Arrays Using Simulated Annealing},
  author = {{Torres-Jimenez}, Jose and {Rodriguez-Tello}, Eduardo},
  year = {2012},
  journal = {Inf. Sci.},
  volume = {185},
  number = {1},
  pages = {137--152},
  doi = {10.1016/j.ins.2011.09.020},
  file = {/Users/harrison/Zotero/storage/BXIVQXU3/Torres-Jimenez and Rodriguez-Tello - 2012 - New bounds for binary covering arrays using simula.pdf}
}

@article{torres-ruizIterationDiscreteProbabilistic2024,
  title = {On {{Iteration}} in {{Discrete Probabilistic Programming}}},
  author = {{Torres-Ruiz}, Mateo and Piedeleu, Robin and Silva, Alexandra and Zanasi, Fabio},
  year = {2024},
  journal = {LIPIcs, Volume 299, FSCD 2024},
  volume = {299},
  pages = {20:1-20:21},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  issn = {1868-8969},
  doi = {10.4230/LIPICS.FSCD.2024.20},
  urldate = {2024-10-09},
  abstract = {Discrete probabilistic programming languages provide an expressive tool for representing and reasoning about probabilistic models. These languages typically define the semantics of a program through its posterior distribution, obtained through exact inference techniques.},
  collaborator = {Rehof, Jakob},
  copyright = {Creative Commons Attribution 4.0 International license, info:eu-repo/semantics/openAccess},
  isbn = {9783959773232},
  langid = {english},
  keywords = {Probabilistic programming,Programming languages semantics,Theory of computation  Program semantics,Unbounded iteration},
  file = {/Users/harrison/Zotero/storage/IQJ33IPU/Torres-Ruiz et al. - 2024 - On Iteration in Discrete Probabilistic Programming.pdf}
}

@article{trnkovaFreeAlgebrasInput1975,
  title = {Free Algebras, Input Processes and Free Monads},
  author = {Trnkov{\'a}, Vera and Ad{\'a}mek, Ji{\v r}{\textbackslash}'{\i}{\textbackslash} and Koubek, V{\'a}clav and Reiterman, Jan},
  year = {1975},
  journal = {Commentationes Mathematicae Universitatis Carolinae},
  volume = {016},
  pages = {339--351},
  langid = {english},
  mrnumber = {0371986}
}

@inproceedings{usaolaTestCaseGeneration2017,
  title = {Test {{Case Generation}} with {{Regular Expressions}} and {{Combinatorial Techniques}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation Workshops}} ({{ICSTW}})},
  author = {Usaola, Macario Polo and Romero, Francisco Ruiz and Aranda, Rosana Rodr{\'i}guez-Bobada and Rodr{\'i}guez, Ignacio Garc{\'i}a},
  year = {2017},
  month = mar,
  pages = {189--198},
  doi = {10.1109/ICSTW.2017.38},
  abstract = {A test case describes a specific execution scenario of the system under test (SUT). Its goal is to discover errors by means of its oracle, that emits a pass or fail verdict depending on the SUT behavior. The test case has a sequence of calls to SUT's operations with specific test data, which may come from the application of a combinatorial algorithm. This paper describes a method to describe generic test scenarios by means of regular expressions, whose symbols point to a SUT operation. The tester assigns values to each operation's parameter. A further step expands the regular expression and produces a set of operation sequences, which are then passed to a combinatorial algorithm to generate actual test cases. Regular expressions are annotated with a set of when clauses, that are processed by the combinatorial algorithm to include the oracle in the generated test cases.},
  file = {/Users/harrison/Zotero/storage/YA53GN75/Usaola et al. - 2017 - Test Case Generation with Regular Expressions and .pdf;/Users/harrison/Zotero/storage/GD53RJCW/7899055.html}
}

@book{uttingPracticalModelBasedTesting2010,
  title = {Practical {{Model-Based Testing}}: {{A Tools Approach}}},
  shorttitle = {Practical {{Model-Based Testing}}},
  author = {Utting, Mark and Legeard, Bruno},
  year = {2010},
  month = jul,
  publisher = {Elsevier},
  abstract = {Practical Model-Based Testing gives a practical introduction to model-based testing, showing how to write models for testing purposes and how to use model-based testing tools to generate test suites. It is aimed at testers and software developers who wish to use model-based testing, rather than at tool-developers or academics. The book focuses on the mainstream practice of functional black-box testing and covers different styles of models, especially transition-based models (UML state machines) and pre/post models (UML/OCL specifications and B notation). The steps of applying model-based testing are demonstrated on examples and case studies from a variety of software domains, including embedded software and information systems. From this book you will learn:  The basic principles and terminology of model-based testing How model-based testing differs from other testing processes How model-based testing fits into typical software lifecycles such as agile methods and the Unified Process The benefits and limitations of model-based testing, its cost effectiveness and how it can reduce time-to-market A step-by-step process for applying model-based testing How to write good models for model-based testing How to use a variety of test selection criteria to control the tests that are generated from your models How model-based testing can connect to existing automated test execution platforms such as Mercury Test Director, Java JUnit, and proprietary test execution environments Presents the basic principles and terminology of model-based testing Shows how model-based testing fits into the software lifecycle, its cost-effectiveness, and how it can reduce time to market Offers guidance on how to use different kinds of modeling techniques, useful test generation strategies, how to apply model-based testing techniques to real applications using case studies},
  isbn = {978-0-08-046648-4},
  langid = {english},
  keywords = {Computers / Languages / General,Computers / Programming / General,Computers / Programming / Object Oriented}
}

@article{vanderrestCompletelyUniqueAccount2022,
  title = {A Completely Unique Account of Enumeration},
  author = {{van der Rest}, Cas and Swierstra, Wouter},
  year = {2022},
  month = aug,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {6},
  number = {ICFP},
  pages = {105:411--105:437},
  doi = {10.1145/3547636},
  urldate = {2023-06-24},
  abstract = {How can we enumerate the inhabitants of an algebraic datatype? This paper explores a datatype generic solution that works for all regular types and indexed families. The enumerators presented here are provably both complete and unique---they will eventually produce every value exactly once---and fair---they avoid bias when composing enumerators. Finally, these enumerators memoise previously enumerated values whenever possible, thereby avoiding repeatedly recomputing recursive results.},
  keywords = {Agda,datatype generic programming,dependently typed programming,interactive proof assistants,property-based testing},
  file = {/Users/harrison/Zotero/storage/6BD2LHUL/van der Rest and Swierstra - 2022 - A completely unique account of enumeration.pdf}
}

@article{vazouRefinementReflectionComplete2017,
  title = {Refinement Reflection: Complete Verification with {{SMT}}},
  shorttitle = {Refinement Reflection},
  author = {Vazou, Niki and Tondwalkar, Anish and Choudhury, Vikraman and Scott, Ryan G. and Newton, Ryan R. and Wadler, Philip and Jhala, Ranjit},
  year = {2017},
  month = dec,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {POPL},
  pages = {53:1--53:31},
  doi = {10.1145/3158141},
  urldate = {2024-05-13},
  abstract = {We introduce Refinement Reflection, a new framework for building SMT-based deductive verifiers. The key idea is to reflect the code implementing a user-defined function into the function's (output) refinement type. As a consequence, at uses of the function, the function definition is instantiated in the SMT logic in a precise fashion that permits decidable verification. Reflection allows the user to write equational proofs of programs just by writing other programs using pattern-matching and recursion to perform case-splitting and induction. Thus, via the propositions-as-types principle, we show that reflection permits the specification of arbitrary functional correctness properties. Finally, we introduce a proof-search algorithm called Proof by Logical Evaluation that uses techniques from model checking and abstract interpretation, to completely automate equational reasoning. We have implemented reflection in Liquid Haskell and used it to verify that the widely used instances of the Monoid, Applicative, Functor, and Monad typeclasses actually satisfy key algebraic laws required to make the clients safe, and have used reflection to build the first library that actually verifies assumptions about associativity and ordering that are crucial for safe deterministic parallelism.},
  keywords = {Haskell,refinement types,theorem proving,verification},
  file = {/Users/harrison/Zotero/storage/JPBMNHE2/Vazou et al. - 2017 - Refinement reflection complete verification with .pdf}
}

@inproceedings{vazouRefinementTypesHaskell2014,
  title = {Refinement Types for {{Haskell}}},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming},
  author = {Vazou, Niki and Seidel, Eric L. and Jhala, Ranjit and Vytiniotis, Dimitrios and {Peyton-Jones}, Simon},
  year = {2014},
  month = aug,
  series = {{{ICFP}} '14},
  pages = {269--282},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2628136.2628161},
  urldate = {2023-10-26},
  abstract = {SMT-based checking of refinement types for call-by-value languages is a well-studied subject. Unfortunately, the classical translation of refinement types to verification conditions is unsound under lazy evaluation. When checking an expression, such systems implicitly assume that all the free variables in the expression are bound to values. This property is trivially guaranteed by eager, but does not hold under lazy, evaluation. Thus, to be sound and precise, a refinement type system for Haskell and the corresponding verification conditions must take into account which subset of binders actually reduces to values. We present a stratified type system that labels binders as potentially diverging or not, and that (circularly) uses refinement types to verify the labeling. We have implemented our system in LIQUIDHASKELL and present an experimental evaluation of our approach on more than 10,000 lines of widely used Haskell libraries. We show that LIQUIDHASKELL is able to prove 96\% of all recursive functions terminating, while requiring a modest 1.7 lines of termination-annotations per 100 lines of code.},
  isbn = {978-1-4503-2873-9},
  file = {/Users/harrison/Zotero/storage/E7ZS6XK8/Vazou et al. - 2014 - Refinement types for Haskell.pdf}
}

@inproceedings{veggalamIFuzzerEvolutionaryInterpreter2016,
  title = {{{IFuzzer}}: {{An Evolutionary Interpreter Fuzzer Using Genetic Programming}}},
  shorttitle = {{{IFuzzer}}},
  booktitle = {Computer {{Security}} -- {{ESORICS}} 2016},
  author = {Veggalam, Spandan and Rawat, Sanjay and Haller, Istvan and Bos, Herbert},
  editor = {Askoxylakis, Ioannis and Ioannidis, Sotiris and Katsikas, Sokratis and Meadows, Catherine},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {581--601},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-45744-4_29},
  abstract = {We present an automated evolutionary fuzzing technique to find bugs in JavaScript interpreters. Fuzzing is an automated black box testing technique used for finding security vulnerabilities in the software by providing random data as input. However, in the case of an interpreter, fuzzing is challenging because the inputs are piece of codes that should be syntactically/semantically valid to pass the interpreter's elementary checks. On the other hand, the fuzzed input should also be uncommon enough to trigger exceptional behavior in the interpreter, such as crashes, memory leaks and failing assertions. In our approach, we use evolutionary computing techniques, specifically genetic programming, to guide the fuzzer in generating uncommon input code fragments that may trigger exceptional behavior in the interpreter. We implement a prototype named IFuzzer to evaluate our technique on real-world examples. IFuzzer uses the language grammar to generate valid inputs. We applied IFuzzer first on an older version of the JavaScript interpreter of Mozilla (to allow for a fair comparison to existing work) and found 40 bugs, of which 12 were exploitable. On subsequently targeting the latest builds of the interpreter, IFuzzer found 17 bugs, of which four were security bugs.},
  isbn = {978-3-319-45744-4},
  langid = {english},
  keywords = {Evolutionary computing,Fuzzing,Genetic programming,System security,Vulnerability},
  file = {/Users/harrison/Zotero/storage/4S7WDJCP/Veggalam et al. - 2016 - IFuzzer An Evolutionary Interpreter Fuzzer Using .pdf}
}

@misc{walkeReact2024,
  title = {React},
  author = {Walke, Jordan},
  year = {2024},
  urldate = {2024-02-27},
  abstract = {React is the library for web and native user interfaces. Build user interfaces out of individual pieces called components written in JavaScript. React is designed to let you seamlessly combine components written by independent people, teams, and organizations.},
  howpublished = {https://react.dev/},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/MAQ5LP4R/react.dev.html}
}

@inproceedings{wangDiffLoopSupporting2022,
  title = {Diff in the Loop: {{Supporting}} Data Comparison in Exploratory Data Analysis},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, April Yi and Epperson, Will and DeLine, Robert A and Drucker, Steven M},
  year = {2022},
  pages = {1--10}
}

@inproceedings{wangFalxSynthesispoweredVisualization2021,
  title = {Falx: {{Synthesis-powered}} Visualization Authoring},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Chenglong and Feng, Yu and Bodik, Rastislav and Dillig, Isil and Cheung, Alvin and Ko, Amy J},
  year = {2021},
  pages = {1--15}
}

@inproceedings{wangSuperionGrammarAwareGreybox2019,
  title = {Superion: {{Grammar-Aware Greybox Fuzzing}}},
  shorttitle = {Superion},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Wang, Junjie and Chen, Bihuan and Wei, Lei and Liu, Yang},
  year = {2019},
  month = may,
  pages = {724--735},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2019.00081},
  abstract = {In recent years, coverage-based greybox fuzzing has proven itself to be one of the most effective techniques for finding security bugs in practice. Particularly, American Fuzzy Lop (AFL for short) is deemed to be a great success in fuzzing relatively simple test inputs. Unfortunately, when it meets structured test inputs such as XML and JavaScript, those grammar-blind trimming and mutation strategies in AFL hinder the effectiveness and efficiency. To this end, we propose a grammar-aware coverage-based greybox fuzzing approach to fuzz programs that process structured inputs. Given the grammar (which is often publicly available) of test inputs, we introduce a grammar-aware trimming strategy to trim test inputs at the tree level using the abstract syntax trees (ASTs) of parsed test inputs. Further, we introduce two grammar-aware mutation strategies (i.e., enhanced dictionary-based mutation and tree-based mutation). Specifically, tree-based mutation works via replacing subtrees using the ASTs of parsed test inputs. Equipped with grammar-awareness, our approach can carry the fuzzing exploration into width and depth. We implemented our approach as an extension to AFL, named Superion; and evaluated the effectiveness of Superion using large- scale programs (i.e., an XML engine libplist and three JavaScript engines WebKit, Jerryscript and ChakraCore). Our results have demonstrated that Superion can improve the code coverage (i.e., 16.7\% and 8.8\% in line and function coverage) and bug-finding capability (i.e., 34 new bugs, among which we discovered 22 new vulnerabilities with 19 CVEs assigned and 3.2K USD bug bounty rewards received) over AFL and jsfunfuzz.},
  keywords = {Computer bugs,Engines,Fuzzing,Grammar,Greybox Fuzzing Structured Inputs ASTs,Instruments,Syntactics,XML},
  file = {/Users/harrison/Zotero/storage/LQDZFLV8/Wang et al. - 2019 - Superion Grammar-Aware Greybox Fuzzing.pdf}
}

@misc{wardJSONLines2024,
  title = {{{JSON Lines}}},
  author = {Ward, Ian},
  year = {2024},
  urldate = {2024-04-30},
  howpublished = {https://jsonlines.org/},
  file = {/Users/harrison/Zotero/storage/JXPKJJTJ/jsonlines.org.html}
}

@article{weirichRoleDependentTypes2019,
  title = {A Role for Dependent Types in {{Haskell}}},
  author = {Weirich, Stephanie and Choudhury, Pritam and Voizard, Antoine and Eisenberg, Richard A.},
  year = {2019},
  month = jul,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {ICFP},
  pages = {1--29},
  issn = {2475-1421},
  doi = {10.1145/3341705},
  urldate = {2024-11-15},
  abstract = {STEPHANIE WEIRICH, University of Pennsylvania, USA PRITAM CHOUDHURY, University of Pennsylvania, USA ANTOINE VOIZARD, University of Pennsylvania, USA RICHARD A. EISENBERG, Bryn Mawr College, USA Modern Haskell supports zero-cost coercions, a mechanism where types that share the same run-time representation may be freely converted between. To make sure such conversions are safe and desirable, this feature relies on a mechanism of roles to prohibit invalid coercions. In this work, we show how to incorporate roles into dependent types systems and prove, using the Coq proof assistant, that the resulting system is sound. We have designed this work as a foundation for the addition of dependent types to the Glasgow Haskell Compiler, but we also expect that it will be of use to designers of other dependently-typed languages who might want to adopt Haskell's safe coercions feature. CCS Concepts: {$\bullet$} Software and its engineering {$\rightarrow$} Functional languages; Polymorphism; {$\bullet$} Theory of computation {$\rightarrow$} Type theory.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/XILQQ2VY/Weirich et al. - 2019 - A role for dependent types in Haskell.pdf}
}

@article{weiserProgramSlicing1984,
  title = {Program {{Slicing}}},
  author = {Weiser, Mark},
  year = {1984},
  month = jul,
  journal = {IEEE Transactions on Software Engineering},
  volume = {SE-10},
  number = {4},
  pages = {352--357},
  issn = {1939-3520},
  doi = {10.1109/TSE.1984.5010248},
  urldate = {2023-11-05},
  abstract = {Program slicing is a method for automatically decomposing programs by analyzing their data flow and control flow. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a ``slice,'' is an independent program guaranteed to represent faithfully the original program within the domain of the specified subset of behavior. Some properties of slices are presented. In particular, finding statement-minimal slices is in general unsolvable, but using data flow analysis is sufficient to find approximate slices. Potential applications include automatic slicing tools for debuggng and parallel processing of slices.},
  file = {/Users/harrison/Zotero/storage/3IUW7TKT/Weiser - 1984 - Program Slicing.pdf;/Users/harrison/Zotero/storage/XPBNHDTF/5010248.html}
}

@misc{wikipediaNameServerWikipedia2024,
  title = {Name Server --- {{Wikipedia}}, {{The Free Encyclopedia}}},
  author = {{Wikipedia}},
  year = {2024}
}

@misc{wikipediaRedBlackTree2024,
  title = {Red--Black Tree --- {{Wikipedia}}, {{The Free Encyclopedia}}},
  author = {{Wikipedia}},
  year = {2024}
}

@inproceedings{wilsonHarnessingCuriosityIncrease2003,
  title = {Harnessing Curiosity to Increase Correctness in End-User Programming},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wilson, Aaron and Burnett, Margaret and Beckwith, Laura and Granatir, Orion and Casburn, Ledah and Cook, Curtis and Durham, Mike and Rothermel, Gregg},
  year = {2003},
  month = apr,
  series = {{{CHI}} '03},
  pages = {305--312},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/642611.642665},
  urldate = {2024-02-07},
  abstract = {Despite their ability to help with program correctness, assertions have been notoriously unpopular--even with professional programmers. End-user programmers seem even less likely to appreciate the value of assertions; yet end-user programs suffer from serious correctness problems that assertions could help detect. This leads to the following question: can end users be enticed to enter assertions? To investigate this question, we have devised a curiosity-centered approach to eliciting assertions from end users, built on a surprise-explain-reward strategy. Our follow-up work with end-user participants shows that the approach is effective in encouraging end users to enter assertions that help them find errors.},
  isbn = {978-1-58113-630-2},
  keywords = {assertions,curiosity,end-user software engineering,forms/3,surprise-explain-reward strategy},
  file = {/Users/harrison/Zotero/storage/5P3KM54W/Wilson et al. - 2003 - Harnessing curiosity to increase correctness in en.pdf}
}

@article{wingFormalMethodsLight1996,
  title = {Formal {{Methods Light}}},
  author = {Wing, J. and Jackson, D. and Jones, C. B.},
  year = {1996},
  month = apr,
  journal = {Computer},
  volume = {29},
  number = {04},
  pages = {20--22},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  issn = {1558-0814},
  doi = {10.1109/MC.1996.10038}
}

@inproceedings{wongEffectiveFaultLocalization2007,
  title = {Effective {{Fault Localization}} Using {{Code Coverage}}},
  booktitle = {31st {{Annual International Computer Software}} and {{Applications Conference}} ({{COMPSAC}} 2007)},
  author = {Wong, W. Eric and Qi, Yu and Zhao, Lei and Cai, Kai-Yuan},
  year = {2007},
  month = jul,
  volume = {1},
  pages = {449--456},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC.2007.109},
  abstract = {Localizing a bug in a program can be a complex and timeconsuming process. In this paper we propose a code coveragebased fault localization method to prioritize suspicious code in terms of its likelihood of containing program bugs. Code with a higher risk should be examined before that with a lower risk, as the former is more suspicious (i.e., more likely to contain program bugs) than the latter. We also answer a very important question: How can each additional test case that executes the program successfully help locate program bugs? We propose that with respect to a piece of code, the aid introduced by the first successful test that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second successful test that executes it, which is larger than or equal to that of the third successful test that executes it, etc. A tool, {\textdiv}Debug, was implemented to automate the computation of the risk of the code and the subsequent prioritization of suspicious code for locating program bugs. A case study using the Siemens suite was also conducted. Data collected from our study support the proposal described above. They also indicate that our method (in particular Heuristics III (c), (d), and (e)) can effectively reduce the search domain for locating program bugs.},
  keywords = {Application software,Automatic testing,Computer bugs,Computer science,Data visualization,Java,Proposals,Software debugging,Software design,Software testing},
  file = {/Users/harrison/Zotero/storage/Z967TXPE/Wong et al. - 2007 - Effective Fault Localization using Code Coverage.pdf;/Users/harrison/Zotero/storage/2XN8NG9N/4291037.html}
}

@inproceedings{wongsuphasawatVoyagerAugmentingVisual2017,
  title = {Voyager 2: {{Augmenting Visual Analysis}} with {{Partial View Specifications}}},
  shorttitle = {Voyager 2},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wongsuphasawat, Kanit and Qu, Zening and Moritz, Dominik and Chang, Riley and Ouk, Felix and Anand, Anushka and Mackinlay, Jock and Howe, Bill and Heer, Jeffrey},
  year = {2017},
  month = may,
  pages = {2648--2659},
  publisher = {ACM},
  address = {Denver Colorado USA},
  doi = {10.1145/3025453.3025768},
  urldate = {2022-11-21},
  abstract = {Visual data analysis involves both open-ended and focused exploration. Manual chart specification tools support question answering, but are often tedious for early-stage exploration where systematic data coverage is needed. Visualization recommenders can encourage broad coverage, but irrelevant suggestions may distract users once they commit to specific questions. We present Voyager 2, a mixed-initiative system that blends manual and automated chart specification to help analysts engage in both open-ended exploration and targeted question answering. We contribute two partial specification interfaces: wildcards let users specify multiple charts in parallel, while related views suggest visualizations relevant to the currently specified chart. We present our interface design and applications of the CompassQL visualization query language to enable these interfaces. In a controlled study we find that Voyager 2 leads to increased data field coverage compared to a traditional specification tool, while still allowing analysts to flexibly drill-down and answer specific questions.},
  isbn = {978-1-4503-4655-9},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/STANYE4Z/Wongsuphasawat et al. - 2017 - Voyager 2 Augmenting Visual Analysis with Partial.pdf}
}

@article{wongsuphasawatVoyagerExploratoryAnalysis2016,
  title = {Voyager: {{Exploratory Analysis}} via {{Faceted Browsing}} of {{Visualization Recommendations}}},
  shorttitle = {Voyager},
  author = {Wongsuphasawat, Kanit and Moritz, Dominik and Anand, Anushka and Mackinlay, Jock and Howe, Bill and Heer, Jeffrey},
  year = {2016},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {22},
  number = {1},
  pages = {649--658},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2015.2467191},
  abstract = {General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.},
  file = {/Users/harrison/Zotero/storage/CJVH7AB9/Wongsuphasawat et al. - 2016 - Voyager Exploratory Analysis via Faceted Browsing.pdf;/Users/harrison/Zotero/storage/4XQTC8H8/7192728.html}
}

@article{wrennUsingRelationalProblems2021,
  title = {Using {{Relational Problems}} to {{Teach Property-Based Testing}}},
  author = {Wrenn, John and Nelson, Tim and {and Krishnamurthi}, Shriram},
  editor = {Gibbons, Jeremy},
  year = {2021},
  month = jan,
  journal = {The art science and engineering of programming},
  volume = {5},
  number = {2},
  doi = {10.22152/programming-journal.org/2021/5/9},
  urldate = {2022-12-17},
  abstract = {CONTEXT The success of QuickCheck has led to the development of property-based testing (PBT) libraries for many languages and the process is getting increasing attention. However, unlike regular testing, PBT is not widespread in collegiate curricula. Furthermore, the value of PBT is not limited to software testing. The growing use of formal methods in, and the growth of software synthesis, all create demand for techniques to train students and developers in the art of specification writing. We posit that PBT forms a strong bridge between testing and the act of specification: it's a form of testing where the tester is actually writing abstract specifications. INQUIRY Even well-informed technologists mention the difficulty of finding good motivating examples for its use. We take steps to fill this lacuna. APPROACH \& KNOWLEDGE We find that the use of ``relational'' problems---those for which an input may admit multiple valid outputs---easily motivates the use of PBT. We also notice that such problems are readily available in the computer science pantheon of problems (e.g., many graph and sorting algorithms). We have been using these for some years now to teach PBT in collegiate courses. GROUNDING In this paper, we describe the problems we use and report on students' completion of them. We believe the problems overcome some of the motivation issues described above. We also show that students can do quite well at PBT for these problems, suggesting that the topic is well within their reach. In the process, we introduce a simple method to evaluate the accuracy of their specifications, and use it to characterize their common mistakes. IMPORTANCE Based on our findings, we believe that relational problems are an underutilized motivating example for PBT. We hope this paper initiates a catalog of such problems for educators (and developers) to use, and also provides a concrete (though by no means exclusive) method to analyze the quality of PBT.},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/XVNDNDXB/Wrenn et al. - 2021 - Using Relational Problems to Teach Property-Based .pdf}
}

@inproceedings{wuEffectHandlersScope2014,
  title = {Effect {{Handlers}} in {{Scope}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {Wu, Nicolas and Schrijvers, Tom and Hinze, Ralf},
  year = {2014},
  series = {Haskell '14},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2633357.2633358},
  abstract = {Algebraic effect handlers are a powerful means for describing effectful computations. They provide a lightweight and orthogonal technique to define and compose the syntax and semantics of different effects. The semantics is captured by handlers, which are functions that transform syntax trees.Unfortunately, the approach does not support syntax for scoping constructs, which arise in a number of scenarios. While handlers can be used to provide a limited form of scope, we demonstrate that this approach constrains the possible interactions of effects and rules out some desired semantics.This paper presents two different ways to capture scoped constructs in syntax, and shows how to achieve different semantics by reordering handlers. The first approach expresses scopes using the existing algebraic handlers framework, but has some limitations. The problem is fully solved in the second approach where we introduce higher-order syntax.},
  isbn = {978-1-4503-3041-1},
  keywords = {effect handlers,Haskell,modularity,monads,semantics,syntax},
  file = {/Users/harrison/Zotero/storage/HDAR23QX/Wu et al. - 2014 - Effect Handlers in Scope.pdf}
}

@inproceedings{wuFusionFree2015,
  title = {Fusion for {{Free}}},
  booktitle = {Mathematics of {{Program Construction}}},
  author = {Wu, Nicolas and Schrijvers, Tom},
  editor = {Hinze, Ralf and Voigtl{\"a}nder, Janis},
  year = {2015},
  pages = {302--322},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-19797-5_15},
  abstract = {Algebraic effect handlers are a recently popular approach for modelling side-effects that separates the syntax and semantics of effectful operations. The shape of syntax is captured by functors, and free monads over these functors denote syntax trees. The semantics is captured by algebras, and effect handlers pass these over the syntax trees to interpret them into a semantic domain.},
  isbn = {978-3-319-19797-5},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/6G63I9FT/Wu and Schrijvers - 2015 - Fusion for Free.pdf}
}

@inproceedings{xiaComposingBidirectionalPrograms2019,
  title = {Composing Bidirectional Programs Monadically},
  booktitle = {European {{Symposium}} on {{Programming}}},
  author = {Xia, Li-yao and Orchard, Dominic and Wang, Meng},
  year = {2019},
  pages = {147--175},
  publisher = {Springer},
  doi = {10.1007/978-3-030-17184-1_6},
  file = {/Users/harrison/Zotero/storage/6F3NIVZ5/Xia et al. - 2019 - Composing Bidirectional Programs Monadically.pdf}
}

@inproceedings{yangFindingUnderstandingBugs2011,
  title = {Finding and Understanding Bugs in {{C}} Compilers},
  booktitle = {Proceedings of the 32nd {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}, {{PLDI}} 2011, {{San Jose}}, {{CA}}, {{USA}}, {{June}} 4-8, 2011},
  author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
  year = {2011},
  pages = {283--294},
  doi = {10.1145/1993498.1993532},
  file = {/Users/harrison/Zotero/storage/AG93DYQK/Yang et al. - 2011 - Finding and understanding bugs in C compilers.pdf}
}

@inproceedings{yanVisualizingExamplesDeep2021,
  title = {Visualizing {{Examples}} of {{Deep Neural Networks}} at {{Scale}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yan, Litao and Glassman, Elena L. and Zhang, Tianyi},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--14},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3411764.3445654},
  urldate = {2024-02-25},
  abstract = {Many programmers want to use deep learning due to its superior accuracy in many challenging domains. Yet our formative study with ten programmers indicated that, when constructing their own deep neural networks (DNNs), they often had a difficult time choosing appropriate model structures and hyperparameter values. This paper presents ExampleNet---a novel interactive visualization system for exploring common and uncommon design choices in a large collection of open-source DNN projects. ExampleNet provides a holistic view of the distribution over model structures and hyperparameter settings in the corpus of DNNs, so users can easily filter the corpus down to projects tackling similar tasks and compare design choices made by others. We evaluated ExampleNet in a within-subjects study with sixteen participants. Compared with the control condition (i.e., online search), participants using ExampleNet were able to inspect more online examples, make more data-driven design decisions, and make fewer design mistakes.},
  isbn = {978-1-4503-8096-6},
  keywords = {code examples,deep neural networks,Visualization},
  file = {/Users/harrison/Zotero/storage/LZ7YFGKW/Yan et al. - 2021 - Visualizing Examples of Deep Neural Networks at Sc.pdf}
}

@inproceedings{yeeFacetedMetadataImage2003,
  title = {Faceted Metadata for Image Search and Browsing},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yee, Ka-Ping and Swearingen, Kirsten and Li, Kevin and Hearst, Marti},
  year = {2003},
  month = apr,
  series = {{{CHI}} '03},
  pages = {401--408},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/642611.642681},
  urldate = {2022-11-21},
  abstract = {There are currently two dominant interface types for searching and browsing large image collections: keyword-based search, and searching by overall similarity to sample images. We present an alternative based on enabling users to navigate along conceptual dimensions that describe the images. The interface makes use of hierarchical faceted metadata and dynamically generated query previews. A usability study, in which 32 art history students explored a collection of 35,000 fine arts images, compares this approach to a standard image search interface. Despite the unfamiliarity and power of the interface (attributes that often lead to rejection of new search interfaces), the study results show that 90\% of the participants preferred the metadata approach overall, 97\% said that it helped them learn more about the collection, 75\% found it more flexible, and 72\% found it easier to use than a standard baseline system. These results indicate that a category-based approach is a successful way to provide access to image collections.},
  isbn = {978-1-58113-630-2},
  file = {/Users/harrison/Zotero/storage/FEDFDCDH/Yee et al. - 2003 - Faceted metadata for image search and browsing.pdf}
}

@inproceedings{youngHigherOrderSpecificationsDeductive2024,
  title = {Higher-{{Order Specifications}} for {{Deductive Synthesis}} of {{Programs}} with {{Pointers}}},
  booktitle = {{{DROPS-IDN}}/v2/Document/10.4230/{{LIPIcs}}.{{ECOOP}}.2024.45},
  author = {Young, David and Yang, Ziyi and Sergey, Ilya and Potanin, Alex},
  year = {2024},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  doi = {10.4230/LIPIcs.ECOOP.2024.45},
  urldate = {2024-09-22},
  abstract = {Synthetic Separation Logic (SSL) is a formalism that powers SuSLik, the state-of-the-art approach for the deductive synthesis of provably-correct programs in C-like languages that manipulate heap-based linked data structures. Despite its expressivity, SSL suffers from two shortcomings that hinder its utility. First, its main specification component, inductive predicates, only admits first-order definitions of data structure shapes, which leads to the proliferation of "boiler-plate" predicates for specifying common patterns. Second, SSL requires concrete definitions of data structures to synthesise programs that manipulate them, which results in the need to change a specification for a synthesis task every time changes are introduced into the layout of the involved structures. We propose to significantly lift the level of abstraction used in writing Separation Logic specifications for synthesis - both simplifying the approach and making the specifications more usable and easy to read and follow. We avoid the need to repetitively re-state low-level representation details throughout the specifications - allowing the reuse of different implementations of the same data structure by abstracting away the details of a specific layout used in memory. Our novel high-level front-end language called Pika significantly improves the expressiveness of SuSLik. We implemented a layout-agnostic synthesiser from Pika to SuSLik enabling push-button synthesis of C programs with in-place memory updates, along with the accompanying full proofs that they meet Separation Logic-style specifications, from high-level specifications that resemble ordinary functional programs. Our experiments show that our tool can produce C code that is comparable in its performance characteristics and is sometimes faster than Haskell.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/harrison/Zotero/storage/5TMG2RND/Young et al. - 2024 - Higher-Order Specifications for Deductive Synthesi.pdf}
}

@inproceedings{yunQSYMPracticalConcolic2018,
  title = {{{QSYM}} : {{A Practical Concolic Execution Engine Tailored}} for {{Hybrid Fuzzing}}},
  booktitle = {27th {{USENIX Security Symposium}} ({{USENIX Security}} 18)},
  author = {Yun, Insu and Lee, Sangho and Xu, Meng and Jang, Yeongjin and Kim, Taesoo},
  year = {2018},
  pages = {745--761},
  publisher = {USENIX Association},
  address = {Baltimore, MD},
  isbn = {978-1-931971-46-1},
  file = {/Users/harrison/Zotero/storage/SNM3ZGI9/Yun et al. - 2018 - QSYM  A Practical Concolic Execution Engine Tailo.pdf}
}

@misc{zalewskiAmericanFuzzyLop2022,
  title = {American {{Fuzzy Lop}} ({{AFL}})},
  author = {Zalewski, Micha{\l}},
  year = {2022},
  month = nov,
  publisher = {Google},
  urldate = {2022-11-22},
  abstract = {american fuzzy lop - a security-oriented fuzzer},
  copyright = {Apache-2.0}
}

@book{zellerFuzzingBook2021,
  title = {The {{Fuzzing Book}}},
  author = {Zeller, Andreas and Gopinath, Rahul and B{\"o}hme, Marcel and Fraser, Gordon and Holler, Christian},
  year = {2021},
  publisher = {CISPA Helmholtz Center for Information Security}
}

@inproceedings{zhangInteractiveProgramSynthesis2020,
  title = {Interactive Program Synthesis by Augmented Examples},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Zhang, Tianyi and Lowmanstone, London and Wang, Xinyu and Glassman, Elena L},
  year = {2020},
  pages = {627--648}
}

@inproceedings{zhaoUnderstandingTriggeractionPrograms2021,
  title = {Understanding Trigger-Action Programs through Novel Visualizations of Program Differences},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhao, Valerie and Zhang, Lefan and Wang, Bo and Littman, Michael L and Lu, Shan and Ur, Blase},
  year = {2021},
  pages = {1--17}
}

@article{zhouCoveringAllBases2023,
  title = {Covering {{All}} the {{Bases}}: {{Type-Based Verification}} of {{Test Input Generators}}},
  shorttitle = {Covering {{All}} the {{Bases}}},
  author = {Zhou, Zhe and Mishra, Ashish and Delaware, Benjamin and Jagannathan, Suresh},
  year = {2023},
  month = jun,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {7},
  number = {PLDI},
  pages = {157:1244--157:1267},
  doi = {10.1145/3591271},
  urldate = {2023-06-24},
  abstract = {Test input generators are an important part of property-based testing (PBT) frameworks. Because PBT is intended to test deep semantic and structural properties of a program, the outputs produced by these generators can be complex data structures, constrained to satisfy properties the developer believes is most relevant to testing the function of interest. An important feature expected of these generators is that they be capable of producing all acceptable elements that satisfy the function's input type and generator-provided constraints. However, it is not readily apparent how we might validate whether a particular generator's output satisfies this coverage requirement. Typically, developers must rely on manual inspection and post-mortem analysis of test runs to determine if the generator is providing sufficient coverage; these approaches are error-prone and difficult to scale as generators become more complex. To address this important concern, we present a new refinement type-based verification procedure for validating the coverage provided by input test generators, based on a novel interpretation of types that embeds ``must-style'' underapproximate reasoning principles as a fundamental part of the type system. The types associated with expressions now capture the set of values guaranteed to be produced by the expression, rather than the typical formulation that uses types to represent the set of values an expression may produce. Beyond formalizing the notion of coverage types in the context of a rich core language with higher-order procedures and inductive datatypes, we also present a detailed evaluation study to justify the utility of our ideas.},
  keywords = {property-based testing,refinement types,underapproximate reasoning},
  file = {/Users/harrison/Zotero/storage/RZYDTA3V/Zhou et al. - 2023 - Covering All the Bases Type-Based Verification of.pdf}
}

@article{braquehaisSimpleIncrementalDevelopment2017,
	title = {A simple incremental development of a property-based testing tool ({Functional} {Pearl})},
	abstract = {Property-based testing tools, like QuickCheck, are widely used for testing Haskell programs. Since QuickCheck’s introduction in 2000, several other similar tools and techniques have been developed. There have been papers, book chapters, and countless blog posts on how to use those tools. In this paper, we describe how to write one. The purpose is to be educational: we don’t present new techniques for generation of values or property-based testing. Instead, we present a way to derive such techniques. We start with a very simple implementation (25 LOC), then iteratively re ne it into a full featured property-based testing tool ({\textless}100 LOC).},
	language = {en},
	author = {Braquehais, Rudy and Walker, Michael and Trilla, José Manuel Calderón and Runciman, Colin},
	year = {2017},
	file = {Braquehais et al. - 2017 - A simple incremental development of a property-bas.pdf:/Users/harrison/Zotero/storage/SLN2TEIG/Braquehais et al. - 2017 - A simple incremental development of a property-bas.pdf:application/pdf},
}
